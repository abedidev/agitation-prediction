{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3080 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, confusion_matrix, f1_score, accuracy_score, \\\n",
    "    precision_score, recall_score, average_precision_score\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import KFold, LeavePGroupsOut\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tabpfn import TabPFNClassifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "import shap\n",
    "\n",
    "%matplotlib inline\n",
    "# matplotlib.use('TkAgg')\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler  # install via: pip install imbalanced-learn\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "from scipy.stats import skew\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "seed = 69\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "\n",
    "label = 'agitation-four'\n",
    "kfold = False\n",
    "weighted = False\n",
    "\n",
    "results_dir = '/home/ali/PycharmProjects/tihm/results/results-06h-quaternary-prediction-lopo-unweighted'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/ali/PycharmProjects/tihm/dataset'\n",
    "\n",
    "dataset_06h = pd.read_csv(os.path.join(root, 'data-06h.csv'))\n",
    "dataset_12h = pd.read_csv(os.path.join(root, 'data-12h.csv'))\n",
    "dataset_24h = pd.read_csv(os.path.join(root, 'data-24h.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(dataset_06h)\n",
    "# display(dataset_12h)\n",
    "# display(dataset_24h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400 117 89 18\n",
      "1752 135\n"
     ]
    }
   ],
   "source": [
    "# print(dataset_06h['agitation'].equals(dataset_06h['agitation-next']))\n",
    "# print(np.sum(dataset_06h['agitation']))\n",
    "# print(np.sum(dataset_06h['agitation-next']))\n",
    "\n",
    "# print(np.where(dataset_06h['agitation'] == 1))\n",
    "# print(np.where(dataset_06h['agitation-next'] == 1))\n",
    "\n",
    "print(np.sum(dataset_06h['agitation-four'] == 0),\n",
    "      np.sum(dataset_06h['agitation-four'] == 1),\n",
    "      np.sum(dataset_06h['agitation-four'] == 2),\n",
    "      np.sum(dataset_06h['agitation-four'] == 3))\n",
    "\n",
    "print(np.sum(dataset_06h['agitation-next'] == 0),\n",
    "      np.sum(dataset_06h['agitation-next'] == 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of participants: 56\n",
      "Total number of participants with at least one agitation episode: 27\n",
      "Total number of participants with no agitation episodes: 29\n",
      "Total number of agitation episodes: 135\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>six_hour_sample_count</th>\n",
       "      <th>unique_dates</th>\n",
       "      <th>agitation_episodes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73f7c</td>\n",
       "      <td>190</td>\n",
       "      <td>48</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>93c14</td>\n",
       "      <td>185</td>\n",
       "      <td>47</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c5031</td>\n",
       "      <td>169</td>\n",
       "      <td>43</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0d5ef</td>\n",
       "      <td>191</td>\n",
       "      <td>49</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16f4b</td>\n",
       "      <td>353</td>\n",
       "      <td>89</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7db78</td>\n",
       "      <td>270</td>\n",
       "      <td>68</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>a2849</td>\n",
       "      <td>234</td>\n",
       "      <td>59</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>d93d8</td>\n",
       "      <td>210</td>\n",
       "      <td>53</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ec812</td>\n",
       "      <td>288</td>\n",
       "      <td>73</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>e4959</td>\n",
       "      <td>262</td>\n",
       "      <td>66</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>b9d58</td>\n",
       "      <td>354</td>\n",
       "      <td>90</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>e8a78</td>\n",
       "      <td>249</td>\n",
       "      <td>63</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8a835</td>\n",
       "      <td>295</td>\n",
       "      <td>75</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>95899</td>\n",
       "      <td>174</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>d7a46</td>\n",
       "      <td>91</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3fb61</td>\n",
       "      <td>242</td>\n",
       "      <td>62</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>6b29b</td>\n",
       "      <td>269</td>\n",
       "      <td>70</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>46286</td>\n",
       "      <td>125</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>d263a</td>\n",
       "      <td>270</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>96adf</td>\n",
       "      <td>214</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>393cb</td>\n",
       "      <td>266</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>201d8</td>\n",
       "      <td>290</td>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0cda9</td>\n",
       "      <td>302</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>099bc</td>\n",
       "      <td>173</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>eca1f</td>\n",
       "      <td>215</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>8d0d4</td>\n",
       "      <td>270</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>e87bd</td>\n",
       "      <td>163</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>65db4</td>\n",
       "      <td>202</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>55cd4</td>\n",
       "      <td>282</td>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2b131</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>561af</td>\n",
       "      <td>122</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>56b6b</td>\n",
       "      <td>220</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>28710</td>\n",
       "      <td>179</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2f54b</td>\n",
       "      <td>230</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0f352</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1fbe4</td>\n",
       "      <td>249</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0697d</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0efe8</td>\n",
       "      <td>186</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>385de</td>\n",
       "      <td>74</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>30a32</td>\n",
       "      <td>258</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>714d7</td>\n",
       "      <td>163</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>c55f8</td>\n",
       "      <td>327</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>ab47a</td>\n",
       "      <td>165</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>b45c2</td>\n",
       "      <td>265</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>a539e</td>\n",
       "      <td>230</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>a380e</td>\n",
       "      <td>103</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>76230</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>b0455</td>\n",
       "      <td>39</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>d8d97</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>d44d2</td>\n",
       "      <td>62</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>c8574</td>\n",
       "      <td>206</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>ca44d</td>\n",
       "      <td>158</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>c5785</td>\n",
       "      <td>266</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>e2472</td>\n",
       "      <td>118</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>f220c</td>\n",
       "      <td>258</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>fd100</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   participant_id  six_hour_sample_count  unique_dates  agitation_episodes\n",
       "0           73f7c                    190            48                  33\n",
       "1           93c14                    185            47                  12\n",
       "2           c5031                    169            43                  12\n",
       "3           0d5ef                    191            49                  11\n",
       "4           16f4b                    353            89                  11\n",
       "5           7db78                    270            68                   8\n",
       "6           a2849                    234            59                   6\n",
       "7           d93d8                    210            53                   6\n",
       "8           ec812                    288            73                   4\n",
       "9           e4959                    262            66                   4\n",
       "10          b9d58                    354            90                   4\n",
       "11          e8a78                    249            63                   3\n",
       "12          8a835                    295            75                   3\n",
       "13          95899                    174            44                   2\n",
       "14          d7a46                     91            24                   2\n",
       "15          3fb61                    242            62                   2\n",
       "16          6b29b                    269            70                   2\n",
       "17          46286                    125            32                   1\n",
       "18          d263a                    270            68                   1\n",
       "19          96adf                    214            54                   1\n",
       "20          393cb                    266            67                   1\n",
       "21          201d8                    290            73                   1\n",
       "22          0cda9                    302            76                   1\n",
       "23          099bc                    173            44                   1\n",
       "24          eca1f                    215            55                   1\n",
       "25          8d0d4                    270            68                   1\n",
       "26          e87bd                    163            42                   1\n",
       "27          65db4                    202            51                   0\n",
       "28          55cd4                    282            71                   0\n",
       "29          2b131                     19             5                   0\n",
       "30          561af                    122            31                   0\n",
       "31          56b6b                    220            56                   0\n",
       "32          28710                    179            46                   0\n",
       "33          2f54b                    230            58                   0\n",
       "34          0f352                     23             6                   0\n",
       "35          1fbe4                    249            63                   0\n",
       "36          0697d                     11             3                   0\n",
       "37          0efe8                    186            47                   0\n",
       "38          385de                     74            19                   0\n",
       "39          30a32                    258            65                   0\n",
       "40          714d7                    163            42                   0\n",
       "41          c55f8                    327            83                   0\n",
       "42          ab47a                    165            42                   0\n",
       "43          b45c2                    265            68                   0\n",
       "44          a539e                    230            58                   0\n",
       "45          a380e                    103            26                   0\n",
       "46          76230                     19             5                   0\n",
       "47          b0455                     39            10                   0\n",
       "48          d8d97                     27             7                   0\n",
       "49          d44d2                     62            16                   0\n",
       "50          c8574                    206            52                   0\n",
       "51          ca44d                    158            40                   0\n",
       "52          c5785                    266            67                   0\n",
       "53          e2472                    118            31                   0\n",
       "54          f220c                    258            65                   0\n",
       "55          fd100                     15             4                   0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>6h_time_block</th>\n",
       "      <th>agitation_episode_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>06-12</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12-18</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18-24</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  6h_time_block  agitation_episode_count\n",
       "0         06-12                       11\n",
       "1         12-18                       79\n",
       "2         18-24                       45"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Agitation Statistics\n",
    "\n",
    "temp = dataset_06h\n",
    "# temp = dataset_12h\n",
    "\n",
    "temp['agitation'].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "print(f\"Total number of participants: {temp['id'].nunique()}\")\n",
    "print(f\"Total number of participants with at least one agitation episode: {temp[temp['agitation'] == 1]['id'].nunique()}\")\n",
    "print(f\"Total number of participants with no agitation episodes: {temp['id'].nunique() - temp[temp['agitation'] == 1]['id'].nunique()}\")\n",
    "print(f\"Total number of agitation episodes: {(temp['agitation'] == 1).sum()}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ids, total_counts = np.unique(temp['id'], return_counts=True)\n",
    "agitated_ids, agitation_counts = np.unique(temp[temp['agitation'] == 1]['id'], return_counts=True)\n",
    "agitation_map = dict(zip(agitated_ids, agitation_counts))\n",
    "unique_dates = temp.groupby('id')['date'].nunique().reindex(ids, fill_value=0).values\n",
    "participant_summary_df = pd.DataFrame({\n",
    "    'participant_id': ids,\n",
    "    'six_hour_sample_count': total_counts,\n",
    "    'unique_dates': unique_dates,\n",
    "    'agitation_episodes': [agitation_map.get(pid, 0) for pid in ids]\n",
    "})\n",
    "participant_summary_df = participant_summary_df.sort_values(by='agitation_episodes', ascending=False).reset_index(drop=True)\n",
    "display(participant_summary_df)\n",
    "\n",
    "\n",
    "values, counts = np.unique(temp.loc[temp['agitation'] == 1, '6h'], return_counts=True)\n",
    "# values, counts = np.unique(temp.loc[temp['agitation'] == 1, '12h'], return_counts=True)\n",
    "\n",
    "agitation_temporal_df = pd.DataFrame({\n",
    "    '6h_time_block': values,\n",
    "    'agitation_episode_count': counts\n",
    "})\n",
    "display(agitation_temporal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "[0 1 2 3]\n",
      "349\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], shape=(10790,))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0, ..., 55, 55, 55], shape=(10790,))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset_06h\n",
    "# dataset = dataset_12h\n",
    "\n",
    "y = np.array(dataset[[label]]).squeeze()\n",
    "\n",
    "# Binary\n",
    "# y[y == -1] = 0\n",
    "# y[y >= 1] = 1\n",
    "\n",
    "# QUATERNARY\n",
    "y[y == -1] = 0\n",
    "y[y == -10] = 0\n",
    "\n",
    "dataset.drop(['agitation',\n",
    "              'agitation-next',\n",
    "              'agitation-four'],\n",
    "              axis=1, inplace=True)\n",
    "\n",
    "ids = np.array(dataset['id']).squeeze()\n",
    "p = np.unique(ids, return_inverse=True)[1]\n",
    "\n",
    "print(np.isnan(y).sum())\n",
    "y = np.nan_to_num(y, nan=0)\n",
    "print(np.isnan(y).sum())\n",
    "print(np.isnan(p).sum())\n",
    "\n",
    "print(np.unique(y))\n",
    "print(y.sum())\n",
    "display(y)\n",
    "display(p)\n",
    "# display(dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>6h</th>\n",
       "      <th>back-door</th>\n",
       "      <th>bathroom</th>\n",
       "      <th>bedroom</th>\n",
       "      <th>fridge-door</th>\n",
       "      <th>front-door</th>\n",
       "      <th>hallway</th>\n",
       "      <th>kitchen</th>\n",
       "      <th>...</th>\n",
       "      <th>systolic-blood-pressure</th>\n",
       "      <th>total-body-water</th>\n",
       "      <th>skin-temperature</th>\n",
       "      <th>blood-pressure</th>\n",
       "      <th>body-water</th>\n",
       "      <th>pulse</th>\n",
       "      <th>weight</th>\n",
       "      <th>body-temperature-label</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0697d</td>\n",
       "      <td>2019-06-28</td>\n",
       "      <td>00-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0697d</td>\n",
       "      <td>2019-06-28</td>\n",
       "      <td>06-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0697d</td>\n",
       "      <td>2019-06-28</td>\n",
       "      <td>12-18</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>...</td>\n",
       "      <td>165.0</td>\n",
       "      <td>50.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0697d</td>\n",
       "      <td>2019-06-28</td>\n",
       "      <td>18-24</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0697d</td>\n",
       "      <td>2019-06-29</td>\n",
       "      <td>00-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10785</th>\n",
       "      <td>fd100</td>\n",
       "      <td>2019-06-29</td>\n",
       "      <td>12-18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10786</th>\n",
       "      <td>fd100</td>\n",
       "      <td>2019-06-29</td>\n",
       "      <td>18-24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10787</th>\n",
       "      <td>fd100</td>\n",
       "      <td>2019-06-30</td>\n",
       "      <td>00-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10788</th>\n",
       "      <td>fd100</td>\n",
       "      <td>2019-06-30</td>\n",
       "      <td>06-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10789</th>\n",
       "      <td>fd100</td>\n",
       "      <td>2019-06-30</td>\n",
       "      <td>12-18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10790 rows × 66 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id        date     6h  back-door  bathroom  bedroom  fridge-door  \\\n",
       "0      0697d  2019-06-28  00-06        0.0       0.0      0.0          0.0   \n",
       "1      0697d  2019-06-28  06-12        0.0       0.0      0.0          0.0   \n",
       "2      0697d  2019-06-28  12-18       12.0       5.0     18.0         23.0   \n",
       "3      0697d  2019-06-28  18-24        2.0       2.0      6.0          0.0   \n",
       "4      0697d  2019-06-29  00-06        0.0       0.0      3.0          0.0   \n",
       "...      ...         ...    ...        ...       ...      ...          ...   \n",
       "10785  fd100  2019-06-29  12-18        0.0      10.0     19.0         17.0   \n",
       "10786  fd100  2019-06-29  18-24        0.0      13.0     16.0          3.0   \n",
       "10787  fd100  2019-06-30  00-06        0.0       0.0      0.0          0.0   \n",
       "10788  fd100  2019-06-30  06-12        0.0      13.0     34.0          5.0   \n",
       "10789  fd100  2019-06-30  12-18        0.0       3.0     29.0          3.0   \n",
       "\n",
       "       front-door  hallway  kitchen  ...  systolic-blood-pressure  \\\n",
       "0             0.0      0.0      0.0  ...                      NaN   \n",
       "1             0.0      0.0      0.0  ...                      NaN   \n",
       "2            27.0     30.0     57.0  ...                    165.0   \n",
       "3             1.0     10.0     49.0  ...                      NaN   \n",
       "4             0.0      0.0      0.0  ...                      NaN   \n",
       "...           ...      ...      ...  ...                      ...   \n",
       "10785         9.0     25.0     56.0  ...                      NaN   \n",
       "10786         3.0     23.0     22.0  ...                      NaN   \n",
       "10787         0.0      0.0      0.0  ...                      NaN   \n",
       "10788         8.0     17.0     38.0  ...                      NaN   \n",
       "10789         4.0      6.0     50.0  ...                      NaN   \n",
       "\n",
       "       total-body-water  skin-temperature  blood-pressure  body-water  pulse  \\\n",
       "0                   NaN               NaN             0.0         0.0    0.0   \n",
       "1                   NaN               NaN             0.0         0.0    0.0   \n",
       "2                  50.6               NaN             1.0         0.0    1.0   \n",
       "3                   NaN               NaN             0.0         0.0    0.0   \n",
       "4                   NaN               NaN             0.0         0.0    0.0   \n",
       "...                 ...               ...             ...         ...    ...   \n",
       "10785               NaN               NaN             NaN         NaN    NaN   \n",
       "10786               NaN               NaN             NaN         NaN    NaN   \n",
       "10787               NaN               NaN             NaN         NaN    NaN   \n",
       "10788               NaN               NaN             NaN         NaN    NaN   \n",
       "10789               NaN               NaN             NaN         NaN    NaN   \n",
       "\n",
       "       weight  body-temperature-label  age  sex  \n",
       "0         0.0                     0.0   85    1  \n",
       "1         0.0                     0.0   85    1  \n",
       "2         0.0                     0.0   85    1  \n",
       "3         0.0                     0.0   85    1  \n",
       "4         0.0                     0.0   85    1  \n",
       "...       ...                     ...  ...  ...  \n",
       "10785     NaN                     NaN  100    0  \n",
       "10786     NaN                     NaN  100    0  \n",
       "10787     NaN                     NaN  100    0  \n",
       "10788     NaN                     NaN  100    0  \n",
       "10789     NaN                     NaN  100    0  \n",
       "\n",
       "[10790 rows x 66 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id' 'date' '6h' 'back-door' 'bathroom' 'bedroom' 'fridge-door'\n",
      " 'front-door' 'hallway' 'kitchen' 'lounge' 'total-events'\n",
      " 'unique-locations' 'active-location-ratio' 'private-to-public-ratio'\n",
      " 'location-entropy' 'location-dominance-ratio' 'back-and-forth-count'\n",
      " 'num-transitions' 'back-door-count-max' 'back-door-count-mean'\n",
      " 'back-door-count-std' 'back-door-count-sum' 'bathroom-count-max'\n",
      " 'bathroom-count-mean' 'bathroom-count-std' 'bathroom-count-sum'\n",
      " 'bedroom-count-max' 'bedroom-count-mean' 'bedroom-count-std'\n",
      " 'bedroom-count-sum' 'fridge-door-count-max' 'fridge-door-count-mean'\n",
      " 'fridge-door-count-std' 'fridge-door-count-sum' 'front-door-count-max'\n",
      " 'front-door-count-mean' 'front-door-count-std' 'front-door-count-sum'\n",
      " 'hallway-count-max' 'hallway-count-mean' 'hallway-count-std'\n",
      " 'hallway-count-sum' 'kitchen-count-max' 'kitchen-count-mean'\n",
      " 'kitchen-count-std' 'kitchen-count-sum' 'lounge-count-max'\n",
      " 'lounge-count-mean' 'lounge-count-std' 'lounge-count-sum'\n",
      " 'body-temperature' 'body-weight' 'diastolic-blood-pressure' 'heart-rate'\n",
      " 'muscle-mass' 'systolic-blood-pressure' 'total-body-water'\n",
      " 'skin-temperature' 'blood-pressure' 'body-water' 'pulse' 'weight'\n",
      " 'body-temperature-label' 'age' 'sex']\n"
     ]
    }
   ],
   "source": [
    "columns = dataset.columns\n",
    "dataset = dataset[columns]\n",
    "display(dataset)\n",
    "print(dataset.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 32 8 8 2 5 56\n"
     ]
    }
   ],
   "source": [
    "columns_count = ['back-door', 'bathroom', 'bedroom', 'fridge-door',\n",
    "                 'front-door', 'hallway', 'kitchen', 'lounge']\n",
    "\n",
    "columns_statistical = ['back-door-count-max', 'back-door-count-mean', 'back-door-count-std', 'back-door-count-sum',\n",
    "                       'bathroom-count-max', 'bathroom-count-mean', 'bathroom-count-std', 'bathroom-count-sum',\n",
    "                       'bedroom-count-max', 'bedroom-count-mean', 'bedroom-count-std', 'bedroom-count-sum',\n",
    "                       'fridge-door-count-max', 'fridge-door-count-mean', 'fridge-door-count-std', 'fridge-door-count-sum',\n",
    "                       'front-door-count-max', 'front-door-count-mean', 'front-door-count-std', 'front-door-count-sum',\n",
    "                       'hallway-count-max', 'hallway-count-mean', 'hallway-count-std', 'hallway-count-sum',\n",
    "                       'kitchen-count-max', 'kitchen-count-mean', 'kitchen-count-std','kitchen-count-sum',\n",
    "                       'lounge-count-max', 'lounge-count-mean', 'lounge-count-std','lounge-count-sum']\n",
    "\n",
    "columns_contextual = ['total-events', 'unique-locations', 'active-location-ratio', 'private-to-public-ratio',\n",
    "                      'location-entropy', 'location-dominance-ratio', 'back-and-forth-count', 'num-transitions']\n",
    "\n",
    "columns_physiology = ['body-temperature', 'body-weight', 'diastolic-blood-pressure', 'heart-rate',\n",
    "                      'muscle-mass', 'systolic-blood-pressure', 'total-body-water', 'skin-temperature']\n",
    "\n",
    "columns_demographics = ['age', 'sex']\n",
    "\n",
    "columns_labels = ['blood-pressure', 'body-water', 'pulse', 'weight', 'body-temperature-label']\n",
    "\n",
    "# feature_names = columns_count + columns_statistical + columns_contextual + columns_physiology + columns_demographics + columns_labels\n",
    "feature_names = columns_count + columns_statistical + columns_contextual + columns_physiology\n",
    "\n",
    "print(len(columns_count),\n",
    "      len(columns_statistical),\n",
    "      len(columns_contextual),\n",
    "      len(columns_physiology),\n",
    "      len(columns_demographics),\n",
    "      len(columns_labels),\n",
    "      len(feature_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134585\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Imputation\n",
    "import utils_data\n",
    "importlib.reload(utils_data)\n",
    "from utils_data import hierarchical_imputation\n",
    "\n",
    "print(dataset.isna().sum().sum())\n",
    "\n",
    "dataset = hierarchical_imputation(dataset)\n",
    "\n",
    "print(dataset.isna().sum().sum())\n",
    "\n",
    "dataset.drop([\n",
    "    'id',\n",
    "    'date',\n",
    "    \n",
    "    '6h',\n",
    "    # '12h',\n",
    "\n",
    "    ], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10790, 63)\n",
      "(10790, 58)\n",
      "(10790, 61)\n",
      "(10790, 56)\n",
      "(10790, 48)\n",
      "(10790, 40)\n",
      "(10790, 8)\n"
     ]
    }
   ],
   "source": [
    "# Ablation\n",
    "dataset_CNT_STA_CTX_PHY_DMG_LBL = dataset\n",
    "dataset_CNT_STA_CTX_PHY_DMG_LBL.to_csv(os.path.join(root,   'dataset_06h_imputed_CNT_STA_CTX_PHY_DMG_LBL.csv'), index=False)\n",
    "print(dataset_CNT_STA_CTX_PHY_DMG_LBL.shape)\n",
    "\n",
    "dataset_CNT_STA_CTX_PHY_DMG = dataset.drop(columns_labels, axis=1, inplace=False)\n",
    "dataset_CNT_STA_CTX_PHY_DMG.to_csv(os.path.join(root,       'dataset_06h_imputed_CNT_STA_CTX_PHY_DMG.csv'), index=False)\n",
    "print(dataset_CNT_STA_CTX_PHY_DMG.shape)\n",
    "\n",
    "dataset_CNT_STA_CTX_PHY_LBL = dataset.drop(columns_demographics, axis=1, inplace=False)\n",
    "dataset_CNT_STA_CTX_PHY_LBL.to_csv(os.path.join(root,       'dataset_06h_imputed_CNT_STA_CTX_PHY_LBL.csv'), index=False)\n",
    "print(dataset_CNT_STA_CTX_PHY_LBL.shape)\n",
    "\n",
    "dataset_CNT_STA_CTX_PHY = dataset.drop(columns_demographics +\n",
    "                                       columns_labels, axis=1, inplace=False)\n",
    "dataset_CNT_STA_CTX_PHY.to_csv(os.path.join(root,       'dataset_06h_imputed_CNT_STA_CTX_PHY.csv'), index=False)\n",
    "print(dataset_CNT_STA_CTX_PHY.shape)\n",
    "\n",
    "dataset_CNT_STA_CTX = dataset.drop(columns_physiology +\n",
    "                                   columns_demographics +\n",
    "                                   columns_labels, axis=1, inplace=False)\n",
    "dataset_CNT_STA_CTX.to_csv(os.path.join(root,       'dataset_06h_imputed_CNT_STA_CTX.csv'), index=False)\n",
    "print(dataset_CNT_STA_CTX.shape)\n",
    "\n",
    "\n",
    "dataset_CNT_STA = dataset.drop(columns_contextual +\n",
    "                               columns_physiology +\n",
    "                               columns_demographics +\n",
    "                               columns_labels, axis=1, inplace=False)\n",
    "dataset_CNT_STA.to_csv(os.path.join(root,           'dataset_06h_imputed_CNT_STA.csv'), index=False)\n",
    "print(dataset_CNT_STA.shape)\n",
    "\n",
    "dataset_CNT = dataset.drop(columns_statistical +\n",
    "                           columns_contextual +\n",
    "                           columns_physiology +\n",
    "                           columns_demographics +\n",
    "                           columns_labels, axis=1, inplace=False)\n",
    "dataset_CNT.to_csv(os.path.join(root,           'dataset_06h_imputed_CNT.csv'), index=False)\n",
    "print(dataset_CNT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ablation\n",
    "# dataset_CNT_STA_CTX_PHY_DMG_LBL = dataset\n",
    "# dataset_CNT_STA_CTX_PHY_DMG_LBL.to_csv(os.path.join(root,   'dataset_12h_imputed_CNT_STA_CTX_PHY_DMG_LBL.csv'), index=False)\n",
    "# print(dataset_CNT_STA_CTX_PHY_DMG_LBL.shape)\n",
    "\n",
    "# dataset_CNT_STA_CTX_PHY_DMG = dataset.drop(columns_labels, axis=1, inplace=False)\n",
    "# dataset_CNT_STA_CTX_PHY_DMG.to_csv(os.path.join(root,       'dataset_12h_imputed_CNT_STA_CTX_PHY_DMG.csv'), index=False)\n",
    "# print(dataset_CNT_STA_CTX_PHY_DMG.shape)\n",
    "\n",
    "# dataset_CNT_STA_CTX_PHY = dataset.drop(columns_demographics, axis=1, inplace=False)\n",
    "# dataset_CNT_STA_CTX_PHY.to_csv(os.path.join(root,       'dataset_12h_imputed_CNT_STA_CTX_PHY.csv'), index=False)\n",
    "# print(dataset_CNT_STA_CTX_PHY.shape)\n",
    "\n",
    "# dataset_CNT_STA_CTX = dataset.drop(columns_physiology, axis=1, inplace=False)\n",
    "# dataset_CNT_STA_CTX.to_csv(os.path.join(root,       'dataset_12h_imputed_CNT_STA_CTX.csv'), index=False)\n",
    "# print(dataset_CNT_STA_CTX.shape)\n",
    "\n",
    "\n",
    "# dataset_CNT_STA = dataset.drop(columns_contextual + columns_physiology, axis=1, inplace=False)\n",
    "# dataset_CNT_STA.to_csv(os.path.join(root,           'dataset_12h_imputed_CNT_STA.csv'), index=False)\n",
    "# print(dataset_CNT_STA.shape)\n",
    "\n",
    "# dataset_CNT = dataset.drop(columns_statistical + columns_contextual + columns_physiology, axis=1, inplace=False)\n",
    "# dataset_CNT.to_csv(os.path.join(root,           'dataset_12h_imputed_CNT.csv'), index=False)\n",
    "# print(dataset_CNT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ablation\n",
    "# dataset_CNT_STA_CTX_PHY_DMG_LBL = dataset\n",
    "# dataset_CNT_STA_CTX_PHY_DMG_LBL.to_csv(os.path.join(root,   'dataset_24h_imputed_CNT_STA_CTX_PHY_DMG_LBL.csv'), index=False)\n",
    "# print(dataset_CNT_STA_CTX_PHY_DMG_LBL.shape)\n",
    "\n",
    "# dataset_CNT_STA_CTX_PHY_DMG = dataset.drop(columns_labels, axis=1, inplace=False)\n",
    "# dataset_CNT_STA_CTX_PHY_DMG.to_csv(os.path.join(root,       'dataset_24h_imputed_CNT_STA_CTX_PHY_DMG.csv'), index=False)\n",
    "# print(dataset_CNT_STA_CTX_PHY_DMG.shape)\n",
    "\n",
    "# dataset_CNT_STA_CTX_PHY = dataset.drop(columns_demographics, axis=1, inplace=False)\n",
    "# dataset_CNT_STA_CTX_PHY.to_csv(os.path.join(root,       'dataset_24h_imputed_CNT_STA_CTX_PHY.csv'), index=False)\n",
    "# print(dataset_CNT_STA_CTX_PHY.shape)\n",
    "\n",
    "# dataset_CNT_STA_CTX = dataset.drop(columns_physiology, axis=1, inplace=False)\n",
    "# dataset_CNT_STA_CTX.to_csv(os.path.join(root,       'dataset_24h_imputed_CNT_STA_CTX.csv'), index=False)\n",
    "# print(dataset_CNT_STA_CTX.shape)\n",
    "\n",
    "\n",
    "# dataset_CNT_STA = dataset.drop(columns_contextual + columns_physiology, axis=1, inplace=False)\n",
    "# dataset_CNT_STA.to_csv(os.path.join(root,           'dataset_24h_imputed_CNT_STA.csv'), index=False)\n",
    "# print(dataset_CNT_STA.shape)\n",
    "\n",
    "# dataset_CNT = dataset.drop(columns_statistical + columns_contextual + columns_physiology, axis=1, inplace=False)\n",
    "# dataset_CNT.to_csv(os.path.join(root,           'dataset_24h_imputed_CNT.csv'), index=False)\n",
    "# print(dataset_CNT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y.shape)\n",
    "# display(dataset)\n",
    "\n",
    "# dataset.to_csv(os.path.join(root, 'dataset_06h_imputed.csv'))\n",
    "# np.save('dataset_06h_y.npy', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['back-door' 'bathroom' 'bedroom' 'fridge-door' 'front-door' 'hallway'\n",
      " 'kitchen' 'lounge' 'total-events' 'unique-locations'\n",
      " 'active-location-ratio' 'private-to-public-ratio' 'location-entropy'\n",
      " 'location-dominance-ratio' 'back-and-forth-count' 'num-transitions'\n",
      " 'back-door-count-max' 'back-door-count-mean' 'back-door-count-std'\n",
      " 'back-door-count-sum' 'bathroom-count-max' 'bathroom-count-mean'\n",
      " 'bathroom-count-std' 'bathroom-count-sum' 'bedroom-count-max'\n",
      " 'bedroom-count-mean' 'bedroom-count-std' 'bedroom-count-sum'\n",
      " 'fridge-door-count-max' 'fridge-door-count-mean' 'fridge-door-count-std'\n",
      " 'fridge-door-count-sum' 'front-door-count-max' 'front-door-count-mean'\n",
      " 'front-door-count-std' 'front-door-count-sum' 'hallway-count-max'\n",
      " 'hallway-count-mean' 'hallway-count-std' 'hallway-count-sum'\n",
      " 'kitchen-count-max' 'kitchen-count-mean' 'kitchen-count-std'\n",
      " 'kitchen-count-sum' 'lounge-count-max' 'lounge-count-mean'\n",
      " 'lounge-count-std' 'lounge-count-sum' 'body-temperature' 'body-weight'\n",
      " 'diastolic-blood-pressure' 'heart-rate' 'muscle-mass'\n",
      " 'systolic-blood-pressure' 'total-body-water' 'skin-temperature']\n",
      "(10790,)\n",
      "(10790, 56)\n",
      "[0 1 2 3]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>back-door</th>\n",
       "      <th>bathroom</th>\n",
       "      <th>bedroom</th>\n",
       "      <th>fridge-door</th>\n",
       "      <th>front-door</th>\n",
       "      <th>hallway</th>\n",
       "      <th>kitchen</th>\n",
       "      <th>lounge</th>\n",
       "      <th>total-events</th>\n",
       "      <th>unique-locations</th>\n",
       "      <th>...</th>\n",
       "      <th>lounge-count-std</th>\n",
       "      <th>lounge-count-sum</th>\n",
       "      <th>body-temperature</th>\n",
       "      <th>body-weight</th>\n",
       "      <th>diastolic-blood-pressure</th>\n",
       "      <th>heart-rate</th>\n",
       "      <th>muscle-mass</th>\n",
       "      <th>systolic-blood-pressure</th>\n",
       "      <th>total-body-water</th>\n",
       "      <th>skin-temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.5675</td>\n",
       "      <td>69.1708</td>\n",
       "      <td>75.6146</td>\n",
       "      <td>72.1250</td>\n",
       "      <td>47.5853</td>\n",
       "      <td>136.4271</td>\n",
       "      <td>49.8206</td>\n",
       "      <td>34.2843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.5675</td>\n",
       "      <td>69.1708</td>\n",
       "      <td>75.6146</td>\n",
       "      <td>72.1250</td>\n",
       "      <td>47.5853</td>\n",
       "      <td>136.4271</td>\n",
       "      <td>49.8206</td>\n",
       "      <td>34.2843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.6999</td>\n",
       "      <td>49.0</td>\n",
       "      <td>36.1864</td>\n",
       "      <td>86.3000</td>\n",
       "      <td>82.0000</td>\n",
       "      <td>42.0000</td>\n",
       "      <td>64.5000</td>\n",
       "      <td>165.0000</td>\n",
       "      <td>50.6000</td>\n",
       "      <td>34.2843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.8130</td>\n",
       "      <td>31.0</td>\n",
       "      <td>36.5675</td>\n",
       "      <td>69.1708</td>\n",
       "      <td>75.6146</td>\n",
       "      <td>72.1250</td>\n",
       "      <td>47.5853</td>\n",
       "      <td>136.4271</td>\n",
       "      <td>49.8206</td>\n",
       "      <td>34.2843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.5425</td>\n",
       "      <td>71.7159</td>\n",
       "      <td>74.1458</td>\n",
       "      <td>76.0469</td>\n",
       "      <td>48.6600</td>\n",
       "      <td>130.7188</td>\n",
       "      <td>51.7600</td>\n",
       "      <td>34.5600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10785</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>182.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4828</td>\n",
       "      <td>46.0</td>\n",
       "      <td>36.5425</td>\n",
       "      <td>71.7159</td>\n",
       "      <td>74.1458</td>\n",
       "      <td>76.0469</td>\n",
       "      <td>48.6600</td>\n",
       "      <td>130.7188</td>\n",
       "      <td>51.7600</td>\n",
       "      <td>34.5600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10786</th>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.7583</td>\n",
       "      <td>34.0</td>\n",
       "      <td>36.2400</td>\n",
       "      <td>71.7159</td>\n",
       "      <td>74.1458</td>\n",
       "      <td>76.0469</td>\n",
       "      <td>48.6600</td>\n",
       "      <td>130.7188</td>\n",
       "      <td>51.7600</td>\n",
       "      <td>34.5600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10787</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.7593</td>\n",
       "      <td>68.9053</td>\n",
       "      <td>73.7692</td>\n",
       "      <td>70.3526</td>\n",
       "      <td>46.2964</td>\n",
       "      <td>134.0000</td>\n",
       "      <td>51.1929</td>\n",
       "      <td>34.6478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10788</th>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.4683</td>\n",
       "      <td>31.0</td>\n",
       "      <td>36.7593</td>\n",
       "      <td>68.9053</td>\n",
       "      <td>73.7692</td>\n",
       "      <td>70.3526</td>\n",
       "      <td>46.2964</td>\n",
       "      <td>134.0000</td>\n",
       "      <td>51.1929</td>\n",
       "      <td>34.6478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10789</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.9766</td>\n",
       "      <td>32.0</td>\n",
       "      <td>36.7593</td>\n",
       "      <td>68.9053</td>\n",
       "      <td>73.7692</td>\n",
       "      <td>70.3526</td>\n",
       "      <td>46.2964</td>\n",
       "      <td>134.0000</td>\n",
       "      <td>51.1929</td>\n",
       "      <td>34.6478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10790 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       back-door  bathroom  bedroom  fridge-door  front-door  hallway  \\\n",
       "0            0.0       0.0      0.0          0.0         0.0      0.0   \n",
       "1            0.0       0.0      0.0          0.0         0.0      0.0   \n",
       "2           12.0       5.0     18.0         23.0        27.0     30.0   \n",
       "3            2.0       2.0      6.0          0.0         1.0     10.0   \n",
       "4            0.0       0.0      3.0          0.0         0.0      0.0   \n",
       "...          ...       ...      ...          ...         ...      ...   \n",
       "10785        0.0      10.0     19.0         17.0         9.0     25.0   \n",
       "10786        0.0      13.0     16.0          3.0         3.0     23.0   \n",
       "10787        0.0       0.0      0.0          0.0         0.0      0.0   \n",
       "10788        0.0      13.0     34.0          5.0         8.0     17.0   \n",
       "10789        0.0       3.0     29.0          3.0         4.0      6.0   \n",
       "\n",
       "       kitchen  lounge  total-events  unique-locations  ...  lounge-count-std  \\\n",
       "0          0.0     0.0           0.0               0.0  ...            0.0000   \n",
       "1          0.0     0.0           0.0               0.0  ...            0.0000   \n",
       "2         57.0    49.0         221.0               8.0  ...            4.6999   \n",
       "3         49.0    31.0         101.0               7.0  ...            3.8130   \n",
       "4          0.0     0.0           3.0               1.0  ...            0.0000   \n",
       "...        ...     ...           ...               ...  ...               ...   \n",
       "10785     56.0    46.0         182.0               7.0  ...            4.4828   \n",
       "10786     22.0    34.0         114.0               7.0  ...            3.7583   \n",
       "10787      0.0     0.0           0.0               0.0  ...            0.0000   \n",
       "10788     38.0    31.0         146.0               7.0  ...            3.4683   \n",
       "10789     50.0    32.0         127.0               7.0  ...            2.9766   \n",
       "\n",
       "       lounge-count-sum  body-temperature  body-weight  \\\n",
       "0                   0.0           36.5675      69.1708   \n",
       "1                   0.0           36.5675      69.1708   \n",
       "2                  49.0           36.1864      86.3000   \n",
       "3                  31.0           36.5675      69.1708   \n",
       "4                   0.0           36.5425      71.7159   \n",
       "...                 ...               ...          ...   \n",
       "10785              46.0           36.5425      71.7159   \n",
       "10786              34.0           36.2400      71.7159   \n",
       "10787               0.0           36.7593      68.9053   \n",
       "10788              31.0           36.7593      68.9053   \n",
       "10789              32.0           36.7593      68.9053   \n",
       "\n",
       "       diastolic-blood-pressure  heart-rate  muscle-mass  \\\n",
       "0                       75.6146     72.1250      47.5853   \n",
       "1                       75.6146     72.1250      47.5853   \n",
       "2                       82.0000     42.0000      64.5000   \n",
       "3                       75.6146     72.1250      47.5853   \n",
       "4                       74.1458     76.0469      48.6600   \n",
       "...                         ...         ...          ...   \n",
       "10785                   74.1458     76.0469      48.6600   \n",
       "10786                   74.1458     76.0469      48.6600   \n",
       "10787                   73.7692     70.3526      46.2964   \n",
       "10788                   73.7692     70.3526      46.2964   \n",
       "10789                   73.7692     70.3526      46.2964   \n",
       "\n",
       "       systolic-blood-pressure  total-body-water  skin-temperature  \n",
       "0                     136.4271           49.8206           34.2843  \n",
       "1                     136.4271           49.8206           34.2843  \n",
       "2                     165.0000           50.6000           34.2843  \n",
       "3                     136.4271           49.8206           34.2843  \n",
       "4                     130.7188           51.7600           34.5600  \n",
       "...                        ...               ...               ...  \n",
       "10785                 130.7188           51.7600           34.5600  \n",
       "10786                 130.7188           51.7600           34.5600  \n",
       "10787                 134.0000           51.1929           34.6478  \n",
       "10788                 134.0000           51.1929           34.6478  \n",
       "10789                 134.0000           51.1929           34.6478  \n",
       "\n",
       "[10790 rows x 56 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dataset\n",
    "\n",
    "dataset = pd.read_csv(os.path.join(root, 'dataset_06h_imputed_CNT_STA_CTX_PHY.csv'))\n",
    "\n",
    "# dataset.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "# y = np.load('dataset_06h_y.npy')\n",
    "\n",
    "dataset = dataset[dataset.columns]\n",
    "x = np.array(dataset)\n",
    "\n",
    "\n",
    "print(dataset.columns.values)\n",
    "print(y.shape)\n",
    "print(x.shape)\n",
    "print(np.unique(y))\n",
    "display(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10779 11\n",
      "1 10617 173\n",
      "2 10488 302\n",
      "3 10599 191\n",
      "4 10604 186\n",
      "5 10767 23\n",
      "6 10437 353\n",
      "7 10541 249\n",
      "8 10500 290\n",
      "9 10611 179\n",
      "10 10771 19\n",
      "11 10560 230\n",
      "12 10532 258\n",
      "13 10716 74\n",
      "14 10524 266\n",
      "15 10548 242\n",
      "16 10665 125\n",
      "17 10508 282\n",
      "18 10668 122\n",
      "19 10570 220\n",
      "20 10588 202\n",
      "21 10521 269\n",
      "22 10627 163\n",
      "23 10600 190\n",
      "24 10771 19\n",
      "25 10520 270\n",
      "26 10495 295\n",
      "27 10520 270\n",
      "28 10605 185\n",
      "29 10616 174\n",
      "30 10576 214\n",
      "31 10556 234\n",
      "32 10687 103\n",
      "33 10560 230\n",
      "34 10625 165\n",
      "35 10751 39\n",
      "36 10525 265\n",
      "37 10436 354\n",
      "38 10621 169\n",
      "39 10463 327\n",
      "40 10524 266\n",
      "41 10584 206\n",
      "42 10632 158\n",
      "43 10520 270\n",
      "44 10728 62\n",
      "45 10699 91\n",
      "46 10763 27\n",
      "47 10580 210\n",
      "48 10672 118\n",
      "49 10528 262\n",
      "50 10627 163\n",
      "51 10541 249\n",
      "52 10502 288\n",
      "53 10575 215\n",
      "54 10532 258\n",
      "55 10775 15\n",
      "[[10563     3     0     0]\n",
      " [  116     0     0     1]\n",
      " [   89     0     0     0]\n",
      " [   18     0     0     0]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auc-roc-macro</th>\n",
       "      <th>auc-roc-weighted</th>\n",
       "      <th>auc-pr-macro</th>\n",
       "      <th>auc-pr-weighted</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>cm-0</th>\n",
       "      <th>cm-1</th>\n",
       "      <th>cm-2</th>\n",
       "      <th>cm-3</th>\n",
       "      <th>...</th>\n",
       "      <th>cm-6</th>\n",
       "      <th>cm-7</th>\n",
       "      <th>cm-8</th>\n",
       "      <th>cm-9</th>\n",
       "      <th>cm-10</th>\n",
       "      <th>cm-11</th>\n",
       "      <th>cm-12</th>\n",
       "      <th>cm-13</th>\n",
       "      <th>cm-14</th>\n",
       "      <th>cm-15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8669</td>\n",
       "      <td>0.8456</td>\n",
       "      <td>0.3018</td>\n",
       "      <td>0.9721</td>\n",
       "      <td>0.9790</td>\n",
       "      <td>[10563 3 0 0]\\n[116 0 0 1]\\n[89 0 0 0]\\n[18 0 ...</td>\n",
       "      <td>10563</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  auc-roc-macro auc-roc-weighted auc-pr-macro auc-pr-weighted accuracy  \\\n",
       "0        0.8669           0.8456       0.3018          0.9721   0.9790   \n",
       "\n",
       "                                    confusion_matrix   cm-0 cm-1 cm-2 cm-3  \\\n",
       "0  [10563 3 0 0]\\n[116 0 0 1]\\n[89 0 0 0]\\n[18 0 ...  10563    3    0    0   \n",
       "\n",
       "   ... cm-6 cm-7 cm-8 cm-9 cm-10 cm-11 cm-12 cm-13 cm-14 cm-15  \n",
       "0  ...    0    1   89    0     0     0    18     0     0     0  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LightGBM\n",
    "\n",
    "name = 'LightGBM'.lower()\n",
    "\n",
    "Y_TRUES = np.empty([0])\n",
    "Y_PROBS = []\n",
    "Y_PREDS = np.empty([0])\n",
    "\n",
    "\n",
    "\n",
    "# Explainability\n",
    "importances_list = []\n",
    "shap_list = []\n",
    "x_test_list = []\n",
    "\n",
    "\n",
    "if kfold:\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    split_iterator = cv.split(x)\n",
    "else:\n",
    "    cv = LeavePGroupsOut(n_groups=1)\n",
    "    split_iterator = cv.split(x, y, groups=p)\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(split_iterator, start=1):\n",
    "    participant = i if kfold else np.unique(p[test_idx])[0]\n",
    "\n",
    "\n",
    "\n",
    "    x_train, x_test = x[train_idx], x[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    print(participant, x_train.shape[0], x_test.shape[0])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    normalizer = MinMaxScaler()\n",
    "    normalizer.fit(x_train)\n",
    "    x_train = normalizer.transform(x_train)\n",
    "    x_test = normalizer.transform(x_test)\n",
    "\n",
    "    # Oversample class 1\n",
    "\n",
    "    # ros = RandomOverSampler(sampling_strategy=.1, random_state=seed)\n",
    "    # x_train, y_train = ros.fit_resample(x_train, y_train)\n",
    "\n",
    "    # 1. Check original class distribution\n",
    "    # print(\"Original class distribution:\", Counter(y_train))\n",
    "    # # 2. Define the minority class (adjust if needed)\n",
    "    # minority_class = 1  # change this if your minority class label is different\n",
    "    # current_minority_count = sum(y_train == minority_class)\n",
    "    # # 3. Define desired new total count for the minority class (10x)\n",
    "    # target_minority_count = current_minority_count * 10\n",
    "    # # 4. Setup SMOTE with custom sampling strategy\n",
    "    # smote = SMOTE(sampling_strategy={minority_class: target_minority_count}, random_state=seed)\n",
    "    # # 5. Fit and resample\n",
    "    # x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "    # # 6. Confirm new distribution\n",
    "    # print(\"Resampled class distribution:\", Counter(y_train))\n",
    "\n",
    "\n",
    "\n",
    "    # ----- LightGBM\n",
    "    train_data = lgb.Dataset(x_train, label=y_train)\n",
    "    test_data = lgb.Dataset(x_test, label=y_test, reference=train_data)\n",
    "    params = {\n",
    "        'verbose': -1,  # 👈 turn off training output\n",
    "        'objective': 'multiclass',\n",
    "        'num_class': 4,  # Specify number of classes\n",
    "        'metric': 'multi_logloss',  # or 'auc' if you prefer\n",
    "        'num_leaves': 64,\n",
    "        'learning_rate': 0.01,\n",
    "        'n_estimators': 100,\n",
    "        \n",
    "        'is_unbalance': weighted  # Automatically balances positive and negative classes\n",
    "\n",
    "        # 'scale_pos_weight': (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    }\n",
    "    model = lgb.train(params, train_data, valid_sets=[train_data, test_data])\n",
    "    y_probs = model.predict(x_test, num_iteration=model.best_iteration)\n",
    "\n",
    "\n",
    "    Y_TRUES = np.append(Y_TRUES, y_test)\n",
    "    Y_PROBS.append(y_probs)\n",
    "    Y_PREDS = np.append(Y_PREDS, np.argmax(y_probs, axis=1))\n",
    "\n",
    "\n",
    "\n",
    "    # Explainability\n",
    "    # importances = model.feature_importance(importance_type='gain')\n",
    "    importances = model.feature_importance(importance_type='split')\n",
    "    importances_list.append(importances)\n",
    "\n",
    "\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(x_test)\n",
    "    shap_list.append(shap_values)\n",
    "    x_test_list.append(x_test)\n",
    "\n",
    "\n",
    "Y_PROBS = np.concatenate(Y_PROBS, axis=0)\n",
    "\n",
    "\n",
    "indx = Y_TRUES.argsort()\n",
    "Y_TRUES = Y_TRUES[indx]\n",
    "Y_PROBS = Y_PROBS[indx]\n",
    "Y_PREDS = Y_PREDS[indx]\n",
    "\n",
    "\n",
    "# Evaluation metrics (multiclass)\n",
    "auc_roc_macro = roc_auc_score(Y_TRUES, Y_PROBS, multi_class='ovr', average='macro')\n",
    "auc_roc_weighted = roc_auc_score(Y_TRUES, Y_PROBS, multi_class='ovr', average='weighted')\n",
    "auc_pr_macro = average_precision_score(Y_TRUES, Y_PROBS, average='macro')\n",
    "auc_pr_weighted = average_precision_score(Y_TRUES, Y_PROBS, average='weighted')\n",
    "acc = accuracy_score(Y_TRUES, Y_PREDS)\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(Y_TRUES, Y_PREDS)\n",
    "print(cm)\n",
    "cm_str = '\\n'.join(['[' + ' '.join(map(str, row)) + ']' for row in cm])\n",
    "\n",
    "cm_flat = cm.flatten()\n",
    "\n",
    "results = {\n",
    "    'auc-roc-macro': f\"{auc_roc_macro:.4f}\",\n",
    "    'auc-roc-weighted': f\"{auc_roc_weighted:.4f}\",\n",
    "    'auc-pr-macro': f\"{auc_pr_macro:.4f}\",\n",
    "    'auc-pr-weighted': f\"{auc_pr_weighted:.4f}\",\n",
    "    'accuracy': f\"{acc:.4f}\",\n",
    "    'confusion_matrix': cm_str\n",
    "}\n",
    "\n",
    "for i, val in enumerate(cm_flat):\n",
    "    results[f'cm-{i}'] = str(int(val))\n",
    "\n",
    "results = pd.DataFrame([results])\n",
    "results.to_csv(os.path.join(results_dir, name + '.csv'), index=False)\n",
    "display(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Explainability\n",
    "\n",
    "# print(len(feature_names), np.vstack(importances_list).mean(axis=0).round(4).shape, x.shape, y.shape, x_test.shape, y_test.shape)\n",
    "\n",
    "# importance_df = pd.DataFrame({\n",
    "#     'feature': feature_names,\n",
    "#     'importance': np.vstack(importances_list).mean(axis=0).round(4)\n",
    "# }).sort_values(by='importance', ascending=False)\n",
    "# display(importance_df)\n",
    "# importance_df.to_csv('importance.csv', index=False)\n",
    "\n",
    "\n",
    "# # Create DataFrame for summary\n",
    "# shap_df = pd.DataFrame({\n",
    "#     'feature': feature_names,\n",
    "#     'mean_abs_shap': np.abs(np.vstack(shap_list)).mean(axis=0).round(4)\n",
    "# }).sort_values(by='mean_abs_shap', ascending=False)\n",
    "# shap_df.to_csv('importance-shap.csv', index=False)\n",
    "\n",
    "# print(np.vstack(x_test_list).shape)\n",
    "\n",
    "# shap.summary_plot(shap_values, pd.DataFrame(x_test, columns=feature_names), max_display=16)\n",
    "# shap.summary_plot(np.vstack(shap_list), pd.DataFrame(np.vstack(x_test_list), columns=feature_names), max_display=16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10779 11\n",
      "1 10617 173\n",
      "2 10488 302\n",
      "3 10599 191\n",
      "4 10604 186\n",
      "5 10767 23\n",
      "6 10437 353\n",
      "7 10541 249\n",
      "8 10500 290\n",
      "9 10611 179\n",
      "10 10771 19\n",
      "11 10560 230\n",
      "12 10532 258\n",
      "13 10716 74\n",
      "14 10524 266\n",
      "15 10548 242\n",
      "16 10665 125\n",
      "17 10508 282\n",
      "18 10668 122\n",
      "19 10570 220\n",
      "20 10588 202\n",
      "21 10521 269\n",
      "22 10627 163\n",
      "23 10600 190\n",
      "24 10771 19\n",
      "25 10520 270\n",
      "26 10495 295\n",
      "27 10520 270\n",
      "28 10605 185\n",
      "29 10616 174\n",
      "30 10576 214\n",
      "31 10556 234\n",
      "32 10687 103\n",
      "33 10560 230\n",
      "34 10625 165\n",
      "35 10751 39\n",
      "36 10525 265\n",
      "37 10436 354\n",
      "38 10621 169\n",
      "39 10463 327\n",
      "40 10524 266\n",
      "41 10584 206\n",
      "42 10632 158\n",
      "43 10520 270\n",
      "44 10728 62\n",
      "45 10699 91\n",
      "46 10763 27\n",
      "47 10580 210\n",
      "48 10672 118\n",
      "49 10528 262\n",
      "50 10627 163\n",
      "51 10541 249\n",
      "52 10502 288\n",
      "53 10575 215\n",
      "54 10532 258\n",
      "55 10775 15\n",
      "[[10563     3     0     0]\n",
      " [  115     2     0     0]\n",
      " [   86     3     0     0]\n",
      " [   17     1     0     0]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auc-roc-macro</th>\n",
       "      <th>auc-roc-weighted</th>\n",
       "      <th>auc-pr-macro</th>\n",
       "      <th>auc-pr-weighted</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>cm-0</th>\n",
       "      <th>cm-1</th>\n",
       "      <th>cm-2</th>\n",
       "      <th>cm-3</th>\n",
       "      <th>...</th>\n",
       "      <th>cm-6</th>\n",
       "      <th>cm-7</th>\n",
       "      <th>cm-8</th>\n",
       "      <th>cm-9</th>\n",
       "      <th>cm-10</th>\n",
       "      <th>cm-11</th>\n",
       "      <th>cm-12</th>\n",
       "      <th>cm-13</th>\n",
       "      <th>cm-14</th>\n",
       "      <th>cm-15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.7820</td>\n",
       "      <td>0.7197</td>\n",
       "      <td>0.3023</td>\n",
       "      <td>0.9719</td>\n",
       "      <td>0.9791</td>\n",
       "      <td>[10563 3 0 0]\\n[115 2 0 0]\\n[86 3 0 0]\\n[17 1 ...</td>\n",
       "      <td>10563</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  auc-roc-macro auc-roc-weighted auc-pr-macro auc-pr-weighted accuracy  \\\n",
       "0        0.7820           0.7197       0.3023          0.9719   0.9791   \n",
       "\n",
       "                                    confusion_matrix   cm-0 cm-1 cm-2 cm-3  \\\n",
       "0  [10563 3 0 0]\\n[115 2 0 0]\\n[86 3 0 0]\\n[17 1 ...  10563    3    0    0   \n",
       "\n",
       "   ... cm-6 cm-7 cm-8 cm-9 cm-10 cm-11 cm-12 cm-13 cm-14 cm-15  \n",
       "0  ...    0    0   86    3     0     0    17     1     0     0  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Transformer\n",
    "\n",
    "if not weighted:\n",
    "\n",
    "    name = 'Transformer'.lower()\n",
    "\n",
    "    Y_TRUES = np.empty([0])\n",
    "    Y_PROBS = []\n",
    "    Y_PREDS = np.empty([0])\n",
    "\n",
    "\n",
    "    if kfold:\n",
    "        cv = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "        split_iterator = cv.split(x)\n",
    "    else:\n",
    "        cv = LeavePGroupsOut(n_groups=1)\n",
    "        split_iterator = cv.split(x, y, groups=p)\n",
    "\n",
    "    for i, (train_idx, test_idx) in enumerate(split_iterator, start=1):\n",
    "        participant = i if kfold else np.unique(p[test_idx])[0]\n",
    "\n",
    "        x_train, x_test = x[train_idx], x[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        print(participant, x_train.shape[0], x_test.shape[0])\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(x_train)\n",
    "        x_train = scaler.transform(x_train)\n",
    "        x_test = scaler.transform(x_test)\n",
    "\n",
    "        normalizer = MinMaxScaler()\n",
    "        normalizer.fit(x_train)\n",
    "        x_train = normalizer.transform(x_train)\n",
    "        x_test = normalizer.transform(x_test)\n",
    "\n",
    "        # Oversample class 1\n",
    "\n",
    "        # ros = RandomOverSampler(sampling_strategy=.1, random_state=seed)\n",
    "        # x_train, y_train = ros.fit_resample(x_train, y_train)\n",
    "\n",
    "        # # 1. Check original class distribution\n",
    "        # print(\"Original class distribution:\", Counter(y_train))\n",
    "        # # 2. Define the minority class (adjust if needed)\n",
    "        # minority_class = 1  # change this if your minority class label is different\n",
    "        # current_minority_count = sum(y_train == minority_class)\n",
    "        # # 3. Define desired new total count for the minority class (10x)\n",
    "        # target_minority_count = current_minority_count * 10\n",
    "        # # 4. Setup SMOTE with custom sampling strategy\n",
    "        # smote = SMOTE(sampling_strategy={minority_class: target_minority_count}, random_state=seed)\n",
    "        # # 5. Fit and resample\n",
    "        # x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "        # # 6. Confirm new distribution\n",
    "        # print(\"Resampled class distribution:\", Counter(y_train))\n",
    "\n",
    "\n",
    "\n",
    "        # ----- LightGBM\n",
    "        # train_data = lgb.Dataset(x_train, label=y_train)\n",
    "        # test_data = lgb.Dataset(x_test, label=y_test, reference=train_data)\n",
    "        # params = {\n",
    "        #     'verbose': -1,  # 👈 turn off training output\n",
    "        #     'objective': 'binary',\n",
    "        #     'metric': 'binary_logloss',  # or 'auc' if you prefer\n",
    "        #     'num_leaves': 64,\n",
    "        #     'learning_rate': 0.01,\n",
    "        #     'n_estimators': 100,\n",
    "        #     # 'is_unbalance': True  # Automatically balances positive and negative classes\n",
    "        #     # 'scale_pos_weight': (y_train == 0).sum() / (y_train == 1).sum()\n",
    "        # }\n",
    "        # bst = lgb.train(params, train_data, valid_sets=[train_data, test_data])\n",
    "        # y_probs = bst.predict(x_test, num_iteration=bst.best_iteration)\n",
    "\n",
    "\n",
    "        # ----- Transformer\n",
    "        model = TabPFNClassifier(\n",
    "            device='cuda',\n",
    "            random_state=seed,\n",
    "            n_estimators = 4,\n",
    "            ignore_pretraining_limits=True)\n",
    "        model.fit(x_train, y_train)\n",
    "        y_probs = model.predict_proba(x_test)\n",
    "\n",
    "\n",
    "        Y_TRUES = np.append(Y_TRUES, y_test)\n",
    "        Y_PROBS.append(y_probs)\n",
    "        Y_PREDS = np.append(Y_PREDS, np.argmax(y_probs, axis=1))\n",
    "\n",
    "    Y_PROBS = np.concatenate(Y_PROBS, axis=0)\n",
    "\n",
    "\n",
    "    indx = Y_TRUES.argsort()\n",
    "    Y_TRUES = Y_TRUES[indx]\n",
    "    Y_PROBS = Y_PROBS[indx]\n",
    "    Y_PREDS = Y_PREDS[indx]\n",
    "\n",
    "\n",
    "# Evaluation metrics (multiclass)\n",
    "auc_roc_macro = roc_auc_score(Y_TRUES, Y_PROBS, multi_class='ovr', average='macro')\n",
    "auc_roc_weighted = roc_auc_score(Y_TRUES, Y_PROBS, multi_class='ovr', average='weighted')\n",
    "auc_pr_macro = average_precision_score(Y_TRUES, Y_PROBS, average='macro')\n",
    "auc_pr_weighted = average_precision_score(Y_TRUES, Y_PROBS, average='weighted')\n",
    "acc = accuracy_score(Y_TRUES, Y_PREDS)\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(Y_TRUES, Y_PREDS)\n",
    "print(cm)\n",
    "cm_str = '\\n'.join(['[' + ' '.join(map(str, row)) + ']' for row in cm])\n",
    "\n",
    "cm_flat = cm.flatten()\n",
    "\n",
    "results = {\n",
    "    'auc-roc-macro': f\"{auc_roc_macro:.4f}\",\n",
    "    'auc-roc-weighted': f\"{auc_roc_weighted:.4f}\",\n",
    "    'auc-pr-macro': f\"{auc_pr_macro:.4f}\",\n",
    "    'auc-pr-weighted': f\"{auc_pr_weighted:.4f}\",\n",
    "    'accuracy': f\"{acc:.4f}\",\n",
    "    'confusion_matrix': cm_str\n",
    "}\n",
    "\n",
    "for i, val in enumerate(cm_flat):\n",
    "    results[f'cm-{i}'] = str(int(val))\n",
    "\n",
    "results = pd.DataFrame([results])\n",
    "results.to_csv(os.path.join(results_dir, name + '.csv'), index=False)\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10779 11\n",
      "1 10617 173\n",
      "2 10488 302\n",
      "3 10599 191\n",
      "4 10604 186\n",
      "5 10767 23\n",
      "6 10437 353\n",
      "7 10541 249\n",
      "8 10500 290\n",
      "9 10611 179\n",
      "10 10771 19\n",
      "11 10560 230\n",
      "12 10532 258\n",
      "13 10716 74\n",
      "14 10524 266\n",
      "15 10548 242\n",
      "16 10665 125\n",
      "17 10508 282\n",
      "18 10668 122\n",
      "19 10570 220\n",
      "20 10588 202\n",
      "21 10521 269\n",
      "22 10627 163\n",
      "23 10600 190\n",
      "24 10771 19\n",
      "25 10520 270\n",
      "26 10495 295\n",
      "27 10520 270\n",
      "28 10605 185\n",
      "29 10616 174\n",
      "30 10576 214\n",
      "31 10556 234\n",
      "32 10687 103\n",
      "33 10560 230\n",
      "34 10625 165\n",
      "35 10751 39\n",
      "36 10525 265\n",
      "37 10436 354\n",
      "38 10621 169\n",
      "39 10463 327\n",
      "40 10524 266\n",
      "41 10584 206\n",
      "42 10632 158\n",
      "43 10520 270\n",
      "44 10728 62\n",
      "45 10699 91\n",
      "46 10763 27\n",
      "47 10580 210\n",
      "48 10672 118\n",
      "49 10528 262\n",
      "50 10627 163\n",
      "51 10541 249\n",
      "52 10502 288\n",
      "53 10575 215\n",
      "54 10532 258\n",
      "55 10775 15\n",
      "[[10566     0     0     0]\n",
      " [  117     0     0     0]\n",
      " [   89     0     0     0]\n",
      " [   18     0     0     0]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auc-roc-macro</th>\n",
       "      <th>auc-roc-weighted</th>\n",
       "      <th>auc-pr-macro</th>\n",
       "      <th>auc-pr-weighted</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>cm-0</th>\n",
       "      <th>cm-1</th>\n",
       "      <th>cm-2</th>\n",
       "      <th>cm-3</th>\n",
       "      <th>...</th>\n",
       "      <th>cm-6</th>\n",
       "      <th>cm-7</th>\n",
       "      <th>cm-8</th>\n",
       "      <th>cm-9</th>\n",
       "      <th>cm-10</th>\n",
       "      <th>cm-11</th>\n",
       "      <th>cm-12</th>\n",
       "      <th>cm-13</th>\n",
       "      <th>cm-14</th>\n",
       "      <th>cm-15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.7945</td>\n",
       "      <td>0.7911</td>\n",
       "      <td>0.3047</td>\n",
       "      <td>0.9678</td>\n",
       "      <td>0.9792</td>\n",
       "      <td>[10566 0 0 0]\\n[117 0 0 0]\\n[89 0 0 0]\\n[18 0 ...</td>\n",
       "      <td>10566</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  auc-roc-macro auc-roc-weighted auc-pr-macro auc-pr-weighted accuracy  \\\n",
       "0        0.7945           0.7911       0.3047          0.9678   0.9792   \n",
       "\n",
       "                                    confusion_matrix   cm-0 cm-1 cm-2 cm-3  \\\n",
       "0  [10566 0 0 0]\\n[117 0 0 0]\\n[89 0 0 0]\\n[18 0 ...  10566    0    0    0   \n",
       "\n",
       "   ... cm-6 cm-7 cm-8 cm-9 cm-10 cm-11 cm-12 cm-13 cm-14 cm-15  \n",
       "0  ...    0    0   89    0     0     0    18     0     0     0  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gradient Boosting Classifier\n",
    "\n",
    "name = 'Gradient-Boosting'.lower()\n",
    "\n",
    "Y_TRUES = np.empty([0])\n",
    "Y_PROBS = []\n",
    "Y_PREDS = np.empty([0])\n",
    "\n",
    "\n",
    "if kfold:\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    split_iterator = cv.split(x)\n",
    "else:\n",
    "    cv = LeavePGroupsOut(n_groups=1)\n",
    "    split_iterator = cv.split(x, y, groups=p)\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(split_iterator, start=1):\n",
    "    participant = i if kfold else np.unique(p[test_idx])[0]\n",
    "\n",
    "    x_train, x_test = x[train_idx], x[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    print(participant, x_train.shape[0], x_test.shape[0])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    normalizer = MinMaxScaler()\n",
    "    normalizer.fit(x_train)\n",
    "    x_train = normalizer.transform(x_train)\n",
    "    x_test = normalizer.transform(x_test)\n",
    "\n",
    "    # Oversample class 1\n",
    "\n",
    "    # ros = RandomOverSampler(sampling_strategy=.1, random_state=seed)\n",
    "    # x_train, y_train = ros.fit_resample(x_train, y_train)\n",
    "\n",
    "    # # 1. Check original class distribution\n",
    "    # print(\"Original class distribution:\", Counter(y_train))\n",
    "    # # 2. Define the minority class (adjust if needed)\n",
    "    # minority_class = 1  # change this if your minority class label is different\n",
    "    # current_minority_count = sum(y_train == minority_class)\n",
    "    # # 3. Define desired new total count for the minority class (10x)\n",
    "    # target_minority_count = current_minority_count * 10\n",
    "    # # 4. Setup SMOTE with custom sampling strategy\n",
    "    # smote = SMOTE(sampling_strategy={minority_class: target_minority_count}, random_state=seed)\n",
    "    # # 5. Fit and resample\n",
    "    # x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "    # # 6. Confirm new distribution\n",
    "    # print(\"Resampled class distribution:\", Counter(y_train))\n",
    "\n",
    "\n",
    "    # ----- LightGBM\n",
    "    # train_data = lgb.Dataset(x_train, label=y_train)\n",
    "    # test_data = lgb.Dataset(x_test, label=y_test, reference=train_data)\n",
    "    # params = {\n",
    "    #     'verbose': -1,  # 👈 turn off training output\n",
    "    #     'objective': 'binary',\n",
    "    #     'metric': 'binary_logloss',  # or 'auc' if you prefer\n",
    "    #     'num_leaves': 64,\n",
    "    #     'learning_rate': 0.01,\n",
    "    #     'n_estimators': 100,\n",
    "    #     # 'is_unbalance': True  # Automatically balances positive and negative classes\n",
    "    #     # 'scale_pos_weight': (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    # }\n",
    "    # bst = lgb.train(params, train_data, valid_sets=[train_data, test_data])\n",
    "    # y_probs = bst.predict(x_test, num_iteration=bst.best_iteration)\n",
    "\n",
    "\n",
    "    # ----- Transformer\n",
    "    # model = TabPFNClassifier(\n",
    "    #     device='cuda',\n",
    "    #     random_state=seed,\n",
    "    #     n_estimators = 4,\n",
    "    #     ignore_pretraining_limits=True)\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- Gradient Boosting Classifier\n",
    "    model = GradientBoostingClassifier(\n",
    "        n_estimators=100,  # match LightGBM\n",
    "        learning_rate=0.001,  # match LightGBM\n",
    "        max_depth=4,  # similar to LightGBM default tree depth\n",
    "        subsample=1.0,  # default\n",
    "        random_state=seed\n",
    "    )\n",
    "    if weighted:\n",
    "        model.fit(x_train, y_train, sample_weight=compute_sample_weight(class_weight='balanced', y=y_train))\n",
    "    else:\n",
    "        model.fit(x_train, y_train)\n",
    "    \n",
    "    y_probs = model.predict_proba(x_test)\n",
    "    y_preds = model.predict(x_test)\n",
    "    \n",
    "\n",
    "    Y_TRUES = np.append(Y_TRUES, y_test)\n",
    "    Y_PROBS.append(y_probs)\n",
    "    Y_PREDS = np.append(Y_PREDS, y_preds)\n",
    "\n",
    "Y_PROBS = np.concatenate(Y_PROBS, axis=0)\n",
    "\n",
    "\n",
    "indx = Y_TRUES.argsort()\n",
    "Y_TRUES = Y_TRUES[indx]\n",
    "Y_PROBS = Y_PROBS[indx]\n",
    "Y_PREDS = Y_PREDS[indx]\n",
    "\n",
    "\n",
    "# Evaluation metrics (multiclass)\n",
    "auc_roc_macro = roc_auc_score(Y_TRUES, Y_PROBS, multi_class='ovr', average='macro')\n",
    "auc_roc_weighted = roc_auc_score(Y_TRUES, Y_PROBS, multi_class='ovr', average='weighted')\n",
    "auc_pr_macro = average_precision_score(Y_TRUES, Y_PROBS, average='macro')\n",
    "auc_pr_weighted = average_precision_score(Y_TRUES, Y_PROBS, average='weighted')\n",
    "acc = accuracy_score(Y_TRUES, Y_PREDS)\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(Y_TRUES, Y_PREDS)\n",
    "print(cm)\n",
    "cm_str = '\\n'.join(['[' + ' '.join(map(str, row)) + ']' for row in cm])\n",
    "\n",
    "cm_flat = cm.flatten()\n",
    "\n",
    "results = {\n",
    "    'auc-roc-macro': f\"{auc_roc_macro:.4f}\",\n",
    "    'auc-roc-weighted': f\"{auc_roc_weighted:.4f}\",\n",
    "    'auc-pr-macro': f\"{auc_pr_macro:.4f}\",\n",
    "    'auc-pr-weighted': f\"{auc_pr_weighted:.4f}\",\n",
    "    'accuracy': f\"{acc:.4f}\",\n",
    "    'confusion_matrix': cm_str\n",
    "}\n",
    "\n",
    "for i, val in enumerate(cm_flat):\n",
    "    results[f'cm-{i}'] = str(int(val))\n",
    "\n",
    "results = pd.DataFrame([results])\n",
    "results.to_csv(os.path.join(results_dir, name + '.csv'), index=False)\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10779 11\n",
      "1 10617 173\n",
      "2 10488 302\n",
      "3 10599 191\n",
      "4 10604 186\n",
      "5 10767 23\n",
      "6 10437 353\n",
      "7 10541 249\n",
      "8 10500 290\n",
      "9 10611 179\n",
      "10 10771 19\n",
      "11 10560 230\n",
      "12 10532 258\n",
      "13 10716 74\n",
      "14 10524 266\n",
      "15 10548 242\n",
      "16 10665 125\n",
      "17 10508 282\n",
      "18 10668 122\n",
      "19 10570 220\n",
      "20 10588 202\n",
      "21 10521 269\n",
      "22 10627 163\n",
      "23 10600 190\n",
      "24 10771 19\n",
      "25 10520 270\n",
      "26 10495 295\n",
      "27 10520 270\n",
      "28 10605 185\n",
      "29 10616 174\n",
      "30 10576 214\n",
      "31 10556 234\n",
      "32 10687 103\n",
      "33 10560 230\n",
      "34 10625 165\n",
      "35 10751 39\n",
      "36 10525 265\n",
      "37 10436 354\n",
      "38 10621 169\n",
      "39 10463 327\n",
      "40 10524 266\n",
      "41 10584 206\n",
      "42 10632 158\n",
      "43 10520 270\n",
      "44 10728 62\n",
      "45 10699 91\n",
      "46 10763 27\n",
      "47 10580 210\n",
      "48 10672 118\n",
      "49 10528 262\n",
      "50 10627 163\n",
      "51 10541 249\n",
      "52 10502 288\n",
      "53 10575 215\n",
      "54 10532 258\n",
      "55 10775 15\n",
      "[[10558     7     1     0]\n",
      " [  112     5     0     0]\n",
      " [   88     1     0     0]\n",
      " [   17     1     0     0]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auc-roc-macro</th>\n",
       "      <th>auc-roc-weighted</th>\n",
       "      <th>auc-pr-macro</th>\n",
       "      <th>auc-pr-weighted</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>cm-0</th>\n",
       "      <th>cm-1</th>\n",
       "      <th>cm-2</th>\n",
       "      <th>cm-3</th>\n",
       "      <th>...</th>\n",
       "      <th>cm-6</th>\n",
       "      <th>cm-7</th>\n",
       "      <th>cm-8</th>\n",
       "      <th>cm-9</th>\n",
       "      <th>cm-10</th>\n",
       "      <th>cm-11</th>\n",
       "      <th>cm-12</th>\n",
       "      <th>cm-13</th>\n",
       "      <th>cm-14</th>\n",
       "      <th>cm-15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8717</td>\n",
       "      <td>0.8449</td>\n",
       "      <td>0.3062</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>0.9790</td>\n",
       "      <td>[10558 7 1 0]\\n[112 5 0 0]\\n[88 1 0 0]\\n[17 1 ...</td>\n",
       "      <td>10558</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  auc-roc-macro auc-roc-weighted auc-pr-macro auc-pr-weighted accuracy  \\\n",
       "0        0.8717           0.8449       0.3062          0.9761   0.9790   \n",
       "\n",
       "                                    confusion_matrix   cm-0 cm-1 cm-2 cm-3  \\\n",
       "0  [10558 7 1 0]\\n[112 5 0 0]\\n[88 1 0 0]\\n[17 1 ...  10558    7    1    0   \n",
       "\n",
       "   ... cm-6 cm-7 cm-8 cm-9 cm-10 cm-11 cm-12 cm-13 cm-14 cm-15  \n",
       "0  ...    0    0   88    1     0     0    17     1     0     0  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# XGBoost\n",
    "\n",
    "name = 'XGBoost'.lower()\n",
    "\n",
    "Y_TRUES = np.empty([0])\n",
    "Y_PROBS = []\n",
    "Y_PREDS = np.empty([0])\n",
    "\n",
    "\n",
    "if kfold:\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    split_iterator = cv.split(x)\n",
    "else:\n",
    "    cv = LeavePGroupsOut(n_groups=1)\n",
    "    split_iterator = cv.split(x, y, groups=p)\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(split_iterator, start=1):\n",
    "    participant = i if kfold else np.unique(p[test_idx])[0]\n",
    "\n",
    "    x_train, x_test = x[train_idx], x[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    print(participant, x_train.shape[0], x_test.shape[0])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    normalizer = MinMaxScaler()\n",
    "    normalizer.fit(x_train)\n",
    "    x_train = normalizer.transform(x_train)\n",
    "    x_test = normalizer.transform(x_test)\n",
    "\n",
    "    # Oversample class 1\n",
    "\n",
    "    # ros = RandomOverSampler(sampling_strategy=.1, random_state=seed)\n",
    "    # x_train, y_train = ros.fit_resample(x_train, y_train)\n",
    "\n",
    "    # # 1. Check original class distribution\n",
    "    # print(\"Original class distribution:\", Counter(y_train))\n",
    "    # # 2. Define the minority class (adjust if needed)\n",
    "    # minority_class = 1  # change this if your minority class label is different\n",
    "    # current_minority_count = sum(y_train == minority_class)\n",
    "    # # 3. Define desired new total count for the minority class (10x)\n",
    "    # target_minority_count = current_minority_count * 10\n",
    "    # # 4. Setup SMOTE with custom sampling strategy\n",
    "    # smote = SMOTE(sampling_strategy={minority_class: target_minority_count}, random_state=seed)\n",
    "    # # 5. Fit and resample\n",
    "    # x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "    # # 6. Confirm new distribution\n",
    "    # print(\"Resampled class distribution:\", Counter(y_train))\n",
    "\n",
    "\n",
    "    # ----- LightGBM\n",
    "    # train_data = lgb.Dataset(x_train, label=y_train)\n",
    "    # test_data = lgb.Dataset(x_test, label=y_test, reference=train_data)\n",
    "    # params = {\n",
    "    #     'verbose': -1,  # 👈 turn off training output\n",
    "    #     'objective': 'binary',\n",
    "    #     'metric': 'binary_logloss',  # or 'auc' if you prefer\n",
    "    #     'num_leaves': 64,\n",
    "    #     'learning_rate': 0.01,\n",
    "    #     'n_estimators': 100,\n",
    "    #     # 'is_unbalance': True  # Automatically balances positive and negative classes\n",
    "    #     # 'scale_pos_weight': (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    # }\n",
    "    # bst = lgb.train(params, train_data, valid_sets=[train_data, test_data])\n",
    "    # y_probs = bst.predict(x_test, num_iteration=bst.best_iteration)\n",
    "\n",
    "\n",
    "    # ----- Transformer\n",
    "    # model = TabPFNClassifier(\n",
    "    #     device='cuda',\n",
    "    #     random_state=seed,\n",
    "    #     n_estimators = 4,\n",
    "    #     ignore_pretraining_limits=True)\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- Gradient Boosting Classifier\n",
    "    # model = GradientBoostingClassifier(\n",
    "    #     n_estimators=100,  # match LightGBM\n",
    "    #     learning_rate=0.001,  # match LightGBM\n",
    "    #     max_depth=4,  # similar to LightGBM default tree depth\n",
    "    #     subsample=1.0,  # default\n",
    "    #     random_state=seed\n",
    "    # )\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "    \n",
    "\n",
    "    # ----- XGBoost\n",
    "    model = XGBClassifier()\n",
    "    if weighted:\n",
    "        model.fit(x_train, y_train, sample_weight=compute_sample_weight(class_weight='balanced', y=y_train))\n",
    "    else:\n",
    "        model.fit(x_train, y_train)\n",
    "    y_probs = model.predict_proba(x_test)\n",
    "    y_preds = model.predict(x_test)\n",
    "\n",
    "\n",
    "    Y_TRUES = np.append(Y_TRUES, y_test)\n",
    "    Y_PROBS.append(y_probs)\n",
    "    Y_PREDS = np.append(Y_PREDS, y_preds)\n",
    "\n",
    "Y_PROBS = np.concatenate(Y_PROBS, axis=0)\n",
    "\n",
    "\n",
    "indx = Y_TRUES.argsort()\n",
    "Y_TRUES = Y_TRUES[indx]\n",
    "Y_PROBS = Y_PROBS[indx]\n",
    "Y_PREDS = Y_PREDS[indx]\n",
    "\n",
    "\n",
    "# Evaluation metrics (multiclass)\n",
    "auc_roc_macro = roc_auc_score(Y_TRUES, Y_PROBS, multi_class='ovr', average='macro')\n",
    "auc_roc_weighted = roc_auc_score(Y_TRUES, Y_PROBS, multi_class='ovr', average='weighted')\n",
    "auc_pr_macro = average_precision_score(Y_TRUES, Y_PROBS, average='macro')\n",
    "auc_pr_weighted = average_precision_score(Y_TRUES, Y_PROBS, average='weighted')\n",
    "acc = accuracy_score(Y_TRUES, Y_PREDS)\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(Y_TRUES, Y_PREDS)\n",
    "print(cm)\n",
    "cm_str = '\\n'.join(['[' + ' '.join(map(str, row)) + ']' for row in cm])\n",
    "\n",
    "cm_flat = cm.flatten()\n",
    "\n",
    "results = {\n",
    "    'auc-roc-macro': f\"{auc_roc_macro:.4f}\",\n",
    "    'auc-roc-weighted': f\"{auc_roc_weighted:.4f}\",\n",
    "    'auc-pr-macro': f\"{auc_pr_macro:.4f}\",\n",
    "    'auc-pr-weighted': f\"{auc_pr_weighted:.4f}\",\n",
    "    'accuracy': f\"{acc:.4f}\",\n",
    "    'confusion_matrix': cm_str\n",
    "}\n",
    "\n",
    "for i, val in enumerate(cm_flat):\n",
    "    results[f'cm-{i}'] = str(int(val))\n",
    "\n",
    "results = pd.DataFrame([results])\n",
    "results.to_csv(os.path.join(results_dir, name + '.csv'), index=False)\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10779 11\n",
      "1 10617 173\n",
      "2 10488 302\n",
      "3 10599 191\n",
      "4 10604 186\n",
      "5 10767 23\n",
      "6 10437 353\n",
      "7 10541 249\n",
      "8 10500 290\n",
      "9 10611 179\n",
      "10 10771 19\n",
      "11 10560 230\n",
      "12 10532 258\n",
      "13 10716 74\n",
      "14 10524 266\n",
      "15 10548 242\n",
      "16 10665 125\n",
      "17 10508 282\n",
      "18 10668 122\n",
      "19 10570 220\n",
      "20 10588 202\n",
      "21 10521 269\n",
      "22 10627 163\n",
      "23 10600 190\n",
      "24 10771 19\n",
      "25 10520 270\n",
      "26 10495 295\n",
      "27 10520 270\n",
      "28 10605 185\n",
      "29 10616 174\n",
      "30 10576 214\n",
      "31 10556 234\n",
      "32 10687 103\n",
      "33 10560 230\n",
      "34 10625 165\n",
      "35 10751 39\n",
      "36 10525 265\n",
      "37 10436 354\n",
      "38 10621 169\n",
      "39 10463 327\n",
      "40 10524 266\n",
      "41 10584 206\n",
      "42 10632 158\n",
      "43 10520 270\n",
      "44 10728 62\n",
      "45 10699 91\n",
      "46 10763 27\n",
      "47 10580 210\n",
      "48 10672 118\n",
      "49 10528 262\n",
      "50 10627 163\n",
      "51 10541 249\n",
      "52 10502 288\n",
      "53 10575 215\n",
      "54 10532 258\n",
      "55 10775 15\n",
      "[[10564     2     0     0]\n",
      " [  116     1     0     0]\n",
      " [   89     0     0     0]\n",
      " [   18     0     0     0]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auc-roc-macro</th>\n",
       "      <th>auc-roc-weighted</th>\n",
       "      <th>auc-pr-macro</th>\n",
       "      <th>auc-pr-weighted</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>cm-0</th>\n",
       "      <th>cm-1</th>\n",
       "      <th>cm-2</th>\n",
       "      <th>cm-3</th>\n",
       "      <th>...</th>\n",
       "      <th>cm-6</th>\n",
       "      <th>cm-7</th>\n",
       "      <th>cm-8</th>\n",
       "      <th>cm-9</th>\n",
       "      <th>cm-10</th>\n",
       "      <th>cm-11</th>\n",
       "      <th>cm-12</th>\n",
       "      <th>cm-13</th>\n",
       "      <th>cm-14</th>\n",
       "      <th>cm-15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8701</td>\n",
       "      <td>0.8600</td>\n",
       "      <td>0.3039</td>\n",
       "      <td>0.9765</td>\n",
       "      <td>0.9791</td>\n",
       "      <td>[10564 2 0 0]\\n[116 1 0 0]\\n[89 0 0 0]\\n[18 0 ...</td>\n",
       "      <td>10564</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  auc-roc-macro auc-roc-weighted auc-pr-macro auc-pr-weighted accuracy  \\\n",
       "0        0.8701           0.8600       0.3039          0.9765   0.9791   \n",
       "\n",
       "                                    confusion_matrix   cm-0 cm-1 cm-2 cm-3  \\\n",
       "0  [10564 2 0 0]\\n[116 1 0 0]\\n[89 0 0 0]\\n[18 0 ...  10564    2    0    0   \n",
       "\n",
       "   ... cm-6 cm-7 cm-8 cm-9 cm-10 cm-11 cm-12 cm-13 cm-14 cm-15  \n",
       "0  ...    0    0   89    0     0     0    18     0     0     0  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "name = 'Random-Forest'.lower()\n",
    "\n",
    "Y_TRUES = np.empty([0])\n",
    "Y_PROBS = []\n",
    "Y_PREDS = np.empty([0])\n",
    "\n",
    "\n",
    "if kfold:\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    split_iterator = cv.split(x)\n",
    "else:\n",
    "    cv = LeavePGroupsOut(n_groups=1)\n",
    "    split_iterator = cv.split(x, y, groups=p)\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(split_iterator, start=1):\n",
    "    participant = i if kfold else np.unique(p[test_idx])[0]\n",
    "\n",
    "    x_train, x_test = x[train_idx], x[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    print(participant, x_train.shape[0], x_test.shape[0])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    normalizer = MinMaxScaler()\n",
    "    normalizer.fit(x_train)\n",
    "    x_train = normalizer.transform(x_train)\n",
    "    x_test = normalizer.transform(x_test)\n",
    "\n",
    "    # Oversample class 1\n",
    "\n",
    "    # ros = RandomOverSampler(sampling_strategy=.1, random_state=seed)\n",
    "    # x_train, y_train = ros.fit_resample(x_train, y_train)\n",
    "\n",
    "    # # 1. Check original class distribution\n",
    "    # print(\"Original class distribution:\", Counter(y_train))\n",
    "    # # 2. Define the minority class (adjust if needed)\n",
    "    # minority_class = 1  # change this if your minority class label is different\n",
    "    # current_minority_count = sum(y_train == minority_class)\n",
    "    # # 3. Define desired new total count for the minority class (10x)\n",
    "    # target_minority_count = current_minority_count * 10\n",
    "    # # 4. Setup SMOTE with custom sampling strategy\n",
    "    # smote = SMOTE(sampling_strategy={minority_class: target_minority_count}, random_state=seed)\n",
    "    # # 5. Fit and resample\n",
    "    # x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "    # # 6. Confirm new distribution\n",
    "    # print(\"Resampled class distribution:\", Counter(y_train))\n",
    "\n",
    "\n",
    "    # ----- LightGBM\n",
    "    # train_data = lgb.Dataset(x_train, label=y_train)\n",
    "    # test_data = lgb.Dataset(x_test, label=y_test, reference=train_data)\n",
    "    # params = {\n",
    "    #     'verbose': -1,  # 👈 turn off training output\n",
    "    #     'objective': 'binary',\n",
    "    #     'metric': 'binary_logloss',  # or 'auc' if you prefer\n",
    "    #     'num_leaves': 64,\n",
    "    #     'learning_rate': 0.01,\n",
    "    #     'n_estimators': 100,\n",
    "    #     # 'is_unbalance': True  # Automatically balances positive and negative classes\n",
    "    #     # 'scale_pos_weight': (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    # }\n",
    "    # bst = lgb.train(params, train_data, valid_sets=[train_data, test_data])\n",
    "    # y_probs = bst.predict(x_test, num_iteration=bst.best_iteration)\n",
    "\n",
    "\n",
    "    # ----- Transformer\n",
    "    # model = TabPFNClassifier(\n",
    "    #     device='cuda',\n",
    "    #     random_state=seed,\n",
    "    #     n_estimators = 4,\n",
    "    #     ignore_pretraining_limits=True)\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- Gradient Boosting Classifier\n",
    "    # model = GradientBoostingClassifier(\n",
    "    #     n_estimators=100,  # match LightGBM\n",
    "    #     learning_rate=0.001,  # match LightGBM\n",
    "    #     max_depth=4,  # similar to LightGBM default tree depth\n",
    "    #     subsample=1.0,  # default\n",
    "    #     random_state=seed\n",
    "    # )\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "    \n",
    "\n",
    "    # ----- XGBoost\n",
    "    # model = XGBClassifier()\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- Random Forest\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=250,  # number of trees\n",
    "        max_depth=None,  # let the trees grow fully\n",
    "        random_state=seed\n",
    "    )\n",
    "    if weighted:\n",
    "        model.fit(x_train, y_train, sample_weight=compute_sample_weight(class_weight='balanced', y=y_train))\n",
    "    else:\n",
    "        model.fit(x_train, y_train)\n",
    "    y_probs = model.predict_proba(x_test)\n",
    "    y_preds = model.predict(x_test)\n",
    "\n",
    "\n",
    "    Y_TRUES = np.append(Y_TRUES, y_test)\n",
    "    Y_PROBS.append(y_probs)\n",
    "    Y_PREDS = np.append(Y_PREDS, y_preds)\n",
    "\n",
    "Y_PROBS = np.concatenate(Y_PROBS, axis=0)\n",
    "\n",
    "\n",
    "indx = Y_TRUES.argsort()\n",
    "Y_TRUES = Y_TRUES[indx]\n",
    "Y_PROBS = Y_PROBS[indx]\n",
    "Y_PREDS = Y_PREDS[indx]\n",
    "\n",
    "\n",
    "# Evaluation metrics (multiclass)\n",
    "auc_roc_macro = roc_auc_score(Y_TRUES, Y_PROBS, multi_class='ovr', average='macro')\n",
    "auc_roc_weighted = roc_auc_score(Y_TRUES, Y_PROBS, multi_class='ovr', average='weighted')\n",
    "auc_pr_macro = average_precision_score(Y_TRUES, Y_PROBS, average='macro')\n",
    "auc_pr_weighted = average_precision_score(Y_TRUES, Y_PROBS, average='weighted')\n",
    "acc = accuracy_score(Y_TRUES, Y_PREDS)\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(Y_TRUES, Y_PREDS)\n",
    "print(cm)\n",
    "cm_str = '\\n'.join(['[' + ' '.join(map(str, row)) + ']' for row in cm])\n",
    "\n",
    "cm_flat = cm.flatten()\n",
    "\n",
    "results = {\n",
    "    'auc-roc-macro': f\"{auc_roc_macro:.4f}\",\n",
    "    'auc-roc-weighted': f\"{auc_roc_weighted:.4f}\",\n",
    "    'auc-pr-macro': f\"{auc_pr_macro:.4f}\",\n",
    "    'auc-pr-weighted': f\"{auc_pr_weighted:.4f}\",\n",
    "    'accuracy': f\"{acc:.4f}\",\n",
    "    'confusion_matrix': cm_str\n",
    "}\n",
    "\n",
    "for i, val in enumerate(cm_flat):\n",
    "    results[f'cm-{i}'] = str(int(val))\n",
    "\n",
    "results = pd.DataFrame([results])\n",
    "results.to_csv(os.path.join(results_dir, name + '.csv'), index=False)\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10779 11\n",
      "1 10617 173\n",
      "2 10488 302\n",
      "3 10599 191\n",
      "4 10604 186\n",
      "5 10767 23\n",
      "6 10437 353\n",
      "7 10541 249\n",
      "8 10500 290\n",
      "9 10611 179\n",
      "10 10771 19\n",
      "11 10560 230\n",
      "12 10532 258\n",
      "13 10716 74\n",
      "14 10524 266\n",
      "15 10548 242\n",
      "16 10665 125\n",
      "17 10508 282\n",
      "18 10668 122\n",
      "19 10570 220\n",
      "20 10588 202\n",
      "21 10521 269\n",
      "22 10627 163\n",
      "23 10600 190\n",
      "24 10771 19\n",
      "25 10520 270\n",
      "26 10495 295\n",
      "27 10520 270\n",
      "28 10605 185\n",
      "29 10616 174\n",
      "30 10576 214\n",
      "31 10556 234\n",
      "32 10687 103\n",
      "33 10560 230\n",
      "34 10625 165\n",
      "35 10751 39\n",
      "36 10525 265\n",
      "37 10436 354\n",
      "38 10621 169\n",
      "39 10463 327\n",
      "40 10524 266\n",
      "41 10584 206\n",
      "42 10632 158\n",
      "43 10520 270\n",
      "44 10728 62\n",
      "45 10699 91\n",
      "46 10763 27\n",
      "47 10580 210\n",
      "48 10672 118\n",
      "49 10528 262\n",
      "50 10627 163\n",
      "51 10541 249\n",
      "52 10502 288\n",
      "53 10575 215\n",
      "54 10532 258\n",
      "55 10775 15\n",
      "[[10566     0     0     0]\n",
      " [  117     0     0     0]\n",
      " [   89     0     0     0]\n",
      " [   18     0     0     0]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auc-roc-macro</th>\n",
       "      <th>auc-roc-weighted</th>\n",
       "      <th>auc-pr-macro</th>\n",
       "      <th>auc-pr-weighted</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>cm-0</th>\n",
       "      <th>cm-1</th>\n",
       "      <th>cm-2</th>\n",
       "      <th>cm-3</th>\n",
       "      <th>...</th>\n",
       "      <th>cm-6</th>\n",
       "      <th>cm-7</th>\n",
       "      <th>cm-8</th>\n",
       "      <th>cm-9</th>\n",
       "      <th>cm-10</th>\n",
       "      <th>cm-11</th>\n",
       "      <th>cm-12</th>\n",
       "      <th>cm-13</th>\n",
       "      <th>cm-14</th>\n",
       "      <th>cm-15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.6246</td>\n",
       "      <td>0.6140</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>0.9644</td>\n",
       "      <td>0.9792</td>\n",
       "      <td>[10566 0 0 0]\\n[117 0 0 0]\\n[89 0 0 0]\\n[18 0 ...</td>\n",
       "      <td>10566</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  auc-roc-macro auc-roc-weighted auc-pr-macro auc-pr-weighted accuracy  \\\n",
       "0        0.6246           0.6140       0.2732          0.9644   0.9792   \n",
       "\n",
       "                                    confusion_matrix   cm-0 cm-1 cm-2 cm-3  \\\n",
       "0  [10566 0 0 0]\\n[117 0 0 0]\\n[89 0 0 0]\\n[18 0 ...  10566    0    0    0   \n",
       "\n",
       "   ... cm-6 cm-7 cm-8 cm-9 cm-10 cm-11 cm-12 cm-13 cm-14 cm-15  \n",
       "0  ...    0    0   89    0     0     0    18     0     0     0  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SVM\n",
    "\n",
    "name = 'SVM'.lower()\n",
    "\n",
    "Y_TRUES = np.empty([0])\n",
    "Y_PROBS = []\n",
    "Y_PREDS = np.empty([0])\n",
    "\n",
    "\n",
    "if kfold:\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    split_iterator = cv.split(x)\n",
    "else:\n",
    "    cv = LeavePGroupsOut(n_groups=1)\n",
    "    split_iterator = cv.split(x, y, groups=p)\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(split_iterator, start=1):\n",
    "    participant = i if kfold else np.unique(p[test_idx])[0]\n",
    "\n",
    "    x_train, x_test = x[train_idx], x[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    print(participant, x_train.shape[0], x_test.shape[0])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    normalizer = MinMaxScaler()\n",
    "    normalizer.fit(x_train)\n",
    "    x_train = normalizer.transform(x_train)\n",
    "    x_test = normalizer.transform(x_test)\n",
    "\n",
    "    # Oversample class 1\n",
    "\n",
    "    # ros = RandomOverSampler(sampling_strategy=.1, random_state=seed)\n",
    "    # x_train, y_train = ros.fit_resample(x_train, y_train)\n",
    "\n",
    "    # # 1. Check original class distribution\n",
    "    # print(\"Original class distribution:\", Counter(y_train))\n",
    "    # # 2. Define the minority class (adjust if needed)\n",
    "    # minority_class = 1  # change this if your minority class label is different\n",
    "    # current_minority_count = sum(y_train == minority_class)\n",
    "    # # 3. Define desired new total count for the minority class (10x)\n",
    "    # target_minority_count = current_minority_count * 10\n",
    "    # # 4. Setup SMOTE with custom sampling strategy\n",
    "    # smote = SMOTE(sampling_strategy={minority_class: target_minority_count}, random_state=seed)\n",
    "    # # 5. Fit and resample\n",
    "    # x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "    # # 6. Confirm new distribution\n",
    "    # print(\"Resampled class distribution:\", Counter(y_train))\n",
    "\n",
    "\n",
    "    # ----- LightGBM\n",
    "    # train_data = lgb.Dataset(x_train, label=y_train)\n",
    "    # test_data = lgb.Dataset(x_test, label=y_test, reference=train_data)\n",
    "    # params = {\n",
    "    #     'verbose': -1,  # 👈 turn off training output\n",
    "    #     'objective': 'binary',\n",
    "    #     'metric': 'binary_logloss',  # or 'auc' if you prefer\n",
    "    #     'num_leaves': 64,\n",
    "    #     'learning_rate': 0.01,\n",
    "    #     'n_estimators': 100,\n",
    "    #     # 'is_unbalance': True  # Automatically balances positive and negative classes\n",
    "    #     # 'scale_pos_weight': (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    # }\n",
    "    # bst = lgb.train(params, train_data, valid_sets=[train_data, test_data])\n",
    "    # y_probs = bst.predict(x_test, num_iteration=bst.best_iteration)\n",
    "\n",
    "\n",
    "    # ----- Transformer\n",
    "    # model = TabPFNClassifier(\n",
    "    #     device='cuda',\n",
    "    #     random_state=seed,\n",
    "    #     n_estimators = 4,\n",
    "    #     ignore_pretraining_limits=True)\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- Gradient Boosting Classifier\n",
    "    # model = GradientBoostingClassifier(\n",
    "    #     n_estimators=100,  # match LightGBM\n",
    "    #     learning_rate=0.001,  # match LightGBM\n",
    "    #     max_depth=4,  # similar to LightGBM default tree depth\n",
    "    #     subsample=1.0,  # default\n",
    "    #     random_state=seed\n",
    "    # )\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "    \n",
    "\n",
    "    # ----- XGBoost\n",
    "    # model = XGBClassifier()\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- Random Forest\n",
    "    # model = RandomForestClassifier(\n",
    "    #     n_estimators=250,  # number of trees\n",
    "    #     max_depth=None,  # let the trees grow fully\n",
    "    #     random_state=seed\n",
    "    # )\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- SVM\n",
    "    model = SVC(kernel='rbf', probability=True, random_state=seed)\n",
    "    if weighted:\n",
    "        model.fit(x_train, y_train, sample_weight=compute_sample_weight(class_weight='balanced', y=y_train))\n",
    "    else:\n",
    "        model.fit(x_train, y_train)\n",
    "    y_probs = model.predict_proba(x_test)\n",
    "    y_preds = model.predict(x_test)\n",
    "\n",
    "\n",
    "    Y_TRUES = np.append(Y_TRUES, y_test)\n",
    "    Y_PROBS.append(y_probs)\n",
    "    Y_PREDS = np.append(Y_PREDS, y_preds)\n",
    "\n",
    "Y_PROBS = np.concatenate(Y_PROBS, axis=0)\n",
    "\n",
    "\n",
    "indx = Y_TRUES.argsort()\n",
    "Y_TRUES = Y_TRUES[indx]\n",
    "Y_PROBS = Y_PROBS[indx]\n",
    "Y_PREDS = Y_PREDS[indx]\n",
    "\n",
    "\n",
    "# Evaluation metrics (multiclass)\n",
    "auc_roc_macro = roc_auc_score(Y_TRUES, Y_PROBS, multi_class='ovr', average='macro')\n",
    "auc_roc_weighted = roc_auc_score(Y_TRUES, Y_PROBS, multi_class='ovr', average='weighted')\n",
    "auc_pr_macro = average_precision_score(Y_TRUES, Y_PROBS, average='macro')\n",
    "auc_pr_weighted = average_precision_score(Y_TRUES, Y_PROBS, average='weighted')\n",
    "acc = accuracy_score(Y_TRUES, Y_PREDS)\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(Y_TRUES, Y_PREDS)\n",
    "print(cm)\n",
    "cm_str = '\\n'.join(['[' + ' '.join(map(str, row)) + ']' for row in cm])\n",
    "\n",
    "cm_flat = cm.flatten()\n",
    "\n",
    "results = {\n",
    "    'auc-roc-macro': f\"{auc_roc_macro:.4f}\",\n",
    "    'auc-roc-weighted': f\"{auc_roc_weighted:.4f}\",\n",
    "    'auc-pr-macro': f\"{auc_pr_macro:.4f}\",\n",
    "    'auc-pr-weighted': f\"{auc_pr_weighted:.4f}\",\n",
    "    'accuracy': f\"{acc:.4f}\",\n",
    "    'confusion_matrix': cm_str\n",
    "}\n",
    "\n",
    "for i, val in enumerate(cm_flat):\n",
    "    results[f'cm-{i}'] = str(int(val))\n",
    "\n",
    "results = pd.DataFrame([results])\n",
    "results.to_csv(os.path.join(results_dir, name + '.csv'), index=False)\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10779 11\n",
      "1 10617 173\n",
      "2 10488 302\n",
      "3 10599 191\n",
      "4 10604 186\n",
      "5 10767 23\n",
      "6 10437 353\n",
      "7 10541 249\n",
      "8 10500 290\n",
      "9 10611 179\n",
      "10 10771 19\n",
      "11 10560 230\n",
      "12 10532 258\n",
      "13 10716 74\n",
      "14 10524 266\n",
      "15 10548 242\n",
      "16 10665 125\n",
      "17 10508 282\n",
      "18 10668 122\n",
      "19 10570 220\n",
      "20 10588 202\n",
      "21 10521 269\n",
      "22 10627 163\n",
      "23 10600 190\n",
      "24 10771 19\n",
      "25 10520 270\n",
      "26 10495 295\n",
      "27 10520 270\n",
      "28 10605 185\n",
      "29 10616 174\n",
      "30 10576 214\n",
      "31 10556 234\n",
      "32 10687 103\n",
      "33 10560 230\n",
      "34 10625 165\n",
      "35 10751 39\n",
      "36 10525 265\n",
      "37 10436 354\n",
      "38 10621 169\n",
      "39 10463 327\n",
      "40 10524 266\n",
      "41 10584 206\n",
      "42 10632 158\n",
      "43 10520 270\n",
      "44 10728 62\n",
      "45 10699 91\n",
      "46 10763 27\n",
      "47 10580 210\n",
      "48 10672 118\n",
      "49 10528 262\n",
      "50 10627 163\n",
      "51 10541 249\n",
      "52 10502 288\n",
      "53 10575 215\n",
      "54 10532 258\n",
      "55 10775 15\n",
      "[[10318   109   124    15]\n",
      " [   88    22     6     1]\n",
      " [   80     5     1     3]\n",
      " [   15     2     0     1]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auc-roc-macro</th>\n",
       "      <th>auc-roc-weighted</th>\n",
       "      <th>auc-pr-macro</th>\n",
       "      <th>auc-pr-weighted</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>cm-0</th>\n",
       "      <th>cm-1</th>\n",
       "      <th>cm-2</th>\n",
       "      <th>cm-3</th>\n",
       "      <th>...</th>\n",
       "      <th>cm-6</th>\n",
       "      <th>cm-7</th>\n",
       "      <th>cm-8</th>\n",
       "      <th>cm-9</th>\n",
       "      <th>cm-10</th>\n",
       "      <th>cm-11</th>\n",
       "      <th>cm-12</th>\n",
       "      <th>cm-13</th>\n",
       "      <th>cm-14</th>\n",
       "      <th>cm-15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5484</td>\n",
       "      <td>0.5787</td>\n",
       "      <td>0.2585</td>\n",
       "      <td>0.9626</td>\n",
       "      <td>0.9585</td>\n",
       "      <td>[10318 109 124 15]\\n[88 22 6 1]\\n[80 5 1 3]\\n[...</td>\n",
       "      <td>10318</td>\n",
       "      <td>109</td>\n",
       "      <td>124</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  auc-roc-macro auc-roc-weighted auc-pr-macro auc-pr-weighted accuracy  \\\n",
       "0        0.5484           0.5787       0.2585          0.9626   0.9585   \n",
       "\n",
       "                                    confusion_matrix   cm-0 cm-1 cm-2 cm-3  \\\n",
       "0  [10318 109 124 15]\\n[88 22 6 1]\\n[80 5 1 3]\\n[...  10318  109  124   15   \n",
       "\n",
       "   ... cm-6 cm-7 cm-8 cm-9 cm-10 cm-11 cm-12 cm-13 cm-14 cm-15  \n",
       "0  ...    6    1   80    5     1     3    15     2     0     1  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DT\n",
    "\n",
    "name = 'DT'.lower()\n",
    "\n",
    "Y_TRUES = np.empty([0])\n",
    "Y_PROBS = []\n",
    "Y_PREDS = np.empty([0])\n",
    "\n",
    "\n",
    "if kfold:\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    split_iterator = cv.split(x)\n",
    "else:\n",
    "    cv = LeavePGroupsOut(n_groups=1)\n",
    "    split_iterator = cv.split(x, y, groups=p)\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(split_iterator, start=1):\n",
    "    participant = i if kfold else np.unique(p[test_idx])[0]\n",
    "\n",
    "    x_train, x_test = x[train_idx], x[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    print(participant, x_train.shape[0], x_test.shape[0])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    normalizer = MinMaxScaler()\n",
    "    normalizer.fit(x_train)\n",
    "    x_train = normalizer.transform(x_train)\n",
    "    x_test = normalizer.transform(x_test)\n",
    "\n",
    "    # Oversample class 1\n",
    "\n",
    "    # ros = RandomOverSampler(sampling_strategy=.1, random_state=seed)\n",
    "    # x_train, y_train = ros.fit_resample(x_train, y_train)\n",
    "\n",
    "    # # 1. Check original class distribution\n",
    "    # print(\"Original class distribution:\", Counter(y_train))\n",
    "    # # 2. Define the minority class (adjust if needed)\n",
    "    # minority_class = 1  # change this if your minority class label is different\n",
    "    # current_minority_count = sum(y_train == minority_class)\n",
    "    # # 3. Define desired new total count for the minority class (10x)\n",
    "    # target_minority_count = current_minority_count * 10\n",
    "    # # 4. Setup SMOTE with custom sampling strategy\n",
    "    # smote = SMOTE(sampling_strategy={minority_class: target_minority_count}, random_state=seed)\n",
    "    # # 5. Fit and resample\n",
    "    # x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "    # # 6. Confirm new distribution\n",
    "    # print(\"Resampled class distribution:\", Counter(y_train))\n",
    "\n",
    "\n",
    "    # ----- LightGBM\n",
    "    # train_data = lgb.Dataset(x_train, label=y_train)\n",
    "    # test_data = lgb.Dataset(x_test, label=y_test, reference=train_data)\n",
    "    # params = {\n",
    "    #     'verbose': -1,  # 👈 turn off training output\n",
    "    #     'objective': 'binary',\n",
    "    #     'metric': 'binary_logloss',  # or 'auc' if you prefer\n",
    "    #     'num_leaves': 64,\n",
    "    #     'learning_rate': 0.01,\n",
    "    #     'n_estimators': 100,\n",
    "    #     # 'is_unbalance': True  # Automatically balances positive and negative classes\n",
    "    #     # 'scale_pos_weight': (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    # }\n",
    "    # bst = lgb.train(params, train_data, valid_sets=[train_data, test_data])\n",
    "    # y_probs = bst.predict(x_test, num_iteration=bst.best_iteration)\n",
    "\n",
    "\n",
    "    # ----- Transformer\n",
    "    # model = TabPFNClassifier(\n",
    "    #     device='cuda',\n",
    "    #     random_state=seed,\n",
    "    #     n_estimators = 4,\n",
    "    #     ignore_pretraining_limits=True)\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- Gradient Boosting Classifier\n",
    "    # model = GradientBoostingClassifier(\n",
    "    #     n_estimators=100,  # match LightGBM\n",
    "    #     learning_rate=0.001,  # match LightGBM\n",
    "    #     max_depth=4,  # similar to LightGBM default tree depth\n",
    "    #     subsample=1.0,  # default\n",
    "    #     random_state=seed\n",
    "    # )\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "    \n",
    "\n",
    "    # ----- XGBoost\n",
    "    # model = XGBClassifier()\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- Random Forest\n",
    "    # model = RandomForestClassifier(\n",
    "    #     n_estimators=250,  # number of trees\n",
    "    #     max_depth=None,  # let the trees grow fully\n",
    "    #     random_state=seed\n",
    "    # )\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- SVM\n",
    "    # model = SVC(kernel='rbf', probability=True, random_state=seed)\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- DT\n",
    "    model = DecisionTreeClassifier(criterion=\"entropy\", max_depth=512, random_state=seed)\n",
    "    if weighted:\n",
    "        model.fit(x_train, y_train, sample_weight=compute_sample_weight(class_weight='balanced', y=y_train))\n",
    "    else:\n",
    "        model.fit(x_train, y_train)\n",
    "    y_probs = model.predict_proba(x_test)\n",
    "    y_preds = model.predict(x_test)\n",
    "\n",
    "\n",
    "    Y_TRUES = np.append(Y_TRUES, y_test)\n",
    "    Y_PROBS.append(y_probs)\n",
    "    Y_PREDS = np.append(Y_PREDS, y_preds)\n",
    "\n",
    "Y_PROBS = np.concatenate(Y_PROBS, axis=0)\n",
    "\n",
    "\n",
    "indx = Y_TRUES.argsort()\n",
    "Y_TRUES = Y_TRUES[indx]\n",
    "Y_PROBS = Y_PROBS[indx]\n",
    "Y_PREDS = Y_PREDS[indx]\n",
    "\n",
    "\n",
    "# Evaluation metrics (multiclass)\n",
    "auc_roc_macro = roc_auc_score(Y_TRUES, Y_PROBS, multi_class='ovr', average='macro')\n",
    "auc_roc_weighted = roc_auc_score(Y_TRUES, Y_PROBS, multi_class='ovr', average='weighted')\n",
    "auc_pr_macro = average_precision_score(Y_TRUES, Y_PROBS, average='macro')\n",
    "auc_pr_weighted = average_precision_score(Y_TRUES, Y_PROBS, average='weighted')\n",
    "acc = accuracy_score(Y_TRUES, Y_PREDS)\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(Y_TRUES, Y_PREDS)\n",
    "print(cm)\n",
    "cm_str = '\\n'.join(['[' + ' '.join(map(str, row)) + ']' for row in cm])\n",
    "\n",
    "cm_flat = cm.flatten()\n",
    "\n",
    "results = {\n",
    "    'auc-roc-macro': f\"{auc_roc_macro:.4f}\",\n",
    "    'auc-roc-weighted': f\"{auc_roc_weighted:.4f}\",\n",
    "    'auc-pr-macro': f\"{auc_pr_macro:.4f}\",\n",
    "    'auc-pr-weighted': f\"{auc_pr_weighted:.4f}\",\n",
    "    'accuracy': f\"{acc:.4f}\",\n",
    "    'confusion_matrix': cm_str\n",
    "}\n",
    "\n",
    "for i, val in enumerate(cm_flat):\n",
    "    results[f'cm-{i}'] = str(int(val))\n",
    "\n",
    "results = pd.DataFrame([results])\n",
    "results.to_csv(os.path.join(results_dir, name + '.csv'), index=False)\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10779 11\n",
      "1 10617 173\n",
      "2 10488 302\n",
      "3 10599 191\n",
      "4 10604 186\n",
      "5 10767 23\n",
      "6 10437 353\n",
      "7 10541 249\n",
      "8 10500 290\n",
      "9 10611 179\n",
      "10 10771 19\n",
      "11 10560 230\n",
      "12 10532 258\n",
      "13 10716 74\n",
      "14 10524 266\n",
      "15 10548 242\n",
      "16 10665 125\n",
      "17 10508 282\n",
      "18 10668 122\n",
      "19 10570 220\n",
      "20 10588 202\n",
      "21 10521 269\n",
      "22 10627 163\n",
      "23 10600 190\n",
      "24 10771 19\n",
      "25 10520 270\n",
      "26 10495 295\n",
      "27 10520 270\n",
      "28 10605 185\n",
      "29 10616 174\n",
      "30 10576 214\n",
      "31 10556 234\n",
      "32 10687 103\n",
      "33 10560 230\n",
      "34 10625 165\n",
      "35 10751 39\n",
      "36 10525 265\n",
      "37 10436 354\n",
      "38 10621 169\n",
      "39 10463 327\n",
      "40 10524 266\n",
      "41 10584 206\n",
      "42 10632 158\n",
      "43 10520 270\n",
      "44 10728 62\n",
      "45 10699 91\n",
      "46 10763 27\n",
      "47 10580 210\n",
      "48 10672 118\n",
      "49 10528 262\n",
      "50 10627 163\n",
      "51 10541 249\n",
      "52 10502 288\n",
      "53 10575 215\n",
      "54 10532 258\n",
      "55 10775 15\n",
      "[[10566     0     0     0]\n",
      " [  115     2     0     0]\n",
      " [   89     0     0     0]\n",
      " [   18     0     0     0]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auc-roc-macro</th>\n",
       "      <th>auc-roc-weighted</th>\n",
       "      <th>auc-pr-macro</th>\n",
       "      <th>auc-pr-weighted</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>cm-0</th>\n",
       "      <th>cm-1</th>\n",
       "      <th>cm-2</th>\n",
       "      <th>cm-3</th>\n",
       "      <th>...</th>\n",
       "      <th>cm-6</th>\n",
       "      <th>cm-7</th>\n",
       "      <th>cm-8</th>\n",
       "      <th>cm-9</th>\n",
       "      <th>cm-10</th>\n",
       "      <th>cm-11</th>\n",
       "      <th>cm-12</th>\n",
       "      <th>cm-13</th>\n",
       "      <th>cm-14</th>\n",
       "      <th>cm-15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8155</td>\n",
       "      <td>0.7937</td>\n",
       "      <td>0.2904</td>\n",
       "      <td>0.9734</td>\n",
       "      <td>0.9794</td>\n",
       "      <td>[10566 0 0 0]\\n[115 2 0 0]\\n[89 0 0 0]\\n[18 0 ...</td>\n",
       "      <td>10566</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  auc-roc-macro auc-roc-weighted auc-pr-macro auc-pr-weighted accuracy  \\\n",
       "0        0.8155           0.7937       0.2904          0.9734   0.9794   \n",
       "\n",
       "                                    confusion_matrix   cm-0 cm-1 cm-2 cm-3  \\\n",
       "0  [10566 0 0 0]\\n[115 2 0 0]\\n[89 0 0 0]\\n[18 0 ...  10566    0    0    0   \n",
       "\n",
       "   ... cm-6 cm-7 cm-8 cm-9 cm-10 cm-11 cm-12 cm-13 cm-14 cm-15  \n",
       "0  ...    0    0   89    0     0     0    18     0     0     0  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MLP\n",
    "\n",
    "name = 'MLP'.lower()\n",
    "\n",
    "Y_TRUES = np.empty([0])\n",
    "Y_PROBS = []\n",
    "Y_PREDS = np.empty([0])\n",
    "\n",
    "\n",
    "if kfold:\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    split_iterator = cv.split(x)\n",
    "else:\n",
    "    cv = LeavePGroupsOut(n_groups=1)\n",
    "    split_iterator = cv.split(x, y, groups=p)\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(split_iterator, start=1):\n",
    "    participant = i if kfold else np.unique(p[test_idx])[0]\n",
    "\n",
    "    x_train, x_test = x[train_idx], x[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    print(participant, x_train.shape[0], x_test.shape[0])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    normalizer = MinMaxScaler()\n",
    "    normalizer.fit(x_train)\n",
    "    x_train = normalizer.transform(x_train)\n",
    "    x_test = normalizer.transform(x_test)\n",
    "\n",
    "    # Oversample class 1\n",
    "\n",
    "    # ros = RandomOverSampler(sampling_strategy=.1, random_state=seed)\n",
    "    # x_train, y_train = ros.fit_resample(x_train, y_train)\n",
    "\n",
    "    # # 1. Check original class distribution\n",
    "    # print(\"Original class distribution:\", Counter(y_train))\n",
    "    # # 2. Define the minority class (adjust if needed)\n",
    "    # minority_class = 1  # change this if your minority class label is different\n",
    "    # current_minority_count = sum(y_train == minority_class)\n",
    "    # # 3. Define desired new total count for the minority class (10x)\n",
    "    # target_minority_count = current_minority_count * 10\n",
    "    # # 4. Setup SMOTE with custom sampling strategy\n",
    "    # smote = SMOTE(sampling_strategy={minority_class: target_minority_count}, random_state=seed)\n",
    "    # # 5. Fit and resample\n",
    "    # x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "    # # 6. Confirm new distribution\n",
    "    # print(\"Resampled class distribution:\", Counter(y_train))\n",
    "\n",
    "\n",
    "    # ----- LightGBM\n",
    "    # train_data = lgb.Dataset(x_train, label=y_train)\n",
    "    # test_data = lgb.Dataset(x_test, label=y_test, reference=train_data)\n",
    "    # params = {\n",
    "    #     'verbose': -1,  # 👈 turn off training output\n",
    "    #     'objective': 'binary',\n",
    "    #     'metric': 'binary_logloss',  # or 'auc' if you prefer\n",
    "    #     'num_leaves': 64,\n",
    "    #     'learning_rate': 0.01,\n",
    "    #     'n_estimators': 100,\n",
    "    #     # 'is_unbalance': True  # Automatically balances positive and negative classes\n",
    "    #     # 'scale_pos_weight': (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    # }\n",
    "    # bst = lgb.train(params, train_data, valid_sets=[train_data, test_data])\n",
    "    # y_probs = bst.predict(x_test, num_iteration=bst.best_iteration)\n",
    "\n",
    "\n",
    "    # ----- Transformer\n",
    "    # model = TabPFNClassifier(\n",
    "    #     device='cuda',\n",
    "    #     random_state=seed,\n",
    "    #     n_estimators = 4,\n",
    "    #     ignore_pretraining_limits=True)\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- Gradient Boosting Classifier\n",
    "    # model = GradientBoostingClassifier(\n",
    "    #     n_estimators=100,  # match LightGBM\n",
    "    #     learning_rate=0.001,  # match LightGBM\n",
    "    #     max_depth=4,  # similar to LightGBM default tree depth\n",
    "    #     subsample=1.0,  # default\n",
    "    #     random_state=seed\n",
    "    # )\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "    \n",
    "\n",
    "    # ----- XGBoost\n",
    "    # model = XGBClassifier()\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- Random Forest\n",
    "    # model = RandomForestClassifier(\n",
    "    #     n_estimators=250,  # number of trees\n",
    "    #     max_depth=None,  # let the trees grow fully\n",
    "    #     random_state=seed\n",
    "    # )\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- SVM\n",
    "    # model = SVC(kernel='rbf', probability=True, random_state=seed)\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- DT\n",
    "    # model = DecisionTreeClassifier(criterion=\"log_loss\", max_depth=512, random_state=seed)\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict(x_test)\n",
    "\n",
    "\n",
    "    # MLP\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=(128, 64),    # 2 hidden layers: 64 and 32 neurons\n",
    "        activation='relu',              # good default: 'relu'\n",
    "        solver='adam',                  # optimizer\n",
    "        alpha=0.0001,                   # L2 regularization\n",
    "        learning_rate_init=0.001,\n",
    "        max_iter=500,\n",
    "        early_stopping=True,\n",
    "        random_state=seed,\n",
    "        verbose=False,\n",
    "    )\n",
    "    model.fit(x_train, y_train)\n",
    "    y_probs = model.predict_proba(x_test)\n",
    "    y_preds = model.predict(x_test)\n",
    "\n",
    "\n",
    "    Y_TRUES = np.append(Y_TRUES, y_test)\n",
    "    Y_PROBS.append(y_probs)\n",
    "    Y_PREDS = np.append(Y_PREDS, y_preds)\n",
    "\n",
    "Y_PROBS = np.concatenate(Y_PROBS, axis=0)\n",
    "\n",
    "\n",
    "indx = Y_TRUES.argsort()\n",
    "Y_TRUES = Y_TRUES[indx]\n",
    "Y_PROBS = Y_PROBS[indx]\n",
    "Y_PREDS = Y_PREDS[indx]\n",
    "\n",
    "\n",
    "# Evaluation metrics (multiclass)\n",
    "auc_roc_macro = roc_auc_score(Y_TRUES, Y_PROBS, multi_class='ovr', average='macro')\n",
    "auc_roc_weighted = roc_auc_score(Y_TRUES, Y_PROBS, multi_class='ovr', average='weighted')\n",
    "auc_pr_macro = average_precision_score(Y_TRUES, Y_PROBS, average='macro')\n",
    "auc_pr_weighted = average_precision_score(Y_TRUES, Y_PROBS, average='weighted')\n",
    "acc = accuracy_score(Y_TRUES, Y_PREDS)\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(Y_TRUES, Y_PREDS)\n",
    "print(cm)\n",
    "cm_str = '\\n'.join(['[' + ' '.join(map(str, row)) + ']' for row in cm])\n",
    "\n",
    "cm_flat = cm.flatten()\n",
    "\n",
    "results = {\n",
    "    'auc-roc-macro': f\"{auc_roc_macro:.4f}\",\n",
    "    'auc-roc-weighted': f\"{auc_roc_weighted:.4f}\",\n",
    "    'auc-pr-macro': f\"{auc_pr_macro:.4f}\",\n",
    "    'auc-pr-weighted': f\"{auc_pr_weighted:.4f}\",\n",
    "    'accuracy': f\"{acc:.4f}\",\n",
    "    'confusion_matrix': cm_str\n",
    "}\n",
    "\n",
    "for i, val in enumerate(cm_flat):\n",
    "    results[f'cm-{i}'] = str(int(val))\n",
    "\n",
    "results = pd.DataFrame([results])\n",
    "results.to_csv(os.path.join(results_dir, name + '.csv'), index=False)\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10779 11\n",
      "1 10617 173\n",
      "2 10488 302\n",
      "3 10599 191\n",
      "4 10604 186\n",
      "5 10767 23\n",
      "6 10437 353\n",
      "7 10541 249\n",
      "8 10500 290\n",
      "9 10611 179\n",
      "10 10771 19\n",
      "11 10560 230\n",
      "12 10532 258\n",
      "13 10716 74\n",
      "14 10524 266\n",
      "15 10548 242\n",
      "16 10665 125\n",
      "17 10508 282\n",
      "18 10668 122\n",
      "19 10570 220\n",
      "20 10588 202\n",
      "21 10521 269\n",
      "22 10627 163\n",
      "23 10600 190\n",
      "24 10771 19\n",
      "25 10520 270\n",
      "26 10495 295\n",
      "27 10520 270\n",
      "28 10605 185\n",
      "29 10616 174\n",
      "30 10576 214\n",
      "31 10556 234\n",
      "32 10687 103\n",
      "33 10560 230\n",
      "34 10625 165\n",
      "35 10751 39\n",
      "36 10525 265\n",
      "37 10436 354\n",
      "38 10621 169\n",
      "39 10463 327\n",
      "40 10524 266\n",
      "41 10584 206\n",
      "42 10632 158\n",
      "43 10520 270\n",
      "44 10728 62\n",
      "45 10699 91\n",
      "46 10763 27\n",
      "47 10580 210\n",
      "48 10672 118\n",
      "49 10528 262\n",
      "50 10627 163\n",
      "51 10541 249\n",
      "52 10502 288\n",
      "53 10575 215\n",
      "54 10532 258\n",
      "55 10775 15\n",
      "[[10559     7     0     0]\n",
      " [  111     6     0     0]\n",
      " [   86     3     0     0]\n",
      " [   18     0     0     0]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auc-roc-macro</th>\n",
       "      <th>auc-roc-weighted</th>\n",
       "      <th>auc-pr-macro</th>\n",
       "      <th>auc-pr-weighted</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>cm-0</th>\n",
       "      <th>cm-1</th>\n",
       "      <th>cm-2</th>\n",
       "      <th>cm-3</th>\n",
       "      <th>...</th>\n",
       "      <th>cm-6</th>\n",
       "      <th>cm-7</th>\n",
       "      <th>cm-8</th>\n",
       "      <th>cm-9</th>\n",
       "      <th>cm-10</th>\n",
       "      <th>cm-11</th>\n",
       "      <th>cm-12</th>\n",
       "      <th>cm-13</th>\n",
       "      <th>cm-14</th>\n",
       "      <th>cm-15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8976</td>\n",
       "      <td>0.8875</td>\n",
       "      <td>0.3185</td>\n",
       "      <td>0.9784</td>\n",
       "      <td>0.9791</td>\n",
       "      <td>[10559 7 0 0]\\n[111 6 0 0]\\n[86 3 0 0]\\n[18 0 ...</td>\n",
       "      <td>10559</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  auc-roc-macro auc-roc-weighted auc-pr-macro auc-pr-weighted accuracy  \\\n",
       "0        0.8976           0.8875       0.3185          0.9784   0.9791   \n",
       "\n",
       "                                    confusion_matrix   cm-0 cm-1 cm-2 cm-3  \\\n",
       "0  [10559 7 0 0]\\n[111 6 0 0]\\n[86 3 0 0]\\n[18 0 ...  10559    7    0    0   \n",
       "\n",
       "   ... cm-6 cm-7 cm-8 cm-9 cm-10 cm-11 cm-12 cm-13 cm-14 cm-15  \n",
       "0  ...    0    0   86    3     0     0    18     0     0     0  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "name = 'Logistic-Regression'.lower()\n",
    "\n",
    "Y_TRUES = np.empty([0])\n",
    "Y_PROBS = []\n",
    "Y_PREDS = np.empty([0])\n",
    "\n",
    "\n",
    "if kfold:\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    split_iterator = cv.split(x)\n",
    "else:\n",
    "    cv = LeavePGroupsOut(n_groups=1)\n",
    "    split_iterator = cv.split(x, y, groups=p)\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(split_iterator, start=1):\n",
    "    participant = i if kfold else np.unique(p[test_idx])[0]\n",
    "\n",
    "    x_train, x_test = x[train_idx], x[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    print(participant, x_train.shape[0], x_test.shape[0])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    normalizer = MinMaxScaler()\n",
    "    normalizer.fit(x_train)\n",
    "    x_train = normalizer.transform(x_train)\n",
    "    x_test = normalizer.transform(x_test)\n",
    "\n",
    "    # Oversample class 1\n",
    "\n",
    "    # ros = RandomOverSampler(sampling_strategy=.1, random_state=seed)\n",
    "    # x_train, y_train = ros.fit_resample(x_train, y_train)\n",
    "\n",
    "    # 1. Check original class distribution\n",
    "    # print(\"Original class distribution:\", Counter(y_train))\n",
    "    # # 2. Define the minority class (adjust if needed)\n",
    "    # minority_class = 1  # change this if your minority class label is different\n",
    "    # current_minority_count = sum(y_train == minority_class)\n",
    "    # # 3. Define desired new total count for the minority class (10x)\n",
    "    # target_minority_count = current_minority_count * 10\n",
    "    # # 4. Setup SMOTE with custom sampling strategy\n",
    "    # smote = SMOTE(sampling_strategy={minority_class: target_minority_count}, random_state=seed)\n",
    "    # # 5. Fit and resample\n",
    "    # x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "    # # 6. Confirm new distribution\n",
    "    # print(\"Resampled class distribution:\", Counter(y_train))\n",
    "\n",
    "\n",
    "    # Undersample class 0\n",
    "    # reduction_percent = 99\n",
    "    # print(\"Original class distribution:\", Counter(y_train))    \n",
    "    # class_0_idx = np.where(y_train == 0)[0]\n",
    "    # class_1_idx = np.where(y_train == 1)[0]\n",
    "    # # How many class 0 samples to keep\n",
    "    # n_keep = int(len(class_0_idx) * (1 - reduction_percent / 100.0))\n",
    "    # sampled_0_idx = np.random.choice(class_0_idx, size=n_keep, replace=False)\n",
    "    # # Combine sampled class 0 with all class 1\n",
    "    # final_idx = np.concatenate([sampled_0_idx, class_1_idx])\n",
    "    # x_train = x_train[final_idx]\n",
    "    # y_train =  y_train[final_idx]\n",
    "    # print(\"Resampled class distribution:\", Counter(y_train))\n",
    "\n",
    "\n",
    "    # ----- LightGBM\n",
    "    # train_data = lgb.Dataset(x_train, label=y_train)\n",
    "    # test_data = lgb.Dataset(x_test, label=y_test, reference=train_data)\n",
    "    # params = {\n",
    "    #     'verbose': -1,  # 👈 turn off training output\n",
    "    #     'objective': 'binary',\n",
    "    #     'metric': 'binary_logloss',  # or 'auc' if you prefer\n",
    "    #     'num_leaves': 64,\n",
    "    #     'learning_rate': 0.01,\n",
    "    #     'n_estimators': 100,\n",
    "    #     # 'is_unbalance': True  # Automatically balances positive and negative classes\n",
    "    #     # 'scale_pos_weight': (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    # }\n",
    "    # bst = lgb.train(params, train_data, valid_sets=[train_data, test_data])\n",
    "    # y_probs = bst.predict(x_test, num_iteration=bst.best_iteration)\n",
    "\n",
    "\n",
    "    # ----- Transformer\n",
    "    # model = TabPFNClassifier(\n",
    "    #     device='cuda',\n",
    "    #     random_state=seed,\n",
    "    #     n_estimators = 4,\n",
    "    #     ignore_pretraining_limits=True)\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- Gradient Boosting Classifier\n",
    "    # model = GradientBoostingClassifier(\n",
    "    #     n_estimators=100,  # match LightGBM\n",
    "    #     learning_rate=0.001,  # match LightGBM\n",
    "    #     max_depth=4,  # similar to LightGBM default tree depth\n",
    "    #     subsample=1.0,  # default\n",
    "    #     random_state=seed\n",
    "    # )\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "    \n",
    "\n",
    "    # ----- XGBoost\n",
    "    # model = XGBClassifier()\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- Random Forest\n",
    "    # model = RandomForestClassifier(\n",
    "    #     n_estimators=250,  # number of trees\n",
    "    #     max_depth=None,  # let the trees grow fully\n",
    "    #     random_state=seed\n",
    "    # )\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- SVM\n",
    "    # model = SVC(kernel='rbf', probability=True, random_state=seed)\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- DT\n",
    "    # model = DecisionTreeClassifier(criterion=\"log_loss\", max_depth=512, random_state=seed)\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict(x_test)\n",
    "\n",
    "\n",
    "    # MLP\n",
    "    # model = MLPClassifier(\n",
    "    #     hidden_layer_sizes=(128, 64),    # 2 hidden layers: 64 and 32 neurons\n",
    "    #     activation='relu',              # good default: 'relu'\n",
    "    #     solver='adam',                  # optimizer\n",
    "    #     alpha=0.0001,                   # L2 regularization\n",
    "    #     learning_rate_init=0.001,\n",
    "    #     max_iter=500,\n",
    "    #     early_stopping=True,\n",
    "    #     random_state=seed,\n",
    "    #     verbose=True,\n",
    "    # )\n",
    "    # model.fit(x_train, y_train)    \n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "    # y_preds = model.predict(x_test)\n",
    "\n",
    "\n",
    "    # LR\n",
    "    model = LogisticRegression(\n",
    "    penalty='l2',               # regularization (default)\n",
    "    C=1.0,                      # inverse of regularization strength\n",
    "    solver='lbfgs',             # optimizer (good default for small/medium data)\n",
    "    max_iter=1000,\n",
    "\n",
    "    class_weight='balanced' if weighted else None,\n",
    "\n",
    "    random_state=seed)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_probs = model.predict_proba(x_test)\n",
    "    y_preds = model.predict(x_test)\n",
    "\n",
    "\n",
    "    Y_TRUES = np.append(Y_TRUES, y_test)\n",
    "    Y_PROBS.append(y_probs)\n",
    "    Y_PREDS = np.append(Y_PREDS, y_preds)\n",
    "\n",
    "Y_PROBS = np.concatenate(Y_PROBS, axis=0)\n",
    "\n",
    "\n",
    "indx = Y_TRUES.argsort()\n",
    "Y_TRUES = Y_TRUES[indx]\n",
    "Y_PROBS = Y_PROBS[indx]\n",
    "Y_PREDS = Y_PREDS[indx]\n",
    "\n",
    "\n",
    "# Evaluation metrics (multiclass)\n",
    "auc_roc_macro = roc_auc_score(Y_TRUES, Y_PROBS, multi_class='ovr', average='macro')\n",
    "auc_roc_weighted = roc_auc_score(Y_TRUES, Y_PROBS, multi_class='ovr', average='weighted')\n",
    "auc_pr_macro = average_precision_score(Y_TRUES, Y_PROBS, average='macro')\n",
    "auc_pr_weighted = average_precision_score(Y_TRUES, Y_PROBS, average='weighted')\n",
    "acc = accuracy_score(Y_TRUES, Y_PREDS)\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(Y_TRUES, Y_PREDS)\n",
    "print(cm)\n",
    "cm_str = '\\n'.join(['[' + ' '.join(map(str, row)) + ']' for row in cm])\n",
    "\n",
    "cm_flat = cm.flatten()\n",
    "\n",
    "results = {\n",
    "    'auc-roc-macro': f\"{auc_roc_macro:.4f}\",\n",
    "    'auc-roc-weighted': f\"{auc_roc_weighted:.4f}\",\n",
    "    'auc-pr-macro': f\"{auc_pr_macro:.4f}\",\n",
    "    'auc-pr-weighted': f\"{auc_pr_weighted:.4f}\",\n",
    "    'accuracy': f\"{acc:.4f}\",\n",
    "    'confusion_matrix': cm_str\n",
    "}\n",
    "\n",
    "for i, val in enumerate(cm_flat):\n",
    "    results[f'cm-{i}'] = str(int(val))\n",
    "\n",
    "results = pd.DataFrame([results])\n",
    "results.to_csv(os.path.join(results_dir, name + '.csv'), index=False)\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10779 11\n",
      "1 10617 173\n",
      "2 10488 302\n",
      "3 10599 191\n",
      "4 10604 186\n",
      "5 10767 23\n",
      "6 10437 353\n",
      "7 10541 249\n",
      "8 10500 290\n",
      "9 10611 179\n",
      "10 10771 19\n",
      "11 10560 230\n",
      "12 10532 258\n",
      "13 10716 74\n",
      "14 10524 266\n",
      "15 10548 242\n",
      "16 10665 125\n",
      "17 10508 282\n",
      "18 10668 122\n",
      "19 10570 220\n",
      "20 10588 202\n",
      "21 10521 269\n",
      "22 10627 163\n",
      "23 10600 190\n",
      "24 10771 19\n",
      "25 10520 270\n",
      "26 10495 295\n",
      "27 10520 270\n",
      "28 10605 185\n",
      "29 10616 174\n",
      "30 10576 214\n",
      "31 10556 234\n",
      "32 10687 103\n",
      "33 10560 230\n",
      "34 10625 165\n",
      "35 10751 39\n",
      "36 10525 265\n",
      "37 10436 354\n",
      "38 10621 169\n",
      "39 10463 327\n",
      "40 10524 266\n",
      "41 10584 206\n",
      "42 10632 158\n",
      "43 10520 270\n",
      "44 10728 62\n",
      "45 10699 91\n",
      "46 10763 27\n",
      "47 10580 210\n",
      "48 10672 118\n",
      "49 10528 262\n",
      "50 10627 163\n",
      "51 10541 249\n",
      "52 10502 288\n",
      "53 10575 215\n",
      "54 10532 258\n",
      "55 10775 15\n",
      "[[7928 1143 1284  211]\n",
      " [  15   68   16   18]\n",
      " [  18   22   33   16]\n",
      " [   0   11    3    4]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auc-roc-macro</th>\n",
       "      <th>auc-roc-weighted</th>\n",
       "      <th>auc-pr-macro</th>\n",
       "      <th>auc-pr-weighted</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>cm-0</th>\n",
       "      <th>cm-1</th>\n",
       "      <th>cm-2</th>\n",
       "      <th>cm-3</th>\n",
       "      <th>...</th>\n",
       "      <th>cm-6</th>\n",
       "      <th>cm-7</th>\n",
       "      <th>cm-8</th>\n",
       "      <th>cm-9</th>\n",
       "      <th>cm-10</th>\n",
       "      <th>cm-11</th>\n",
       "      <th>cm-12</th>\n",
       "      <th>cm-13</th>\n",
       "      <th>cm-14</th>\n",
       "      <th>cm-15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8234</td>\n",
       "      <td>0.8583</td>\n",
       "      <td>0.2743</td>\n",
       "      <td>0.9757</td>\n",
       "      <td>0.7445</td>\n",
       "      <td>[7928 1143 1284 211]\\n[15 68 16 18]\\n[18 22 33...</td>\n",
       "      <td>7928</td>\n",
       "      <td>1143</td>\n",
       "      <td>1284</td>\n",
       "      <td>211</td>\n",
       "      <td>...</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>33</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  auc-roc-macro auc-roc-weighted auc-pr-macro auc-pr-weighted accuracy  \\\n",
       "0        0.8234           0.8583       0.2743          0.9757   0.7445   \n",
       "\n",
       "                                    confusion_matrix  cm-0  cm-1  cm-2 cm-3  \\\n",
       "0  [7928 1143 1284 211]\\n[15 68 16 18]\\n[18 22 33...  7928  1143  1284  211   \n",
       "\n",
       "   ... cm-6 cm-7 cm-8 cm-9 cm-10 cm-11 cm-12 cm-13 cm-14 cm-15  \n",
       "0  ...   16   18   18   22    33    16     0    11     3     4  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "\n",
    "name = 'Naive-Bayes'.lower()\n",
    "\n",
    "Y_TRUES = np.empty([0])\n",
    "Y_PROBS = []\n",
    "Y_PREDS = np.empty([0])\n",
    "\n",
    "\n",
    "if kfold:\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    split_iterator = cv.split(x)\n",
    "else:\n",
    "    cv = LeavePGroupsOut(n_groups=1)\n",
    "    split_iterator = cv.split(x, y, groups=p)\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(split_iterator, start=1):\n",
    "    participant = i if kfold else np.unique(p[test_idx])[0]\n",
    "\n",
    "    x_train, x_test = x[train_idx], x[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    print(participant, x_train.shape[0], x_test.shape[0])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    normalizer = MinMaxScaler()\n",
    "    normalizer.fit(x_train)\n",
    "    x_train = normalizer.transform(x_train)\n",
    "    x_test = normalizer.transform(x_test)\n",
    "\n",
    "    # Oversample class 1\n",
    "\n",
    "    # ros = RandomOverSampler(sampling_strategy=.1, random_state=seed)\n",
    "    # x_train, y_train = ros.fit_resample(x_train, y_train)\n",
    "\n",
    "    # # 1. Check original class distribution\n",
    "    # print(\"Original class distribution:\", Counter(y_train))\n",
    "    # # 2. Define the minority class (adjust if needed)\n",
    "    # minority_class = 1  # change this if your minority class label is different\n",
    "    # current_minority_count = sum(y_train == minority_class)\n",
    "    # # 3. Define desired new total count for the minority class (10x)\n",
    "    # target_minority_count = current_minority_count * 10\n",
    "    # # 4. Setup SMOTE with custom sampling strategy\n",
    "    # smote = SMOTE(sampling_strategy={minority_class: target_minority_count}, random_state=seed)\n",
    "    # # 5. Fit and resample\n",
    "    # x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "    # # 6. Confirm new distribution\n",
    "    # print(\"Resampled class distribution:\", Counter(y_train))\n",
    "\n",
    "\n",
    "    # Undersample class 0\n",
    "    # data_percent = 1\n",
    "    # reduction_percent = 100 - data_percent\n",
    "    # print(\"Original class distribution:\", Counter(y_train))    \n",
    "    # class_0_idx = np.where(y_train == 0)[0]\n",
    "    # class_1_idx = np.where(y_train == 1)[0]\n",
    "    # # How many class 0 samples to keep\n",
    "    # n_keep = int(len(class_0_idx) * (1 - reduction_percent / 100.0))\n",
    "    # sampled_0_idx = np.random.choice(class_0_idx, size=n_keep, replace=False)\n",
    "    # # Combine sampled class 0 with all class 1\n",
    "    # final_idx = np.concatenate([sampled_0_idx, class_1_idx])\n",
    "    # x_train = x_train[final_idx]\n",
    "    # y_train =  y_train[final_idx]\n",
    "    # print(\"Resampled class distribution:\", Counter(y_train))\n",
    "\n",
    "\n",
    "    # ----- LightGBM\n",
    "    # train_data = lgb.Dataset(x_train, label=y_train)\n",
    "    # test_data = lgb.Dataset(x_test, label=y_test, reference=train_data)\n",
    "    # params = {\n",
    "    #     'verbose': -1,  # 👈 turn off training output\n",
    "    #     'objective': 'binary',\n",
    "    #     'metric': 'binary_logloss',  # or 'auc' if you prefer\n",
    "    #     'num_leaves': 64,\n",
    "    #     'learning_rate': 0.01,\n",
    "    #     'n_estimators': 100,\n",
    "    #     # 'is_unbalance': True  # Automatically balances positive and negative classes\n",
    "    #     # 'scale_pos_weight': (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    # }\n",
    "    # bst = lgb.train(params, train_data, valid_sets=[train_data, test_data])\n",
    "    # y_probs = bst.predict(x_test, num_iteration=bst.best_iteration)\n",
    "\n",
    "\n",
    "    # ----- Transformer\n",
    "    # model = TabPFNClassifier(\n",
    "    #     device='cuda',\n",
    "    #     random_state=seed,\n",
    "    #     n_estimators = 4,\n",
    "    #     ignore_pretraining_limits=True)\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- Gradient Boosting Classifier\n",
    "    # model = GradientBoostingClassifier(\n",
    "    #     n_estimators=100,  # match LightGBM\n",
    "    #     learning_rate=0.001,  # match LightGBM\n",
    "    #     max_depth=4,  # similar to LightGBM default tree depth\n",
    "    #     subsample=1.0,  # default\n",
    "    #     random_state=seed\n",
    "    # )\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "    \n",
    "\n",
    "    # ----- XGBoost\n",
    "    # model = XGBClassifier()\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- Random Forest\n",
    "    # model = RandomForestClassifier(\n",
    "    #     n_estimators=250,  # number of trees\n",
    "    #     max_depth=None,  # let the trees grow fully\n",
    "    #     random_state=seed\n",
    "    # )\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- SVM\n",
    "    # model = SVC(kernel='rbf', probability=True, random_state=seed)\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- DT\n",
    "    # model = DecisionTreeClassifier(criterion=\"log_loss\", max_depth=512, random_state=seed)\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict(x_test)\n",
    "\n",
    "\n",
    "    # MLP\n",
    "    # model = MLPClassifier(\n",
    "    #     hidden_layer_sizes=(128, 64),    # 2 hidden layers: 64 and 32 neurons\n",
    "    #     activation='relu',              # good default: 'relu'\n",
    "    #     solver='adam',                  # optimizer\n",
    "    #     alpha=0.0001,                   # L2 regularization\n",
    "    #     learning_rate_init=0.001,\n",
    "    #     max_iter=500,\n",
    "    #     early_stopping=True,\n",
    "    #     random_state=seed,\n",
    "    #     verbose=True,\n",
    "    # )\n",
    "    # model.fit(x_train, y_train)    \n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "    # y_preds = model.predict(x_test)\n",
    "\n",
    "\n",
    "    # LR\n",
    "    # model = LogisticRegression(\n",
    "    # penalty='l2',               # regularization (default)\n",
    "    # C=1.0,                      # inverse of regularization strength\n",
    "    # solver='lbfgs',             # optimizer (good default for small/medium data)\n",
    "    # max_iter=1000,\n",
    "    # class_weight='balanced',\n",
    "    # random_state=seed)\n",
    "    # model.fit(x_train, y_train)    \n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "    # y_preds = model.predict(x_test)\n",
    "\n",
    "\n",
    "    # NB\n",
    "    model = GaussianNB()\n",
    "    model.fit(x_train,\n",
    "              y_train,\n",
    "\n",
    "              sample_weight=compute_sample_weight(class_weight='balanced', y=y_train) if weighted else None,\n",
    "\n",
    "              )\n",
    "    y_probs = model.predict_proba(x_test)\n",
    "    y_preds = model.predict(x_test)\n",
    "\n",
    "\n",
    "    Y_TRUES = np.append(Y_TRUES, y_test)\n",
    "    Y_PROBS.append(y_probs)\n",
    "    Y_PREDS = np.append(Y_PREDS, y_preds)\n",
    "\n",
    "Y_PROBS = np.concatenate(Y_PROBS, axis=0)\n",
    "\n",
    "\n",
    "indx = Y_TRUES.argsort()\n",
    "Y_TRUES = Y_TRUES[indx]\n",
    "Y_PROBS = Y_PROBS[indx]\n",
    "Y_PREDS = Y_PREDS[indx]\n",
    "\n",
    "\n",
    "# Evaluation metrics (multiclass)\n",
    "auc_roc_macro = roc_auc_score(Y_TRUES, Y_PROBS, multi_class='ovr', average='macro')\n",
    "auc_roc_weighted = roc_auc_score(Y_TRUES, Y_PROBS, multi_class='ovr', average='weighted')\n",
    "auc_pr_macro = average_precision_score(Y_TRUES, Y_PROBS, average='macro')\n",
    "auc_pr_weighted = average_precision_score(Y_TRUES, Y_PROBS, average='weighted')\n",
    "acc = accuracy_score(Y_TRUES, Y_PREDS)\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(Y_TRUES, Y_PREDS)\n",
    "print(cm)\n",
    "cm_str = '\\n'.join(['[' + ' '.join(map(str, row)) + ']' for row in cm])\n",
    "\n",
    "cm_flat = cm.flatten()\n",
    "\n",
    "results = {\n",
    "    'auc-roc-macro': f\"{auc_roc_macro:.4f}\",\n",
    "    'auc-roc-weighted': f\"{auc_roc_weighted:.4f}\",\n",
    "    'auc-pr-macro': f\"{auc_pr_macro:.4f}\",\n",
    "    'auc-pr-weighted': f\"{auc_pr_weighted:.4f}\",\n",
    "    'accuracy': f\"{acc:.4f}\",\n",
    "    'confusion_matrix': cm_str\n",
    "}\n",
    "\n",
    "for i, val in enumerate(cm_flat):\n",
    "    results[f'cm-{i}'] = str(int(val))\n",
    "\n",
    "results = pd.DataFrame([results])\n",
    "results.to_csv(os.path.join(results_dir, name + '.csv'), index=False)\n",
    "display(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ali-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
