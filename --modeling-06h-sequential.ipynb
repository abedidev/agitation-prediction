{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3080 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, confusion_matrix, f1_score, accuracy_score, \\\n",
    "    precision_score, recall_score\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import KFold, LeavePGroupsOut\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tabpfn import TabPFNClassifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "%matplotlib inline\n",
    "# matplotlib.use('TkAgg')\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler  # install via: pip install imbalanced-learn\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "from scipy.stats import skew\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "\n",
    "from sktime.transformations.panel.rocket import Rocket\n",
    "\n",
    "\n",
    "seed = 69\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/ali/PycharmProjects/tihm/dataset'\n",
    "\n",
    "dataset_06h = pd.read_csv(os.path.join(root, 'data-06h.csv'))\n",
    "dataset_12h = pd.read_csv(os.path.join(root, 'data-12h.csv'))\n",
    "dataset_24h = pd.read_csv(os.path.join(root, 'data-24h.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(dataset_06h)\n",
    "# display(dataset_12h)\n",
    "# display(dataset_24h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400 117 89 18\n",
      "1752 135\n"
     ]
    }
   ],
   "source": [
    "# print(dataset_06h['agitation'].equals(dataset_06h['agitation-next']))\n",
    "# print(np.sum(dataset_06h['agitation']))\n",
    "# print(np.sum(dataset_06h['agitation-next']))\n",
    "\n",
    "# print(np.where(dataset_06h['agitation'] == 1))\n",
    "# print(np.where(dataset_06h['agitation-next'] == 1))\n",
    "\n",
    "print(np.sum(dataset_06h['agitation-four'] == 0),\n",
    "      np.sum(dataset_06h['agitation-four'] == 1),\n",
    "      np.sum(dataset_06h['agitation-four'] == 2),\n",
    "      np.sum(dataset_06h['agitation-four'] == 3))\n",
    "\n",
    "print(np.sum(dataset_06h['agitation-next'] == 0),\n",
    "      np.sum(dataset_06h['agitation-next'] == 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 8 32 8 5 56\n"
     ]
    }
   ],
   "source": [
    "columns_count = ['back-door', 'bathroom', 'bedroom', 'fridge-door',\n",
    "                 'front-door', 'hallway', 'kitchen', 'lounge']\n",
    "\n",
    "columns_contextual = ['total-events', 'unique-locations', 'active-location-ratio', 'private-to-public-ratio',\n",
    "                      'location-entropy', 'location-dominance-ratio', 'back-and-forth-count', 'num-transitions']\n",
    "\n",
    "columns_statistical = ['back-door-count-max', 'back-door-count-mean', 'back-door-count-std', 'back-door-count-sum',\n",
    "                       'bathroom-count-max', 'bathroom-count-mean', 'bathroom-count-std', 'bathroom-count-sum',\n",
    "                       'bedroom-count-max', 'bedroom-count-mean', 'bedroom-count-std', 'bedroom-count-sum',\n",
    "                       'fridge-door-count-max', 'fridge-door-count-mean', 'fridge-door-count-std', 'fridge-door-count-sum',\n",
    "                       'front-door-count-max', 'front-door-count-mean', 'front-door-count-std', 'front-door-count-sum',\n",
    "                       'hallway-count-max', 'hallway-count-mean', 'hallway-count-std', 'hallway-count-sum',\n",
    "                       'kitchen-count-max', 'kitchen-count-mean', 'kitchen-count-std','kitchen-count-sum',\n",
    "                       'lounge-count-max', 'lounge-count-mean', 'lounge-count-std','lounge-count-sum']\n",
    "\n",
    "columns_physiology = ['body-temperature', 'body-weight', 'diastolic-blood-pressure', 'heart-rate',\n",
    "                      'muscle-mass', 'systolic-blood-pressure', 'total-body-water', 'skin-temperature']\n",
    "\n",
    "columns_labels = ['blood-pressure', 'body-water', 'pulse', 'weight', 'body-temperature-label']\n",
    "\n",
    "# feature_names = columns_count + columns_contextual + columns_statistical + columns_physiology + columns_labels\n",
    "feature_names = columns_count + columns_contextual + columns_statistical + columns_physiology\n",
    "\n",
    "print(len(columns_count),\n",
    "      len(columns_contextual),\n",
    "      len(columns_statistical),\n",
    "      len(columns_physiology),\n",
    "      len(columns_labels),\n",
    "      len(feature_names))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10790, 67)\n",
      "(10790, 62)\n",
      "107888\n",
      "17803\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Imputation\n",
    "\n",
    "dataset = dataset_06h\n",
    "\n",
    "print(dataset.shape)\n",
    "\n",
    "dataset.drop(columns_labels, axis=1, inplace=True)\n",
    "\n",
    "print(dataset.shape)\n",
    "\n",
    "\n",
    "import utils_data\n",
    "importlib.reload(utils_data)\n",
    "from utils_data import hierarchical_imputation, hierarchical_imputation_columns_to_exclude\n",
    "\n",
    "print(dataset.isna().sum().sum())\n",
    "\n",
    "dataset = hierarchical_imputation_columns_to_exclude(dataset, columns_to_exclude=['id',\n",
    "                                                                                  'date',\n",
    "                                                                                  '6h',\n",
    "                                                                                  'agitation',\n",
    "                                                                                  'agitation-next',\n",
    "                                                                                  'agitation-four'])\n",
    "print(dataset.isna().sum().sum())\n",
    "temp = dataset.drop([                                                             'id',\n",
    "                                                                                  'date',\n",
    "                                                                                  '6h',\n",
    "                                                                                  'agitation',\n",
    "                                                                                  'agitation-next',\n",
    "                                                                                  'agitation-four'], axis=1, inplace=False)\n",
    "print(temp.isna().sum().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10622, 3, 56) (10622,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[  0.    ,   0.    ,   0.    , ..., 136.4271,  49.8206,\n",
       "          34.2843],\n",
       "        [  0.    ,   0.    ,   0.    , ..., 136.4271,  49.8206,\n",
       "          34.2843],\n",
       "        [ 12.    ,   5.    ,  18.    , ..., 165.    ,  50.6   ,\n",
       "          34.2843]],\n",
       "\n",
       "       [[  0.    ,   0.    ,   0.    , ..., 136.4271,  49.8206,\n",
       "          34.2843],\n",
       "        [ 12.    ,   5.    ,  18.    , ..., 165.    ,  50.6   ,\n",
       "          34.2843],\n",
       "        [  2.    ,   2.    ,   6.    , ..., 136.4271,  49.8206,\n",
       "          34.2843]],\n",
       "\n",
       "       [[ 12.    ,   5.    ,  18.    , ..., 165.    ,  50.6   ,\n",
       "          34.2843],\n",
       "        [  2.    ,   2.    ,   6.    , ..., 136.4271,  49.8206,\n",
       "          34.2843],\n",
       "        [  0.    ,   0.    ,   3.    , ..., 130.7188,  51.76  ,\n",
       "          34.56  ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[  0.    ,  10.    ,  21.    , ..., 130.7188,  51.76  ,\n",
       "          34.56  ],\n",
       "        [  0.    ,  10.    ,  19.    , ..., 130.7188,  51.76  ,\n",
       "          34.56  ],\n",
       "        [  0.    ,  13.    ,  16.    , ..., 130.7188,  51.76  ,\n",
       "          34.56  ]],\n",
       "\n",
       "       [[  0.    ,  10.    ,  19.    , ..., 130.7188,  51.76  ,\n",
       "          34.56  ],\n",
       "        [  0.    ,  13.    ,  16.    , ..., 130.7188,  51.76  ,\n",
       "          34.56  ],\n",
       "        [  0.    ,   0.    ,   0.    , ..., 134.    ,  51.1929,\n",
       "          34.6478]],\n",
       "\n",
       "       [[  0.    ,  13.    ,  16.    , ..., 130.7188,  51.76  ,\n",
       "          34.56  ],\n",
       "        [  0.    ,   0.    ,   0.    , ..., 134.    ,  51.1929,\n",
       "          34.6478],\n",
       "        [  0.    ,  13.    ,  34.    , ..., 134.    ,  51.1929,\n",
       "          34.6478]]], shape=(10622, 3, 56))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], shape=(10622,))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "np.int64(135)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sequential\n",
    "\n",
    "segment_length = 3\n",
    "\n",
    "import utils_data\n",
    "importlib.reload(utils_data)\n",
    "from utils_data import segment_dataframe\n",
    "\n",
    "dataset.fillna(0, inplace=True)\n",
    "\n",
    "sements, labels = segment_dataframe(dataset, segment_length)\n",
    "\n",
    "x = np.array(sements)\n",
    "y = np.array(labels)\n",
    "\n",
    "print(x.shape, y.shape)\n",
    "display(x, y)\n",
    "\n",
    "np.sum(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Ablation\n",
    "# dataset_CNT_STA_CTX_PHY_LBL = dataset\n",
    "# dataset_CNT_STA_CTX_PHY_LBL.to_csv(os.path.join(root,   'dataset_06h_imputed_CNT_STA_CTX_PHY_LBL.csv'), index=False)\n",
    "# print(dataset_CNT_STA_CTX_PHY_LBL.shape)\n",
    "\n",
    "# dataset_CNT_STA_CTX_PHY = dataset.drop(columns_labels, axis=1, inplace=False)\n",
    "# dataset_CNT_STA_CTX_PHY.to_csv(os.path.join(root,       'dataset_06h_imputed_CNT_STA_CTX_PHY.csv'), index=False)\n",
    "# print(dataset_CNT_STA_CTX_PHY.shape)\n",
    "\n",
    "# dataset_CNT_STA_CTX = dataset.drop(columns_physiology, axis=1, inplace=False)\n",
    "# dataset_CNT_STA_CTX.to_csv(os.path.join(root,       'dataset_06h_imputed_CNT_STA_CTX.csv'), index=False)\n",
    "# print(dataset_CNT_STA_CTX.shape)\n",
    "\n",
    "\n",
    "# dataset_CNT_STA = dataset.drop(columns_contextual + columns_physiology, axis=1, inplace=False)\n",
    "# dataset_CNT_STA.to_csv(os.path.join(root,           'dataset_06h_imputed_CNT_STA.csv'), index=False)\n",
    "# print(dataset_CNT_STA.shape)\n",
    "\n",
    "# dataset_CNT = dataset.drop(columns_statistical + columns_contextual + columns_physiology, axis=1, inplace=False)\n",
    "# dataset_CNT.to_csv(os.path.join(root,           'dataset_06h_imputed_CNT.csv'), index=False)\n",
    "# print(dataset_CNT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 8497 2125\n"
     ]
    }
   ],
   "source": [
    "# ROCKET\n",
    "model_name = 'rocket'\n",
    "\n",
    "Y_TRUES = np.empty([0])\n",
    "Y_PROBS = np.empty([0])\n",
    "Y_PREDS = np.empty([0])\n",
    "\n",
    "\n",
    "# cv = LeavePGroupsOut(n_groups=1)\n",
    "# for train_idx, test_idx in cv.split(x, y, groups=p):\n",
    "#     participant = np.unique(p[test_idx])[0]\n",
    "\n",
    "\n",
    "# cv = KFold(n_splits=x.shape[0], shuffle=True, random_state=seed)\n",
    "# for fold, (train_idx, test_idx) in enumerate(cv.split(x), start=1):\n",
    "#     participant = np.unique(p[test_idx])[0]\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "for fold, (train_idx, test_idx) in enumerate(cv.split(x), start=1):\n",
    "    participant = fold\n",
    "\n",
    "    x_train, x_test = x[train_idx], x[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    print(participant, x_train.shape[0], x_test.shape[0])\n",
    "\n",
    "\n",
    "    num_samples, time_steps, num_features = x_train.shape\n",
    "    x_train_flat = x_train.reshape(-1, num_features)\n",
    "    x_test_flat = x_test.reshape(-1, num_features)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train_flat)\n",
    "    x_train_scaled = scaler.transform(x_train_flat)\n",
    "    x_test_scaled = scaler.transform(x_test_flat)\n",
    "\n",
    "    normalizer = MinMaxScaler()\n",
    "    normalizer.fit(x_train_scaled)\n",
    "    x_train_scaled = normalizer.transform(x_train_scaled)\n",
    "    x_test_scaled = normalizer.transform(x_test_scaled)\n",
    "\n",
    "    x_train = x_train_scaled.reshape(num_samples, time_steps, num_features)\n",
    "    x_test = x_test_scaled.reshape(x_test.shape[0], time_steps, num_features)\n",
    "\n",
    "    # Oversample class 1\n",
    "\n",
    "    # ros = RandomOverSampler(sampling_strategy=.1, random_state=seed)\n",
    "    # x_train, y_train = ros.fit_resample(x_train, y_train)\n",
    "\n",
    "    # # 1. Check original class distribution\n",
    "    # print(\"Original class distribution:\", Counter(y_train))\n",
    "    # # 2. Define the minority class (adjust if needed)\n",
    "    # minority_class = 1  # change this if your minority class label is different\n",
    "    # current_minority_count = sum(y_train == minority_class)\n",
    "    # # 3. Define desired new total count for the minority class (10x)\n",
    "    # target_minority_count = current_minority_count * 10\n",
    "    # # 4. Setup SMOTE with custom sampling strategy\n",
    "    # smote = SMOTE(sampling_strategy={minority_class: target_minority_count}, random_state=seed)\n",
    "    # # 5. Fit and resample\n",
    "    # x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "    # # 6. Confirm new distribution\n",
    "    # print(\"Resampled class distribution:\", Counter(y_train))\n",
    "\n",
    "\n",
    "    rocket = Rocket(num_kernels=1_000)\n",
    "    rocket.fit(x_train)\n",
    "    x_train_transformed = rocket.transform(x_train)\n",
    "    x_test_transformed = rocket.transform(x_test)\n",
    "\n",
    "\n",
    "    # ----- Gradient Boosting Classifier\n",
    "    model = GradientBoostingClassifier(\n",
    "        n_estimators=100,  # match LightGBM\n",
    "        learning_rate=0.001,  # match LightGBM\n",
    "        max_depth=4,  # similar to LightGBM default tree depth\n",
    "        subsample=1.0,  # default\n",
    "        random_state=seed\n",
    "    )\n",
    "    # model.fit(x_train, y_train)\n",
    "    model.fit(x_train_transformed, y_train, sample_weight=compute_sample_weight(class_weight='balanced', y=y_train))\n",
    "    y_probs = model.predict_proba(x_test_transformed)[:, 1]\n",
    "    \n",
    "\n",
    "    Y_TRUES = np.append(Y_TRUES, y_test)\n",
    "    Y_PROBS = np.append(Y_PROBS, y_probs)\n",
    "    Y_PREDS = np.append(Y_PREDS, (y_probs >= .5).astype(int))\n",
    "\n",
    "\n",
    "indx = Y_TRUES.argsort()\n",
    "Y_TRUES = Y_TRUES[indx]\n",
    "Y_PROBS = Y_PROBS[indx]\n",
    "Y_PREDS = Y_PREDS[indx]\n",
    "\n",
    "\n",
    "# Evaluation metrics\n",
    "auc_roc = roc_auc_score(Y_TRUES, Y_PROBS)\n",
    "precision, recall, _ = precision_recall_curve(Y_TRUES, Y_PROBS)\n",
    "auc_pr = auc(recall, precision)\n",
    "acc = accuracy_score(Y_TRUES, Y_PREDS)\n",
    "pre = precision_score(Y_TRUES, Y_PREDS)\n",
    "rec = recall_score(Y_TRUES, Y_PREDS)\n",
    "f1 = f1_score(Y_TRUES, Y_PREDS)\n",
    "\n",
    "# Confusion matrix: [ [TN, FP], [FN, TP] ]\n",
    "tn, fp, fn, tp = confusion_matrix(Y_TRUES, Y_PREDS).ravel()\n",
    "\n",
    "# Sensitivity = Recall = TP / (TP + FN)\n",
    "sensitivity = tp / (tp + fn)\n",
    "\n",
    "# Specificity = TN / (TN + FP)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# Print results\n",
    "print(f\"AUC-ROC:    {auc_roc:.4f}\")\n",
    "print(f\"AUC-PR:     {auc_pr:.4f}\")\n",
    "print(f\"Accuracy:   {acc:.4f}\")\n",
    "print(f\"Precision:  {pre:.4f}\")\n",
    "print(f\"Recall:     {rec:.4f}\")\n",
    "print(f\"F1 Score:   {f1:.4f}\")\n",
    "print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "print(f\"Specificity:          {specificity:.4f}\")\n",
    "print(confusion_matrix(Y_TRUES, Y_PREDS))\n",
    "\n",
    "\n",
    "metrics = {\n",
    "    'auc-roc': f\"{auc_roc:.4f}\",\n",
    "    'auc-pr': f\"{auc_pr:.4f}\",\n",
    "    'accuracy': f\"{acc:.4f}\",\n",
    "    'precision': f\"{pre:.4f}\",\n",
    "    'recall': f\"{rec:.4f}\",\n",
    "    'f1-score': f\"{f1:.4f}\",\n",
    "    'sensitivity': f\"{sensitivity:.4f}\",\n",
    "    'specificity': f\"{specificity:.4f}\",\n",
    "    'tn': str(int(tn)),\n",
    "    'fp': str(int(fp)),\n",
    "    'fn': str(int(fn)),\n",
    "    'tp': str(int(tp))\n",
    "}\n",
    "\n",
    "pd.DataFrame([metrics]).to_csv(\n",
    "    model_name + '.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ali-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
