{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3080 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, confusion_matrix, f1_score, accuracy_score, \\\n",
    "    precision_score, recall_score\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import KFold, LeavePGroupsOut\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tabpfn import TabPFNClassifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "%matplotlib inline\n",
    "# matplotlib.use('TkAgg')\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler  # install via: pip install imbalanced-learn\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "from scipy.stats import skew\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "seed = 69\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/ali/PycharmProjects/tihm/dataset'\n",
    "\n",
    "dataset_06h = pd.read_csv(os.path.join(root, 'data-06h.csv'))\n",
    "dataset_12h = pd.read_csv(os.path.join(root, 'data-12h.csv'))\n",
    "dataset_24h = pd.read_csv(os.path.join(root, 'data-24h.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(dataset_06h)\n",
    "# display(dataset_12h)\n",
    "# display(dataset_24h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1400 117 89 18\n",
      "1752 135\n"
     ]
    }
   ],
   "source": [
    "# print(dataset_06h['agitation'].equals(dataset_06h['agitation-next']))\n",
    "# print(np.sum(dataset_06h['agitation']))\n",
    "# print(np.sum(dataset_06h['agitation-next']))\n",
    "\n",
    "# print(np.where(dataset_06h['agitation'] == 1))\n",
    "# print(np.where(dataset_06h['agitation-next'] == 1))\n",
    "\n",
    "print(np.sum(dataset_06h['agitation-four'] == 0),\n",
    "      np.sum(dataset_06h['agitation-four'] == 1),\n",
    "      np.sum(dataset_06h['agitation-four'] == 2),\n",
    "      np.sum(dataset_06h['agitation-four'] == 3))\n",
    "\n",
    "print(np.sum(dataset_06h['agitation-next'] == 0),\n",
    "      np.sum(dataset_06h['agitation-next'] == 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['agitation-next'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m label = \u001b[33m'\u001b[39m\u001b[33magitation-next\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# label = 'agitation-four'\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m y = np.array(\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m).squeeze()\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Binary\u001b[39;00m\n\u001b[32m     10\u001b[39m y[y == -\u001b[32m1\u001b[39m] = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ali-env/lib/python3.13/site-packages/pandas/core/frame.py:4108\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4107\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4108\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4110\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ali-env/lib/python3.13/site-packages/pandas/core/indexes/base.py:6200\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6197\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6198\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6200\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6202\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6204\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/ali-env/lib/python3.13/site-packages/pandas/core/indexes/base.py:6249\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6247\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[32m   6248\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nmissing == \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[32m-> \u001b[39m\u001b[32m6249\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6251\u001b[39m     not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m   6252\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"None of [Index(['agitation-next'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# dataset = dataset_06h\n",
    "dataset = dataset_12h\n",
    "# label = 'agitation'\n",
    "label = 'agitation-next'\n",
    "# label = 'agitation-four'\n",
    "\n",
    "y = np.array(dataset[[label]]).squeeze()\n",
    "\n",
    "# Binary\n",
    "y[y == -1] = 0\n",
    "y[y >= 1] = 1\n",
    "\n",
    "# QUATERNARY\n",
    "# y[y == -1] = 0\n",
    "# y[y == -10] = 0\n",
    "\n",
    "dataset.drop(['agitation',\n",
    "              'agitation-next',\n",
    "              'agitation-four'],\n",
    "              axis=1, inplace=True)\n",
    "\n",
    "ids = np.array(dataset['id']).squeeze()\n",
    "p = np.unique(ids, return_inverse=True)[1]\n",
    "\n",
    "print(np.isnan(y).sum())\n",
    "y = np.nan_to_num(y, nan=0)\n",
    "print(np.isnan(y).sum())\n",
    "print(np.isnan(p).sum())\n",
    "\n",
    "print(np.unique(y))\n",
    "print(y.sum())\n",
    "display(y)\n",
    "display(p)\n",
    "# display(dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>12h</th>\n",
       "      <th>back-door</th>\n",
       "      <th>bathroom</th>\n",
       "      <th>bedroom</th>\n",
       "      <th>fridge-door</th>\n",
       "      <th>front-door</th>\n",
       "      <th>hallway</th>\n",
       "      <th>kitchen</th>\n",
       "      <th>...</th>\n",
       "      <th>heart-rate</th>\n",
       "      <th>muscle-mass</th>\n",
       "      <th>systolic-blood-pressure</th>\n",
       "      <th>total-body-water</th>\n",
       "      <th>skin-temperature</th>\n",
       "      <th>blood-pressure</th>\n",
       "      <th>body-water</th>\n",
       "      <th>pulse</th>\n",
       "      <th>weight</th>\n",
       "      <th>body-temperature-label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0697d</td>\n",
       "      <td>2019-06-28</td>\n",
       "      <td>00-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0697d</td>\n",
       "      <td>2019-06-28</td>\n",
       "      <td>12-24</td>\n",
       "      <td>14.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>...</td>\n",
       "      <td>42.0</td>\n",
       "      <td>64.5</td>\n",
       "      <td>165.0</td>\n",
       "      <td>50.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0697d</td>\n",
       "      <td>2019-06-29</td>\n",
       "      <td>00-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0697d</td>\n",
       "      <td>2019-06-29</td>\n",
       "      <td>12-24</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>...</td>\n",
       "      <td>60.0</td>\n",
       "      <td>64.6</td>\n",
       "      <td>157.0</td>\n",
       "      <td>51.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0697d</td>\n",
       "      <td>2019-06-30</td>\n",
       "      <td>00-12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5376</th>\n",
       "      <td>fd100</td>\n",
       "      <td>2019-06-28</td>\n",
       "      <td>00-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5377</th>\n",
       "      <td>fd100</td>\n",
       "      <td>2019-06-28</td>\n",
       "      <td>12-24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>...</td>\n",
       "      <td>61.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>138.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5378</th>\n",
       "      <td>fd100</td>\n",
       "      <td>2019-06-29</td>\n",
       "      <td>00-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5379</th>\n",
       "      <td>fd100</td>\n",
       "      <td>2019-06-29</td>\n",
       "      <td>12-24</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5380</th>\n",
       "      <td>fd100</td>\n",
       "      <td>2019-06-30</td>\n",
       "      <td>00-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5381 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id        date    12h  back-door  bathroom  bedroom  fridge-door  \\\n",
       "0     0697d  2019-06-28  00-12        0.0       0.0      0.0          0.0   \n",
       "1     0697d  2019-06-28  12-24       14.0       7.0     24.0         23.0   \n",
       "2     0697d  2019-06-29  00-12        0.0       7.0     17.0          0.0   \n",
       "3     0697d  2019-06-29  12-24        2.0       4.0      9.0          8.0   \n",
       "4     0697d  2019-06-30  00-12        2.0      14.0     43.0          0.0   \n",
       "...     ...         ...    ...        ...       ...      ...          ...   \n",
       "5376  fd100  2019-06-28  00-12        0.0      22.0     41.0          0.0   \n",
       "5377  fd100  2019-06-28  12-24        0.0      10.0     50.0          7.0   \n",
       "5378  fd100  2019-06-29  00-12        0.0      10.0     21.0          7.0   \n",
       "5379  fd100  2019-06-29  12-24        0.0      23.0     35.0         20.0   \n",
       "5380  fd100  2019-06-30  00-12        0.0      13.0     34.0          5.0   \n",
       "\n",
       "      front-door  hallway  kitchen  ...  heart-rate  muscle-mass  \\\n",
       "0            0.0      0.0      0.0  ...         NaN          NaN   \n",
       "1           28.0     40.0    106.0  ...        42.0         64.5   \n",
       "2            8.0     22.0     36.0  ...         NaN          NaN   \n",
       "3           15.0     35.0     84.0  ...        60.0         64.6   \n",
       "4            2.0     22.0     38.0  ...         NaN          NaN   \n",
       "...          ...      ...      ...  ...         ...          ...   \n",
       "5376        16.0     31.0     52.0  ...         NaN          NaN   \n",
       "5377         5.0     27.0     93.0  ...        61.0          NaN   \n",
       "5378         3.0     13.0     32.0  ...         NaN          NaN   \n",
       "5379        12.0     48.0     78.0  ...         NaN          NaN   \n",
       "5380         8.0     17.0     38.0  ...         NaN          NaN   \n",
       "\n",
       "      systolic-blood-pressure  total-body-water  skin-temperature  \\\n",
       "0                         NaN               NaN               NaN   \n",
       "1                       165.0              50.6               NaN   \n",
       "2                         NaN               NaN               NaN   \n",
       "3                       157.0              51.2               NaN   \n",
       "4                         NaN               NaN               NaN   \n",
       "...                       ...               ...               ...   \n",
       "5376                      NaN               NaN               NaN   \n",
       "5377                    138.0               NaN               NaN   \n",
       "5378                      NaN               NaN               NaN   \n",
       "5379                      NaN               NaN               NaN   \n",
       "5380                      NaN               NaN               NaN   \n",
       "\n",
       "      blood-pressure  body-water  pulse  weight  body-temperature-label  \n",
       "0                0.0         0.0    0.0     0.0                     0.0  \n",
       "1                1.0         0.0    1.0     0.0                     0.0  \n",
       "2                0.0         0.0    0.0     0.0                     0.0  \n",
       "3                1.0         0.0    0.0     0.0                     0.0  \n",
       "4                0.0         0.0    0.0     0.0                     0.0  \n",
       "...              ...         ...    ...     ...                     ...  \n",
       "5376             NaN         NaN    NaN     NaN                     NaN  \n",
       "5377             NaN         NaN    NaN     NaN                     NaN  \n",
       "5378             NaN         NaN    NaN     NaN                     NaN  \n",
       "5379             NaN         NaN    NaN     NaN                     NaN  \n",
       "5380             NaN         NaN    NaN     NaN                     NaN  \n",
       "\n",
       "[5381 rows x 64 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id' 'date' '12h' 'back-door' 'bathroom' 'bedroom' 'fridge-door'\n",
      " 'front-door' 'hallway' 'kitchen' 'lounge' 'total-events'\n",
      " 'unique-locations' 'active-location-ratio' 'private-to-public-ratio'\n",
      " 'location-entropy' 'location-dominance-ratio' 'back-and-forth-count'\n",
      " 'num-transitions' 'back-door-count-max' 'back-door-count-mean'\n",
      " 'back-door-count-std' 'back-door-count-sum' 'bathroom-count-max'\n",
      " 'bathroom-count-mean' 'bathroom-count-std' 'bathroom-count-sum'\n",
      " 'bedroom-count-max' 'bedroom-count-mean' 'bedroom-count-std'\n",
      " 'bedroom-count-sum' 'fridge-door-count-max' 'fridge-door-count-mean'\n",
      " 'fridge-door-count-std' 'fridge-door-count-sum' 'front-door-count-max'\n",
      " 'front-door-count-mean' 'front-door-count-std' 'front-door-count-sum'\n",
      " 'hallway-count-max' 'hallway-count-mean' 'hallway-count-std'\n",
      " 'hallway-count-sum' 'kitchen-count-max' 'kitchen-count-mean'\n",
      " 'kitchen-count-std' 'kitchen-count-sum' 'lounge-count-max'\n",
      " 'lounge-count-mean' 'lounge-count-std' 'lounge-count-sum'\n",
      " 'body-temperature' 'body-weight' 'diastolic-blood-pressure' 'heart-rate'\n",
      " 'muscle-mass' 'systolic-blood-pressure' 'total-body-water'\n",
      " 'skin-temperature' 'blood-pressure' 'body-water' 'pulse' 'weight'\n",
      " 'body-temperature-label']\n"
     ]
    }
   ],
   "source": [
    "columns = dataset.columns\n",
    "dataset = dataset[columns]\n",
    "display(dataset)\n",
    "print(dataset.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 8 32 13\n"
     ]
    }
   ],
   "source": [
    "columns_count = ['back-door', 'bathroom', 'bedroom', 'fridge-door',\n",
    "                 'front-door', 'hallway', 'kitchen', 'lounge']\n",
    "\n",
    "columns_contextual = ['total-events', 'unique-locations', 'active-location-ratio', 'private-to-public-ratio',\n",
    "                      'location-entropy', 'location-dominance-ratio', 'back-and-forth-count', 'num-transitions']\n",
    "\n",
    "columns_statistical = ['back-door-count-max', 'back-door-count-mean', 'back-door-count-std', 'back-door-count-sum',\n",
    "                       'bathroom-count-max', 'bathroom-count-mean', 'bathroom-count-std', 'bathroom-count-sum',\n",
    "                       'bedroom-count-max', 'bedroom-count-mean', 'bedroom-count-std', 'bedroom-count-sum',\n",
    "                       'fridge-door-count-max', 'fridge-door-count-mean', 'fridge-door-count-std', 'fridge-door-count-sum',\n",
    "                       'front-door-count-max', 'front-door-count-mean', 'front-door-count-std', 'front-door-count-sum',\n",
    "                       'hallway-count-max', 'hallway-count-mean', 'hallway-count-std', 'hallway-count-sum',\n",
    "                       'kitchen-count-max', 'kitchen-count-mean', 'kitchen-count-std','kitchen-count-sum',\n",
    "                       'lounge-count-max', 'lounge-count-mean', 'lounge-count-std','lounge-count-sum']\n",
    "\n",
    "columns_physiology = ['body-temperature', 'body-weight', 'diastolic-blood-pressure', 'heart-rate',\n",
    "                      'muscle-mass', 'systolic-blood-pressure', 'total-body-water', 'skin-temperature',\n",
    "                      'blood-pressure', 'body-water', 'pulse', 'weight',\n",
    "                      'body-temperature-label']\n",
    "\n",
    "print(len(columns_count),\n",
    "      len(columns_contextual),\n",
    "      len(columns_statistical),\n",
    "      len(columns_physiology))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62400\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Imputation\n",
    "import utils_data\n",
    "importlib.reload(utils_data)\n",
    "from utils_data import hierarchical_imputation\n",
    "\n",
    "print(dataset.isna().sum().sum())\n",
    "\n",
    "dataset = hierarchical_imputation(dataset)\n",
    "\n",
    "print(dataset.isna().sum().sum())\n",
    "\n",
    "dataset.drop([\n",
    "    'id',\n",
    "    'date',\n",
    "    '12h',\n",
    "    ], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5381, 61)\n",
      "(5381, 48)\n",
      "(5381, 40)\n",
      "(5381, 8)\n",
      "['back-door' 'bathroom' 'bedroom' 'fridge-door' 'front-door' 'hallway'\n",
      " 'kitchen' 'lounge' 'total-events' 'unique-locations'\n",
      " 'active-location-ratio' 'private-to-public-ratio' 'location-entropy'\n",
      " 'location-dominance-ratio' 'back-and-forth-count' 'num-transitions'\n",
      " 'back-door-count-max' 'back-door-count-mean' 'back-door-count-std'\n",
      " 'back-door-count-sum' 'bathroom-count-max' 'bathroom-count-mean'\n",
      " 'bathroom-count-std' 'bathroom-count-sum' 'bedroom-count-max'\n",
      " 'bedroom-count-mean' 'bedroom-count-std' 'bedroom-count-sum'\n",
      " 'fridge-door-count-max' 'fridge-door-count-mean' 'fridge-door-count-std'\n",
      " 'fridge-door-count-sum' 'front-door-count-max' 'front-door-count-mean'\n",
      " 'front-door-count-std' 'front-door-count-sum' 'hallway-count-max'\n",
      " 'hallway-count-mean' 'hallway-count-std' 'hallway-count-sum'\n",
      " 'kitchen-count-max' 'kitchen-count-mean' 'kitchen-count-std'\n",
      " 'kitchen-count-sum' 'lounge-count-max' 'lounge-count-mean'\n",
      " 'lounge-count-std' 'lounge-count-sum' 'body-temperature' 'body-weight'\n",
      " 'diastolic-blood-pressure' 'heart-rate' 'muscle-mass'\n",
      " 'systolic-blood-pressure' 'total-body-water' 'skin-temperature'\n",
      " 'blood-pressure' 'body-water' 'pulse' 'weight' 'body-temperature-label']\n"
     ]
    }
   ],
   "source": [
    "# Ablation\n",
    "dataset_CNT_STA_CTX_PHY = dataset\n",
    "dataset_CNT_STA_CTX_PHY.to_csv(os.path.join(root,   'dataset_12h_imputed_CNT_STA_CTX_PHY.csv'), index=False)\n",
    "print(dataset_CNT_STA_CTX_PHY.shape)\n",
    "\n",
    "dataset_CNT_STA_CTX = dataset.drop(columns_physiology, axis=1, inplace=False)\n",
    "dataset_CNT_STA_CTX.to_csv(os.path.join(root,       'dataset_12h_imputed_CNT_STA_CTX.csv'), index=False)\n",
    "print(dataset_CNT_STA_CTX.shape)\n",
    "\n",
    "\n",
    "dataset_CNT_STA = dataset.drop(columns_contextual + columns_physiology, axis=1, inplace=False)\n",
    "dataset_CNT_STA.to_csv(os.path.join(root,           'dataset_12h_imputed_CNT_STA.csv'), index=False)\n",
    "print(dataset_CNT_STA.shape)\n",
    "\n",
    "dataset_CNT = dataset.drop(columns_statistical + columns_contextual + columns_physiology, axis=1, inplace=False)\n",
    "dataset_CNT.to_csv(os.path.join(root,           'dataset_12h_imputed_CNT.csv'), index=False)\n",
    "print(dataset_CNT.shape)\n",
    "\n",
    "print(dataset.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(y.shape)\n",
    "# display(dataset)\n",
    "\n",
    "# dataset.to_csv(os.path.join(root, 'dataset_06h_imputed.csv'))\n",
    "# np.save('dataset_06h_y.npy', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['back-door' 'bathroom' 'bedroom' 'fridge-door' 'front-door' 'hallway'\n",
      " 'kitchen' 'lounge' 'total-events' 'unique-locations'\n",
      " 'active-location-ratio' 'private-to-public-ratio' 'location-entropy'\n",
      " 'location-dominance-ratio' 'back-and-forth-count' 'num-transitions'\n",
      " 'back-door-count-max' 'back-door-count-mean' 'back-door-count-std'\n",
      " 'back-door-count-sum' 'bathroom-count-max' 'bathroom-count-mean'\n",
      " 'bathroom-count-std' 'bathroom-count-sum' 'bedroom-count-max'\n",
      " 'bedroom-count-mean' 'bedroom-count-std' 'bedroom-count-sum'\n",
      " 'fridge-door-count-max' 'fridge-door-count-mean' 'fridge-door-count-std'\n",
      " 'fridge-door-count-sum' 'front-door-count-max' 'front-door-count-mean'\n",
      " 'front-door-count-std' 'front-door-count-sum' 'hallway-count-max'\n",
      " 'hallway-count-mean' 'hallway-count-std' 'hallway-count-sum'\n",
      " 'kitchen-count-max' 'kitchen-count-mean' 'kitchen-count-std'\n",
      " 'kitchen-count-sum' 'lounge-count-max' 'lounge-count-mean'\n",
      " 'lounge-count-std' 'lounge-count-sum' 'body-temperature' 'body-weight'\n",
      " 'diastolic-blood-pressure' 'heart-rate' 'muscle-mass'\n",
      " 'systolic-blood-pressure' 'total-body-water' 'skin-temperature'\n",
      " 'blood-pressure' 'body-water' 'pulse' 'weight' 'body-temperature-label']\n",
      "(5381,)\n",
      "(5381, 61)\n",
      "[0. 1.]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>back-door</th>\n",
       "      <th>bathroom</th>\n",
       "      <th>bedroom</th>\n",
       "      <th>fridge-door</th>\n",
       "      <th>front-door</th>\n",
       "      <th>hallway</th>\n",
       "      <th>kitchen</th>\n",
       "      <th>lounge</th>\n",
       "      <th>total-events</th>\n",
       "      <th>unique-locations</th>\n",
       "      <th>...</th>\n",
       "      <th>heart-rate</th>\n",
       "      <th>muscle-mass</th>\n",
       "      <th>systolic-blood-pressure</th>\n",
       "      <th>total-body-water</th>\n",
       "      <th>skin-temperature</th>\n",
       "      <th>blood-pressure</th>\n",
       "      <th>body-water</th>\n",
       "      <th>pulse</th>\n",
       "      <th>weight</th>\n",
       "      <th>body-temperature-label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>72.1250</td>\n",
       "      <td>47.5853</td>\n",
       "      <td>136.4271</td>\n",
       "      <td>49.8206</td>\n",
       "      <td>34.2843</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>42.0000</td>\n",
       "      <td>64.5000</td>\n",
       "      <td>165.0000</td>\n",
       "      <td>50.6000</td>\n",
       "      <td>34.2843</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>75.7273</td>\n",
       "      <td>48.6600</td>\n",
       "      <td>130.7424</td>\n",
       "      <td>51.7600</td>\n",
       "      <td>34.5600</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>84.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>60.0000</td>\n",
       "      <td>64.6000</td>\n",
       "      <td>157.0000</td>\n",
       "      <td>51.2000</td>\n",
       "      <td>34.5600</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>67.4375</td>\n",
       "      <td>45.3364</td>\n",
       "      <td>136.6875</td>\n",
       "      <td>50.1000</td>\n",
       "      <td>34.3144</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5376</th>\n",
       "      <td>0.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>207.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>72.1250</td>\n",
       "      <td>47.5853</td>\n",
       "      <td>136.4271</td>\n",
       "      <td>49.8206</td>\n",
       "      <td>34.2843</td>\n",
       "      <td>0.2222</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5377</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>267.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>61.0000</td>\n",
       "      <td>47.5853</td>\n",
       "      <td>138.0000</td>\n",
       "      <td>49.8206</td>\n",
       "      <td>34.2843</td>\n",
       "      <td>0.2222</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5378</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>75.7273</td>\n",
       "      <td>48.6600</td>\n",
       "      <td>130.7424</td>\n",
       "      <td>51.7600</td>\n",
       "      <td>34.5600</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5379</th>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>75.7273</td>\n",
       "      <td>48.6600</td>\n",
       "      <td>130.7424</td>\n",
       "      <td>51.7600</td>\n",
       "      <td>34.5600</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.3750</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5380</th>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>67.4375</td>\n",
       "      <td>45.3364</td>\n",
       "      <td>136.6875</td>\n",
       "      <td>50.1000</td>\n",
       "      <td>34.3144</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3333</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5381 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      back-door  bathroom  bedroom  fridge-door  front-door  hallway  kitchen  \\\n",
       "0           0.0       0.0      0.0          0.0         0.0      0.0      0.0   \n",
       "1          14.0       7.0     24.0         23.0        28.0     40.0    106.0   \n",
       "2           0.0       7.0     17.0          0.0         8.0     22.0     36.0   \n",
       "3           2.0       4.0      9.0          8.0        15.0     35.0     84.0   \n",
       "4           2.0      14.0     43.0          0.0         2.0     22.0     38.0   \n",
       "...         ...       ...      ...          ...         ...      ...      ...   \n",
       "5376        0.0      22.0     41.0          0.0        16.0     31.0     52.0   \n",
       "5377        0.0      10.0     50.0          7.0         5.0     27.0     93.0   \n",
       "5378        0.0      10.0     21.0          7.0         3.0     13.0     32.0   \n",
       "5379        0.0      23.0     35.0         20.0        12.0     48.0     78.0   \n",
       "5380        0.0      13.0     34.0          5.0         8.0     17.0     38.0   \n",
       "\n",
       "      lounge  total-events  unique-locations  ...  heart-rate  muscle-mass  \\\n",
       "0        0.0           0.0               0.0  ...     72.1250      47.5853   \n",
       "1       80.0         322.0               8.0  ...     42.0000      64.5000   \n",
       "2       18.0         108.0               6.0  ...     75.7273      48.6600   \n",
       "3       99.0         256.0               8.0  ...     60.0000      64.6000   \n",
       "4       25.0         146.0               7.0  ...     67.4375      45.3364   \n",
       "...      ...           ...               ...  ...         ...          ...   \n",
       "5376    45.0         207.0               6.0  ...     72.1250      47.5853   \n",
       "5377    75.0         267.0               7.0  ...     61.0000      47.5853   \n",
       "5378    16.0         102.0               7.0  ...     75.7273      48.6600   \n",
       "5379    80.0         296.0               7.0  ...     75.7273      48.6600   \n",
       "5380    31.0         146.0               7.0  ...     67.4375      45.3364   \n",
       "\n",
       "      systolic-blood-pressure  total-body-water  skin-temperature  \\\n",
       "0                    136.4271           49.8206           34.2843   \n",
       "1                    165.0000           50.6000           34.2843   \n",
       "2                    130.7424           51.7600           34.5600   \n",
       "3                    157.0000           51.2000           34.5600   \n",
       "4                    136.6875           50.1000           34.3144   \n",
       "...                       ...               ...               ...   \n",
       "5376                 136.4271           49.8206           34.2843   \n",
       "5377                 138.0000           49.8206           34.2843   \n",
       "5378                 130.7424           51.7600           34.5600   \n",
       "5379                 130.7424           51.7600           34.5600   \n",
       "5380                 136.6875           50.1000           34.3144   \n",
       "\n",
       "      blood-pressure  body-water   pulse  weight  body-temperature-label  \n",
       "0             0.0000      0.0000  0.0000  0.0000                     0.0  \n",
       "1             1.0000      0.0000  1.0000  0.0000                     0.0  \n",
       "2             0.0000      0.0000  0.0000  0.0000                     0.0  \n",
       "3             1.0000      0.0000  0.0000  0.0000                     0.0  \n",
       "4             0.0000      0.0000  0.0000  0.0000                     0.0  \n",
       "...              ...         ...     ...     ...                     ...  \n",
       "5376          0.2222      0.0000  0.1667  0.0000                     0.0  \n",
       "5377          0.2222      0.0000  0.1667  0.0000                     0.0  \n",
       "5378          0.3125      0.0625  0.3750  0.0625                     0.0  \n",
       "5379          0.3125      0.0625  0.3750  0.0625                     0.0  \n",
       "5380          0.3333      0.0000  0.3333  0.0000                     0.0  \n",
       "\n",
       "[5381 rows x 61 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dataset\n",
    "\n",
    "dataset = pd.read_csv(os.path.join(root, 'dataset_12h_imputed_CNT_STA_CTX_PHY.csv'))\n",
    "# dataset = pd.read_csv(os.path.join(root, 'dataset_12h_imputed_CNT.csv'))\n",
    "# dataset.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "# y = np.load('dataset_06h_y.npy')\n",
    "\n",
    "dataset = dataset[dataset.columns]\n",
    "x = np.array(dataset)\n",
    "\n",
    "\n",
    "print(dataset.columns.values)\n",
    "print(y.shape)\n",
    "print(x.shape)\n",
    "print(np.unique(y))\n",
    "display(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 8632 2158\n",
      "2 8632 2158\n",
      "3 8632 2158\n",
      "4 8632 2158\n",
      "5 8632 2158\n",
      "AUC-ROC:    0.9544\n",
      "AUC-PR:     0.4889\n",
      "Accuracy:   0.9886\n",
      "Precision:  0.8750\n",
      "Recall:     0.1037\n",
      "F1 Score:   0.1854\n",
      "Sensitivity (Recall): 0.1037\n",
      "Specificity:          0.9998\n",
      "[[10653     2]\n",
      " [  121    14]]\n"
     ]
    }
   ],
   "source": [
    "# LightGBM\n",
    "\n",
    "Y_TRUES = np.empty([0])\n",
    "Y_PROBS = np.empty([0])\n",
    "Y_PREDS = np.empty([0])\n",
    "\n",
    "\n",
    "# cv = LeavePGroupsOut(n_groups=1)\n",
    "# for train_idx, test_idx in cv.split(x, y, groups=p):\n",
    "#     participant = np.unique(p[test_idx])[0]\n",
    "\n",
    "\n",
    "# cv = KFold(n_splits=x.shape[0], shuffle=True, random_state=seed)\n",
    "# for fold, (train_idx, test_idx) in enumerate(cv.split(x), start=1):\n",
    "#     participant = np.unique(p[test_idx])[0]\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "for fold, (train_idx, test_idx) in enumerate(cv.split(x), start=1):\n",
    "    participant = fold\n",
    "\n",
    "    x_train, x_test = x[train_idx], x[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    print(participant, x_train.shape[0], x_test.shape[0])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    normalizer = MinMaxScaler()\n",
    "    normalizer.fit(x_train)\n",
    "    x_train = normalizer.transform(x_train)\n",
    "    x_test = normalizer.transform(x_test)\n",
    "\n",
    "    # Oversample class 1\n",
    "\n",
    "    # ros = RandomOverSampler(sampling_strategy=.1, random_state=seed)\n",
    "    # x_train, y_train = ros.fit_resample(x_train, y_train)\n",
    "\n",
    "    # 1. Check original class distribution\n",
    "    # print(\"Original class distribution:\", Counter(y_train))\n",
    "    # # 2. Define the minority class (adjust if needed)\n",
    "    # minority_class = 1  # change this if your minority class label is different\n",
    "    # current_minority_count = sum(y_train == minority_class)\n",
    "    # # 3. Define desired new total count for the minority class (10x)\n",
    "    # target_minority_count = current_minority_count * 10\n",
    "    # # 4. Setup SMOTE with custom sampling strategy\n",
    "    # smote = SMOTE(sampling_strategy={minority_class: target_minority_count}, random_state=seed)\n",
    "    # # 5. Fit and resample\n",
    "    # x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "    # # 6. Confirm new distribution\n",
    "    # print(\"Resampled class distribution:\", Counter(y_train))\n",
    "\n",
    "\n",
    "\n",
    "    # ----- LightGBM\n",
    "    train_data = lgb.Dataset(x_train, label=y_train)\n",
    "    test_data = lgb.Dataset(x_test, label=y_test, reference=train_data)\n",
    "    params = {\n",
    "        'verbose': -1,  # 👈 turn off training output\n",
    "        'objective': 'binary',\n",
    "        'metric': 'binary_logloss',  # or 'auc' if you prefer\n",
    "        'num_leaves': 64,\n",
    "        'learning_rate': 0.01,\n",
    "        'n_estimators': 100,\n",
    "        # 'is_unbalance': True  # Automatically balances positive and negative classes\n",
    "        # 'scale_pos_weight': (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    }\n",
    "    bst = lgb.train(params, train_data, valid_sets=[train_data, test_data])\n",
    "    y_probs = bst.predict(x_test, num_iteration=bst.best_iteration)\n",
    "\n",
    "\n",
    "    # ----- Transformer\n",
    "    # model = TabPFNClassifier(\n",
    "    #     device='cuda',\n",
    "    #     random_state=seed,\n",
    "    #     n_estimators = 4,\n",
    "    #     ignore_pretraining_limits=True)\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    Y_TRUES = np.append(Y_TRUES, y_test)\n",
    "    Y_PROBS = np.append(Y_PROBS, y_probs)\n",
    "    Y_PREDS = np.append(Y_PREDS, (y_probs >= .5).astype(int))\n",
    "\n",
    "\n",
    "indx = Y_TRUES.argsort()\n",
    "Y_TRUES = Y_TRUES[indx]\n",
    "Y_PROBS = Y_PROBS[indx]\n",
    "Y_PREDS = Y_PREDS[indx]\n",
    "\n",
    "\n",
    "# Evaluation metrics\n",
    "auc_roc = roc_auc_score(Y_TRUES, Y_PROBS)\n",
    "precision, recall, _ = precision_recall_curve(Y_TRUES, Y_PROBS)\n",
    "auc_pr = auc(recall, precision)\n",
    "acc = accuracy_score(Y_TRUES, Y_PREDS)\n",
    "pre = precision_score(Y_TRUES, Y_PREDS)\n",
    "rec = recall_score(Y_TRUES, Y_PREDS)\n",
    "f1 = f1_score(Y_TRUES, Y_PREDS)\n",
    "\n",
    "# Confusion matrix: [ [TN, FP], [FN, TP] ]\n",
    "tn, fp, fn, tp = confusion_matrix(Y_TRUES, Y_PREDS).ravel()\n",
    "\n",
    "# Sensitivity = Recall = TP / (TP + FN)\n",
    "sensitivity = tp / (tp + fn)\n",
    "\n",
    "# Specificity = TN / (TN + FP)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# Print results\n",
    "print(f\"AUC-ROC:    {auc_roc:.4f}\")\n",
    "print(f\"AUC-PR:     {auc_pr:.4f}\")\n",
    "print(f\"Accuracy:   {acc:.4f}\")\n",
    "print(f\"Precision:  {pre:.4f}\")\n",
    "print(f\"Recall:     {rec:.4f}\")\n",
    "print(f\"F1 Score:   {f1:.4f}\")\n",
    "print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "print(f\"Specificity:          {specificity:.4f}\")\n",
    "print(confusion_matrix(Y_TRUES, Y_PREDS))\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(Y_TRUES, 'o', color='blue', alpha=.25, markersize=8, label='Ground-truth')\n",
    "# plt.plot(Y_PREDS, 'o', color='red', alpha=.25, markersize=8, label='Prediction')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10779 11\n",
      "1 10617 173\n",
      "2 10488 302\n",
      "3 10599 191\n",
      "4 10604 186\n",
      "5 10767 23\n",
      "6 10437 353\n",
      "7 10541 249\n",
      "8 10500 290\n",
      "9 10611 179\n",
      "10 10771 19\n",
      "11 10560 230\n",
      "12 10532 258\n",
      "13 10716 74\n",
      "14 10524 266\n",
      "15 10548 242\n",
      "16 10665 125\n",
      "17 10508 282\n",
      "18 10668 122\n",
      "19 10570 220\n",
      "20 10588 202\n",
      "21 10521 269\n",
      "22 10627 163\n",
      "23 10600 190\n",
      "24 10771 19\n",
      "25 10520 270\n",
      "26 10495 295\n",
      "27 10520 270\n",
      "28 10605 185\n",
      "29 10616 174\n",
      "30 10576 214\n",
      "31 10556 234\n",
      "32 10687 103\n",
      "33 10560 230\n",
      "34 10625 165\n",
      "35 10751 39\n",
      "36 10525 265\n",
      "37 10436 354\n",
      "38 10621 169\n",
      "39 10463 327\n",
      "40 10524 266\n",
      "41 10584 206\n",
      "42 10632 158\n",
      "43 10520 270\n",
      "44 10728 62\n",
      "45 10699 91\n",
      "46 10763 27\n",
      "47 10580 210\n",
      "48 10672 118\n",
      "49 10528 262\n",
      "50 10627 163\n",
      "51 10541 249\n",
      "52 10502 288\n",
      "53 10575 215\n",
      "54 10532 258\n",
      "55 10775 15\n",
      "AUC-ROC:    0.9730\n",
      "AUC-PR:     0.3946\n",
      "Accuracy:   0.9873\n",
      "Precision:  0.4750\n",
      "Recall:     0.1407\n",
      "F1 Score:   0.2171\n",
      "Sensitivity (Recall): 0.1407\n",
      "Specificity:          0.9980\n",
      "[[10634    21]\n",
      " [  116    19]]\n"
     ]
    }
   ],
   "source": [
    "# Transformer\n",
    "\n",
    "Y_TRUES = np.empty([0])\n",
    "Y_PROBS = np.empty([0])\n",
    "Y_PREDS = np.empty([0])\n",
    "\n",
    "\n",
    "cv = LeavePGroupsOut(n_groups=1)\n",
    "for train_idx, test_idx in cv.split(x, y, groups=p):\n",
    "    participant = np.unique(p[test_idx])[0]\n",
    "\n",
    "\n",
    "# cv = KFold(n_splits=x.shape[0], shuffle=True, random_state=seed)\n",
    "# for fold, (train_idx, test_idx) in enumerate(cv.split(x), start=1):\n",
    "#     participant = np.unique(p[test_idx])[0]\n",
    "\n",
    "# cv = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "# for fold, (train_idx, test_idx) in enumerate(cv.split(x), start=1):\n",
    "#     participant = fold\n",
    "\n",
    "    x_train, x_test = x[train_idx], x[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    print(participant, x_train.shape[0], x_test.shape[0])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    normalizer = MinMaxScaler()\n",
    "    normalizer.fit(x_train)\n",
    "    x_train = normalizer.transform(x_train)\n",
    "    x_test = normalizer.transform(x_test)\n",
    "\n",
    "    # Oversample class 1\n",
    "\n",
    "    # ros = RandomOverSampler(sampling_strategy=.1, random_state=seed)\n",
    "    # x_train, y_train = ros.fit_resample(x_train, y_train)\n",
    "\n",
    "    # # 1. Check original class distribution\n",
    "    # print(\"Original class distribution:\", Counter(y_train))\n",
    "    # # 2. Define the minority class (adjust if needed)\n",
    "    # minority_class = 1  # change this if your minority class label is different\n",
    "    # current_minority_count = sum(y_train == minority_class)\n",
    "    # # 3. Define desired new total count for the minority class (10x)\n",
    "    # target_minority_count = current_minority_count * 10\n",
    "    # # 4. Setup SMOTE with custom sampling strategy\n",
    "    # smote = SMOTE(sampling_strategy={minority_class: target_minority_count}, random_state=seed)\n",
    "    # # 5. Fit and resample\n",
    "    # x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "    # # 6. Confirm new distribution\n",
    "    # print(\"Resampled class distribution:\", Counter(y_train))\n",
    "\n",
    "\n",
    "\n",
    "    # ----- LightGBM\n",
    "    # train_data = lgb.Dataset(x_train, label=y_train)\n",
    "    # test_data = lgb.Dataset(x_test, label=y_test, reference=train_data)\n",
    "    # params = {\n",
    "    #     'verbose': -1,  # 👈 turn off training output\n",
    "    #     'objective': 'binary',\n",
    "    #     'metric': 'binary_logloss',  # or 'auc' if you prefer\n",
    "    #     'num_leaves': 64,\n",
    "    #     'learning_rate': 0.01,\n",
    "    #     'n_estimators': 100,\n",
    "    #     # 'is_unbalance': True  # Automatically balances positive and negative classes\n",
    "    #     # 'scale_pos_weight': (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    # }\n",
    "    # bst = lgb.train(params, train_data, valid_sets=[train_data, test_data])\n",
    "    # y_probs = bst.predict(x_test, num_iteration=bst.best_iteration)\n",
    "\n",
    "\n",
    "    # ----- Transformer\n",
    "    model = TabPFNClassifier(\n",
    "        device='cuda',\n",
    "        random_state=seed,\n",
    "        n_estimators = 4,\n",
    "        ignore_pretraining_limits=True)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    Y_TRUES = np.append(Y_TRUES, y_test)\n",
    "    Y_PROBS = np.append(Y_PROBS, y_probs)\n",
    "    Y_PREDS = np.append(Y_PREDS, (y_probs >= .5).astype(int))\n",
    "\n",
    "\n",
    "indx = Y_TRUES.argsort()\n",
    "Y_TRUES = Y_TRUES[indx]\n",
    "Y_PROBS = Y_PROBS[indx]\n",
    "Y_PREDS = Y_PREDS[indx]\n",
    "\n",
    "\n",
    "# Evaluation metrics\n",
    "auc_roc = roc_auc_score(Y_TRUES, Y_PROBS)\n",
    "precision, recall, _ = precision_recall_curve(Y_TRUES, Y_PROBS)\n",
    "auc_pr = auc(recall, precision)\n",
    "acc = accuracy_score(Y_TRUES, Y_PREDS)\n",
    "pre = precision_score(Y_TRUES, Y_PREDS)\n",
    "rec = recall_score(Y_TRUES, Y_PREDS)\n",
    "f1 = f1_score(Y_TRUES, Y_PREDS)\n",
    "\n",
    "# Confusion matrix: [ [TN, FP], [FN, TP] ]\n",
    "tn, fp, fn, tp = confusion_matrix(Y_TRUES, Y_PREDS).ravel()\n",
    "\n",
    "# Sensitivity = Recall = TP / (TP + FN)\n",
    "sensitivity = tp / (tp + fn)\n",
    "\n",
    "# Specificity = TN / (TN + FP)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# Print results\n",
    "print(f\"AUC-ROC:    {auc_roc:.4f}\")\n",
    "print(f\"AUC-PR:     {auc_pr:.4f}\")\n",
    "print(f\"Accuracy:   {acc:.4f}\")\n",
    "print(f\"Precision:  {pre:.4f}\")\n",
    "print(f\"Recall:     {rec:.4f}\")\n",
    "print(f\"F1 Score:   {f1:.4f}\")\n",
    "print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "print(f\"Specificity:          {specificity:.4f}\")\n",
    "print(confusion_matrix(Y_TRUES, Y_PREDS))\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(Y_TRUES, 'o', color='blue', alpha=.25, markersize=8, label='Ground-truth')\n",
    "# plt.plot(Y_PREDS, 'o', color='red', alpha=.25, markersize=8, label='Prediction')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4304 1077\n",
      "2 4305 1076\n",
      "3 4305 1076\n",
      "4 4305 1076\n",
      "5 4305 1076\n",
      "AUC-ROC:    0.8900\n",
      "AUC-PR:     0.2280\n",
      "Accuracy:   0.9205\n",
      "Precision:  0.1953\n",
      "Recall:     0.8319\n",
      "F1 Score:   0.3163\n",
      "Sensitivity (Recall): 0.8319\n",
      "Specificity:          0.9225\n",
      "[[4854  408]\n",
      " [  20   99]]\n"
     ]
    }
   ],
   "source": [
    "# Gradient Boosting Classifier\n",
    "\n",
    "Y_TRUES = np.empty([0])\n",
    "Y_PROBS = np.empty([0])\n",
    "Y_PREDS = np.empty([0])\n",
    "\n",
    "\n",
    "# cv = LeavePGroupsOut(n_groups=1)\n",
    "# for train_idx, test_idx in cv.split(x, y, groups=p):\n",
    "#     participant = np.unique(p[test_idx])[0]\n",
    "\n",
    "\n",
    "# cv = KFold(n_splits=x.shape[0], shuffle=True, random_state=seed)\n",
    "# for fold, (train_idx, test_idx) in enumerate(cv.split(x), start=1):\n",
    "#     participant = np.unique(p[test_idx])[0]\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "for fold, (train_idx, test_idx) in enumerate(cv.split(x), start=1):\n",
    "    participant = fold\n",
    "\n",
    "    x_train, x_test = x[train_idx], x[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    print(participant, x_train.shape[0], x_test.shape[0])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    normalizer = MinMaxScaler()\n",
    "    normalizer.fit(x_train)\n",
    "    x_train = normalizer.transform(x_train)\n",
    "    x_test = normalizer.transform(x_test)\n",
    "\n",
    "    # Oversample class 1\n",
    "\n",
    "    # ros = RandomOverSampler(sampling_strategy=.1, random_state=seed)\n",
    "    # x_train, y_train = ros.fit_resample(x_train, y_train)\n",
    "\n",
    "    # # 1. Check original class distribution\n",
    "    # print(\"Original class distribution:\", Counter(y_train))\n",
    "    # # 2. Define the minority class (adjust if needed)\n",
    "    # minority_class = 1  # change this if your minority class label is different\n",
    "    # current_minority_count = sum(y_train == minority_class)\n",
    "    # # 3. Define desired new total count for the minority class (10x)\n",
    "    # target_minority_count = current_minority_count * 10\n",
    "    # # 4. Setup SMOTE with custom sampling strategy\n",
    "    # smote = SMOTE(sampling_strategy={minority_class: target_minority_count}, random_state=seed)\n",
    "    # # 5. Fit and resample\n",
    "    # x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "    # # 6. Confirm new distribution\n",
    "    # print(\"Resampled class distribution:\", Counter(y_train))\n",
    "\n",
    "\n",
    "    # ----- LightGBM\n",
    "    # train_data = lgb.Dataset(x_train, label=y_train)\n",
    "    # test_data = lgb.Dataset(x_test, label=y_test, reference=train_data)\n",
    "    # params = {\n",
    "    #     'verbose': -1,  # 👈 turn off training output\n",
    "    #     'objective': 'binary',\n",
    "    #     'metric': 'binary_logloss',  # or 'auc' if you prefer\n",
    "    #     'num_leaves': 64,\n",
    "    #     'learning_rate': 0.01,\n",
    "    #     'n_estimators': 100,\n",
    "    #     # 'is_unbalance': True  # Automatically balances positive and negative classes\n",
    "    #     # 'scale_pos_weight': (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    # }\n",
    "    # bst = lgb.train(params, train_data, valid_sets=[train_data, test_data])\n",
    "    # y_probs = bst.predict(x_test, num_iteration=bst.best_iteration)\n",
    "\n",
    "\n",
    "    # ----- Transformer\n",
    "    # model = TabPFNClassifier(\n",
    "    #     device='cuda',\n",
    "    #     random_state=seed,\n",
    "    #     n_estimators = 4,\n",
    "    #     ignore_pretraining_limits=True)\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- Gradient Boosting Classifier\n",
    "    model = GradientBoostingClassifier(\n",
    "        n_estimators=100,  # match LightGBM\n",
    "        learning_rate=0.001,  # match LightGBM\n",
    "        max_depth=4,  # similar to LightGBM default tree depth\n",
    "        subsample=1.0,  # default\n",
    "        random_state=seed\n",
    "    )\n",
    "    # model.fit(x_train, y_train)\n",
    "    model.fit(x_train, y_train, sample_weight=compute_sample_weight(class_weight='balanced', y=y_train))\n",
    "    y_probs = model.predict_proba(x_test)[:, 1]\n",
    "    \n",
    "\n",
    "    Y_TRUES = np.append(Y_TRUES, y_test)\n",
    "    Y_PROBS = np.append(Y_PROBS, y_probs)\n",
    "    Y_PREDS = np.append(Y_PREDS, (y_probs >= .5).astype(int))\n",
    "\n",
    "\n",
    "indx = Y_TRUES.argsort()\n",
    "Y_TRUES = Y_TRUES[indx]\n",
    "Y_PROBS = Y_PROBS[indx]\n",
    "Y_PREDS = Y_PREDS[indx]\n",
    "\n",
    "\n",
    "# Evaluation metrics\n",
    "auc_roc = roc_auc_score(Y_TRUES, Y_PROBS)\n",
    "precision, recall, _ = precision_recall_curve(Y_TRUES, Y_PROBS)\n",
    "auc_pr = auc(recall, precision)\n",
    "acc = accuracy_score(Y_TRUES, Y_PREDS)\n",
    "pre = precision_score(Y_TRUES, Y_PREDS)\n",
    "rec = recall_score(Y_TRUES, Y_PREDS)\n",
    "f1 = f1_score(Y_TRUES, Y_PREDS)\n",
    "\n",
    "# Confusion matrix: [ [TN, FP], [FN, TP] ]\n",
    "tn, fp, fn, tp = confusion_matrix(Y_TRUES, Y_PREDS).ravel()\n",
    "\n",
    "# Sensitivity = Recall = TP / (TP + FN)\n",
    "sensitivity = tp / (tp + fn)\n",
    "\n",
    "# Specificity = TN / (TN + FP)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# Print results\n",
    "print(f\"AUC-ROC:    {auc_roc:.4f}\")\n",
    "print(f\"AUC-PR:     {auc_pr:.4f}\")\n",
    "print(f\"Accuracy:   {acc:.4f}\")\n",
    "print(f\"Precision:  {pre:.4f}\")\n",
    "print(f\"Recall:     {rec:.4f}\")\n",
    "print(f\"F1 Score:   {f1:.4f}\")\n",
    "print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "print(f\"Specificity:          {specificity:.4f}\")\n",
    "print(confusion_matrix(Y_TRUES, Y_PREDS))\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(Y_TRUES, 'o', color='blue', alpha=.25, markersize=8, label='Ground-truth')\n",
    "# plt.plot(Y_PREDS, 'o', color='red', alpha=.25, markersize=8, label='Prediction')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10779 11\n",
      "1 10617 173\n",
      "2 10488 302\n",
      "3 10599 191\n",
      "4 10604 186\n",
      "5 10767 23\n",
      "6 10437 353\n",
      "7 10541 249\n",
      "8 10500 290\n",
      "9 10611 179\n",
      "10 10771 19\n",
      "11 10560 230\n",
      "12 10532 258\n",
      "13 10716 74\n",
      "14 10524 266\n",
      "15 10548 242\n",
      "16 10665 125\n",
      "17 10508 282\n",
      "18 10668 122\n",
      "19 10570 220\n",
      "20 10588 202\n",
      "21 10521 269\n",
      "22 10627 163\n",
      "23 10600 190\n",
      "24 10771 19\n",
      "25 10520 270\n",
      "26 10495 295\n",
      "27 10520 270\n",
      "28 10605 185\n",
      "29 10616 174\n",
      "30 10576 214\n",
      "31 10556 234\n",
      "32 10687 103\n",
      "33 10560 230\n",
      "34 10625 165\n",
      "35 10751 39\n",
      "36 10525 265\n",
      "37 10436 354\n",
      "38 10621 169\n",
      "39 10463 327\n",
      "40 10524 266\n",
      "41 10584 206\n",
      "42 10632 158\n",
      "43 10520 270\n",
      "44 10728 62\n",
      "45 10699 91\n",
      "46 10763 27\n",
      "47 10580 210\n",
      "48 10672 118\n",
      "49 10528 262\n",
      "50 10627 163\n",
      "51 10541 249\n",
      "52 10502 288\n",
      "53 10575 215\n",
      "54 10532 258\n",
      "55 10775 15\n",
      "AUC-ROC:    0.9632\n",
      "AUC-PR:     0.4177\n",
      "Accuracy:   0.9869\n",
      "Precision:  0.4786\n",
      "Recall:     0.4963\n",
      "F1 Score:   0.4873\n",
      "Sensitivity (Recall): 0.4963\n",
      "Specificity:          0.9931\n",
      "[[10582    73]\n",
      " [   68    67]]\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "\n",
    "Y_TRUES = np.empty([0])\n",
    "Y_PROBS = np.empty([0])\n",
    "Y_PREDS = np.empty([0])\n",
    "\n",
    "\n",
    "cv = LeavePGroupsOut(n_groups=1)\n",
    "for train_idx, test_idx in cv.split(x, y, groups=p):\n",
    "    participant = np.unique(p[test_idx])[0]\n",
    "\n",
    "\n",
    "# cv = KFold(n_splits=x.shape[0], shuffle=True, random_state=seed)\n",
    "# for fold, (train_idx, test_idx) in enumerate(cv.split(x), start=1):\n",
    "#     participant = np.unique(p[test_idx])[0]\n",
    "\n",
    "# cv = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "# for fold, (train_idx, test_idx) in enumerate(cv.split(x), start=1):\n",
    "#     participant = fold\n",
    "\n",
    "    x_train, x_test = x[train_idx], x[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    print(participant, x_train.shape[0], x_test.shape[0])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    normalizer = MinMaxScaler()\n",
    "    normalizer.fit(x_train)\n",
    "    x_train = normalizer.transform(x_train)\n",
    "    x_test = normalizer.transform(x_test)\n",
    "\n",
    "    # Oversample class 1\n",
    "\n",
    "    # ros = RandomOverSampler(sampling_strategy=.1, random_state=seed)\n",
    "    # x_train, y_train = ros.fit_resample(x_train, y_train)\n",
    "\n",
    "    # # 1. Check original class distribution\n",
    "    # print(\"Original class distribution:\", Counter(y_train))\n",
    "    # # 2. Define the minority class (adjust if needed)\n",
    "    # minority_class = 1  # change this if your minority class label is different\n",
    "    # current_minority_count = sum(y_train == minority_class)\n",
    "    # # 3. Define desired new total count for the minority class (10x)\n",
    "    # target_minority_count = current_minority_count * 10\n",
    "    # # 4. Setup SMOTE with custom sampling strategy\n",
    "    # smote = SMOTE(sampling_strategy={minority_class: target_minority_count}, random_state=seed)\n",
    "    # # 5. Fit and resample\n",
    "    # x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "    # # 6. Confirm new distribution\n",
    "    # print(\"Resampled class distribution:\", Counter(y_train))\n",
    "\n",
    "\n",
    "    # ----- LightGBM\n",
    "    # train_data = lgb.Dataset(x_train, label=y_train)\n",
    "    # test_data = lgb.Dataset(x_test, label=y_test, reference=train_data)\n",
    "    # params = {\n",
    "    #     'verbose': -1,  # 👈 turn off training output\n",
    "    #     'objective': 'binary',\n",
    "    #     'metric': 'binary_logloss',  # or 'auc' if you prefer\n",
    "    #     'num_leaves': 64,\n",
    "    #     'learning_rate': 0.01,\n",
    "    #     'n_estimators': 100,\n",
    "    #     # 'is_unbalance': True  # Automatically balances positive and negative classes\n",
    "    #     # 'scale_pos_weight': (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    # }\n",
    "    # bst = lgb.train(params, train_data, valid_sets=[train_data, test_data])\n",
    "    # y_probs = bst.predict(x_test, num_iteration=bst.best_iteration)\n",
    "\n",
    "\n",
    "    # ----- Transformer\n",
    "    # model = TabPFNClassifier(\n",
    "    #     device='cuda',\n",
    "    #     random_state=seed,\n",
    "    #     n_estimators = 4,\n",
    "    #     ignore_pretraining_limits=True)\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- Gradient Boosting Classifier\n",
    "    # model = GradientBoostingClassifier(\n",
    "    #     n_estimators=100,  # match LightGBM\n",
    "    #     learning_rate=0.001,  # match LightGBM\n",
    "    #     max_depth=4,  # similar to LightGBM default tree depth\n",
    "    #     subsample=1.0,  # default\n",
    "    #     random_state=seed\n",
    "    # )\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "    \n",
    "\n",
    "    # ----- XGBoost\n",
    "    model = XGBClassifier()\n",
    "    # model.fit(x_train, y_train)\n",
    "    model.fit(x_train, y_train, sample_weight=compute_sample_weight(class_weight='balanced', y=y_train))\n",
    "    y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    Y_TRUES = np.append(Y_TRUES, y_test)\n",
    "    Y_PROBS = np.append(Y_PROBS, y_probs)\n",
    "    Y_PREDS = np.append(Y_PREDS, (y_probs >= .5).astype(int))\n",
    "\n",
    "\n",
    "indx = Y_TRUES.argsort()\n",
    "Y_TRUES = Y_TRUES[indx]\n",
    "Y_PROBS = Y_PROBS[indx]\n",
    "Y_PREDS = Y_PREDS[indx]\n",
    "\n",
    "\n",
    "# Evaluation metrics\n",
    "auc_roc = roc_auc_score(Y_TRUES, Y_PROBS)\n",
    "precision, recall, _ = precision_recall_curve(Y_TRUES, Y_PROBS)\n",
    "auc_pr = auc(recall, precision)\n",
    "acc = accuracy_score(Y_TRUES, Y_PREDS)\n",
    "pre = precision_score(Y_TRUES, Y_PREDS)\n",
    "rec = recall_score(Y_TRUES, Y_PREDS)\n",
    "f1 = f1_score(Y_TRUES, Y_PREDS)\n",
    "\n",
    "# Confusion matrix: [ [TN, FP], [FN, TP] ]\n",
    "tn, fp, fn, tp = confusion_matrix(Y_TRUES, Y_PREDS).ravel()\n",
    "\n",
    "# Sensitivity = Recall = TP / (TP + FN)\n",
    "sensitivity = tp / (tp + fn)\n",
    "\n",
    "# Specificity = TN / (TN + FP)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# Print results\n",
    "print(f\"AUC-ROC:    {auc_roc:.4f}\")\n",
    "print(f\"AUC-PR:     {auc_pr:.4f}\")\n",
    "print(f\"Accuracy:   {acc:.4f}\")\n",
    "print(f\"Precision:  {pre:.4f}\")\n",
    "print(f\"Recall:     {rec:.4f}\")\n",
    "print(f\"F1 Score:   {f1:.4f}\")\n",
    "print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "print(f\"Specificity:          {specificity:.4f}\")\n",
    "print(confusion_matrix(Y_TRUES, Y_PREDS))\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(Y_TRUES, 'o', color='blue', alpha=.25, markersize=8, label='Ground-truth')\n",
    "# plt.plot(Y_PREDS, 'o', color='red', alpha=.25, markersize=8, label='Prediction')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10779 11\n",
      "1 10617 173\n",
      "2 10488 302\n",
      "3 10599 191\n",
      "4 10604 186\n",
      "5 10767 23\n",
      "6 10437 353\n",
      "7 10541 249\n",
      "8 10500 290\n",
      "9 10611 179\n",
      "10 10771 19\n",
      "11 10560 230\n",
      "12 10532 258\n",
      "13 10716 74\n",
      "14 10524 266\n",
      "15 10548 242\n",
      "16 10665 125\n",
      "17 10508 282\n",
      "18 10668 122\n",
      "19 10570 220\n",
      "20 10588 202\n",
      "21 10521 269\n",
      "22 10627 163\n",
      "23 10600 190\n",
      "24 10771 19\n",
      "25 10520 270\n",
      "26 10495 295\n",
      "27 10520 270\n",
      "28 10605 185\n",
      "29 10616 174\n",
      "30 10576 214\n",
      "31 10556 234\n",
      "32 10687 103\n",
      "33 10560 230\n",
      "34 10625 165\n",
      "35 10751 39\n",
      "36 10525 265\n",
      "37 10436 354\n",
      "38 10621 169\n",
      "39 10463 327\n",
      "40 10524 266\n",
      "41 10584 206\n",
      "42 10632 158\n",
      "43 10520 270\n",
      "44 10728 62\n",
      "45 10699 91\n",
      "46 10763 27\n",
      "47 10580 210\n",
      "48 10672 118\n",
      "49 10528 262\n",
      "50 10627 163\n",
      "51 10541 249\n",
      "52 10502 288\n",
      "53 10575 215\n",
      "54 10532 258\n",
      "55 10775 15\n",
      "AUC-ROC:    0.9270\n",
      "AUC-PR:     0.3763\n",
      "Accuracy:   0.9878\n",
      "Precision:  0.5652\n",
      "Recall:     0.0963\n",
      "F1 Score:   0.1646\n",
      "Sensitivity (Recall): 0.0963\n",
      "Specificity:          0.9991\n",
      "[[10645    10]\n",
      " [  122    13]]\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "\n",
    "Y_TRUES = np.empty([0])\n",
    "Y_PROBS = np.empty([0])\n",
    "Y_PREDS = np.empty([0])\n",
    "\n",
    "\n",
    "cv = LeavePGroupsOut(n_groups=1)\n",
    "for train_idx, test_idx in cv.split(x, y, groups=p):\n",
    "    participant = np.unique(p[test_idx])[0]\n",
    "\n",
    "\n",
    "# cv = KFold(n_splits=x.shape[0], shuffle=True, random_state=seed)\n",
    "# for fold, (train_idx, test_idx) in enumerate(cv.split(x), start=1):\n",
    "#     participant = np.unique(p[test_idx])[0]\n",
    "\n",
    "# cv = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "# for fold, (train_idx, test_idx) in enumerate(cv.split(x), start=1):\n",
    "#     participant = fold\n",
    "\n",
    "    x_train, x_test = x[train_idx], x[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    print(participant, x_train.shape[0], x_test.shape[0])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    normalizer = MinMaxScaler()\n",
    "    normalizer.fit(x_train)\n",
    "    x_train = normalizer.transform(x_train)\n",
    "    x_test = normalizer.transform(x_test)\n",
    "\n",
    "    # Oversample class 1\n",
    "\n",
    "    # ros = RandomOverSampler(sampling_strategy=.1, random_state=seed)\n",
    "    # x_train, y_train = ros.fit_resample(x_train, y_train)\n",
    "\n",
    "    # # 1. Check original class distribution\n",
    "    # print(\"Original class distribution:\", Counter(y_train))\n",
    "    # # 2. Define the minority class (adjust if needed)\n",
    "    # minority_class = 1  # change this if your minority class label is different\n",
    "    # current_minority_count = sum(y_train == minority_class)\n",
    "    # # 3. Define desired new total count for the minority class (10x)\n",
    "    # target_minority_count = current_minority_count * 10\n",
    "    # # 4. Setup SMOTE with custom sampling strategy\n",
    "    # smote = SMOTE(sampling_strategy={minority_class: target_minority_count}, random_state=seed)\n",
    "    # # 5. Fit and resample\n",
    "    # x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "    # # 6. Confirm new distribution\n",
    "    # print(\"Resampled class distribution:\", Counter(y_train))\n",
    "\n",
    "\n",
    "    # ----- LightGBM\n",
    "    # train_data = lgb.Dataset(x_train, label=y_train)\n",
    "    # test_data = lgb.Dataset(x_test, label=y_test, reference=train_data)\n",
    "    # params = {\n",
    "    #     'verbose': -1,  # 👈 turn off training output\n",
    "    #     'objective': 'binary',\n",
    "    #     'metric': 'binary_logloss',  # or 'auc' if you prefer\n",
    "    #     'num_leaves': 64,\n",
    "    #     'learning_rate': 0.01,\n",
    "    #     'n_estimators': 100,\n",
    "    #     # 'is_unbalance': True  # Automatically balances positive and negative classes\n",
    "    #     # 'scale_pos_weight': (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    # }\n",
    "    # bst = lgb.train(params, train_data, valid_sets=[train_data, test_data])\n",
    "    # y_probs = bst.predict(x_test, num_iteration=bst.best_iteration)\n",
    "\n",
    "\n",
    "    # ----- Transformer\n",
    "    # model = TabPFNClassifier(\n",
    "    #     device='cuda',\n",
    "    #     random_state=seed,\n",
    "    #     n_estimators = 4,\n",
    "    #     ignore_pretraining_limits=True)\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- Gradient Boosting Classifier\n",
    "    # model = GradientBoostingClassifier(\n",
    "    #     n_estimators=100,  # match LightGBM\n",
    "    #     learning_rate=0.001,  # match LightGBM\n",
    "    #     max_depth=4,  # similar to LightGBM default tree depth\n",
    "    #     subsample=1.0,  # default\n",
    "    #     random_state=seed\n",
    "    # )\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "    \n",
    "\n",
    "    # ----- XGBoost\n",
    "    # model = XGBClassifier()\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- Random Forest\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=250,  # number of trees\n",
    "        max_depth=None,  # let the trees grow fully\n",
    "        random_state=seed\n",
    "    )\n",
    "    model.fit(x_train, y_train)\n",
    "    y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    Y_TRUES = np.append(Y_TRUES, y_test)\n",
    "    Y_PROBS = np.append(Y_PROBS, y_probs)\n",
    "    Y_PREDS = np.append(Y_PREDS, (y_probs >= .5).astype(int))\n",
    "\n",
    "\n",
    "indx = Y_TRUES.argsort()\n",
    "Y_TRUES = Y_TRUES[indx]\n",
    "Y_PROBS = Y_PROBS[indx]\n",
    "Y_PREDS = Y_PREDS[indx]\n",
    "\n",
    "\n",
    "# Evaluation metrics\n",
    "auc_roc = roc_auc_score(Y_TRUES, Y_PROBS)\n",
    "precision, recall, _ = precision_recall_curve(Y_TRUES, Y_PROBS)\n",
    "auc_pr = auc(recall, precision)\n",
    "acc = accuracy_score(Y_TRUES, Y_PREDS)\n",
    "pre = precision_score(Y_TRUES, Y_PREDS)\n",
    "rec = recall_score(Y_TRUES, Y_PREDS)\n",
    "f1 = f1_score(Y_TRUES, Y_PREDS)\n",
    "\n",
    "# Confusion matrix: [ [TN, FP], [FN, TP] ]\n",
    "tn, fp, fn, tp = confusion_matrix(Y_TRUES, Y_PREDS).ravel()\n",
    "\n",
    "# Sensitivity = Recall = TP / (TP + FN)\n",
    "sensitivity = tp / (tp + fn)\n",
    "\n",
    "# Specificity = TN / (TN + FP)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# Print results\n",
    "print(f\"AUC-ROC:    {auc_roc:.4f}\")\n",
    "print(f\"AUC-PR:     {auc_pr:.4f}\")\n",
    "print(f\"Accuracy:   {acc:.4f}\")\n",
    "print(f\"Precision:  {pre:.4f}\")\n",
    "print(f\"Recall:     {rec:.4f}\")\n",
    "print(f\"F1 Score:   {f1:.4f}\")\n",
    "print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "print(f\"Specificity:          {specificity:.4f}\")\n",
    "print(confusion_matrix(Y_TRUES, Y_PREDS))\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(Y_TRUES, 'o', color='blue', alpha=.25, markersize=8, label='Ground-truth')\n",
    "# plt.plot(Y_PREDS, 'o', color='red', alpha=.25, markersize=8, label='Prediction')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10779 11\n",
      "1 10617 173\n",
      "2 10488 302\n",
      "3 10599 191\n",
      "4 10604 186\n",
      "5 10767 23\n",
      "6 10437 353\n",
      "7 10541 249\n",
      "8 10500 290\n",
      "9 10611 179\n",
      "10 10771 19\n",
      "11 10560 230\n",
      "12 10532 258\n",
      "13 10716 74\n",
      "14 10524 266\n",
      "15 10548 242\n",
      "16 10665 125\n",
      "17 10508 282\n",
      "18 10668 122\n",
      "19 10570 220\n",
      "20 10588 202\n",
      "21 10521 269\n",
      "22 10627 163\n",
      "23 10600 190\n",
      "24 10771 19\n",
      "25 10520 270\n",
      "26 10495 295\n",
      "27 10520 270\n",
      "28 10605 185\n",
      "29 10616 174\n",
      "30 10576 214\n",
      "31 10556 234\n",
      "32 10687 103\n",
      "33 10560 230\n",
      "34 10625 165\n",
      "35 10751 39\n",
      "36 10525 265\n",
      "37 10436 354\n",
      "38 10621 169\n",
      "39 10463 327\n",
      "40 10524 266\n",
      "41 10584 206\n",
      "42 10632 158\n",
      "43 10520 270\n",
      "44 10728 62\n",
      "45 10699 91\n",
      "46 10763 27\n",
      "47 10580 210\n",
      "48 10672 118\n",
      "49 10528 262\n",
      "50 10627 163\n",
      "51 10541 249\n",
      "52 10502 288\n",
      "53 10575 215\n",
      "54 10532 258\n",
      "55 10775 15\n",
      "AUC-ROC:    0.8021\n",
      "AUC-PR:     0.1697\n",
      "Accuracy:   0.9860\n",
      "Precision:  0.2143\n",
      "Recall:     0.0444\n",
      "F1 Score:   0.0736\n",
      "Sensitivity (Recall): 0.0444\n",
      "Specificity:          0.9979\n",
      "[[10633    22]\n",
      " [  129     6]]\n"
     ]
    }
   ],
   "source": [
    "# SVM\n",
    "\n",
    "Y_TRUES = np.empty([0])\n",
    "Y_PROBS = np.empty([0])\n",
    "Y_PREDS = np.empty([0])\n",
    "\n",
    "\n",
    "cv = LeavePGroupsOut(n_groups=1)\n",
    "for train_idx, test_idx in cv.split(x, y, groups=p):\n",
    "    participant = np.unique(p[test_idx])[0]\n",
    "\n",
    "\n",
    "# cv = KFold(n_splits=x.shape[0], shuffle=True, random_state=seed)\n",
    "# for fold, (train_idx, test_idx) in enumerate(cv.split(x), start=1):\n",
    "#     participant = np.unique(p[test_idx])[0]\n",
    "\n",
    "# cv = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "# for fold, (train_idx, test_idx) in enumerate(cv.split(x), start=1):\n",
    "#     participant = fold\n",
    "\n",
    "    x_train, x_test = x[train_idx], x[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    print(participant, x_train.shape[0], x_test.shape[0])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    normalizer = MinMaxScaler()\n",
    "    normalizer.fit(x_train)\n",
    "    x_train = normalizer.transform(x_train)\n",
    "    x_test = normalizer.transform(x_test)\n",
    "\n",
    "    # Oversample class 1\n",
    "\n",
    "    # ros = RandomOverSampler(sampling_strategy=.1, random_state=seed)\n",
    "    # x_train, y_train = ros.fit_resample(x_train, y_train)\n",
    "\n",
    "    # # 1. Check original class distribution\n",
    "    # print(\"Original class distribution:\", Counter(y_train))\n",
    "    # # 2. Define the minority class (adjust if needed)\n",
    "    # minority_class = 1  # change this if your minority class label is different\n",
    "    # current_minority_count = sum(y_train == minority_class)\n",
    "    # # 3. Define desired new total count for the minority class (10x)\n",
    "    # target_minority_count = current_minority_count * 10\n",
    "    # # 4. Setup SMOTE with custom sampling strategy\n",
    "    # smote = SMOTE(sampling_strategy={minority_class: target_minority_count}, random_state=seed)\n",
    "    # # 5. Fit and resample\n",
    "    # x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "    # # 6. Confirm new distribution\n",
    "    # print(\"Resampled class distribution:\", Counter(y_train))\n",
    "\n",
    "\n",
    "    # ----- LightGBM\n",
    "    # train_data = lgb.Dataset(x_train, label=y_train)\n",
    "    # test_data = lgb.Dataset(x_test, label=y_test, reference=train_data)\n",
    "    # params = {\n",
    "    #     'verbose': -1,  # 👈 turn off training output\n",
    "    #     'objective': 'binary',\n",
    "    #     'metric': 'binary_logloss',  # or 'auc' if you prefer\n",
    "    #     'num_leaves': 64,\n",
    "    #     'learning_rate': 0.01,\n",
    "    #     'n_estimators': 100,\n",
    "    #     # 'is_unbalance': True  # Automatically balances positive and negative classes\n",
    "    #     # 'scale_pos_weight': (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    # }\n",
    "    # bst = lgb.train(params, train_data, valid_sets=[train_data, test_data])\n",
    "    # y_probs = bst.predict(x_test, num_iteration=bst.best_iteration)\n",
    "\n",
    "\n",
    "    # ----- Transformer\n",
    "    # model = TabPFNClassifier(\n",
    "    #     device='cuda',\n",
    "    #     random_state=seed,\n",
    "    #     n_estimators = 4,\n",
    "    #     ignore_pretraining_limits=True)\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- Gradient Boosting Classifier\n",
    "    # model = GradientBoostingClassifier(\n",
    "    #     n_estimators=100,  # match LightGBM\n",
    "    #     learning_rate=0.001,  # match LightGBM\n",
    "    #     max_depth=4,  # similar to LightGBM default tree depth\n",
    "    #     subsample=1.0,  # default\n",
    "    #     random_state=seed\n",
    "    # )\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "    \n",
    "\n",
    "    # ----- XGBoost\n",
    "    # model = XGBClassifier()\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- Random Forest\n",
    "    # model = RandomForestClassifier(\n",
    "    #     n_estimators=250,  # number of trees\n",
    "    #     max_depth=None,  # let the trees grow fully\n",
    "    #     random_state=seed\n",
    "    # )\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- SVM\n",
    "    model = SVC(kernel='rbf', probability=True, random_state=seed)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Y_TRUES = np.append(Y_TRUES, y_test)\n",
    "    Y_PROBS = np.append(Y_PROBS, y_probs)\n",
    "    Y_PREDS = np.append(Y_PREDS, (y_probs >= .5).astype(int))\n",
    "\n",
    "\n",
    "indx = Y_TRUES.argsort()\n",
    "Y_TRUES = Y_TRUES[indx]\n",
    "Y_PROBS = Y_PROBS[indx]\n",
    "Y_PREDS = Y_PREDS[indx]\n",
    "\n",
    "\n",
    "# Evaluation metrics\n",
    "auc_roc = roc_auc_score(Y_TRUES, Y_PROBS)\n",
    "precision, recall, _ = precision_recall_curve(Y_TRUES, Y_PROBS)\n",
    "auc_pr = auc(recall, precision)\n",
    "acc = accuracy_score(Y_TRUES, Y_PREDS)\n",
    "pre = precision_score(Y_TRUES, Y_PREDS)\n",
    "rec = recall_score(Y_TRUES, Y_PREDS)\n",
    "f1 = f1_score(Y_TRUES, Y_PREDS)\n",
    "\n",
    "# Confusion matrix: [ [TN, FP], [FN, TP] ]\n",
    "tn, fp, fn, tp = confusion_matrix(Y_TRUES, Y_PREDS).ravel()\n",
    "\n",
    "# Sensitivity = Recall = TP / (TP + FN)\n",
    "sensitivity = tp / (tp + fn)\n",
    "\n",
    "# Specificity = TN / (TN + FP)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# Print results\n",
    "print(f\"AUC-ROC:    {auc_roc:.4f}\")\n",
    "print(f\"AUC-PR:     {auc_pr:.4f}\")\n",
    "print(f\"Accuracy:   {acc:.4f}\")\n",
    "print(f\"Precision:  {pre:.4f}\")\n",
    "print(f\"Recall:     {rec:.4f}\")\n",
    "print(f\"F1 Score:   {f1:.4f}\")\n",
    "print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "print(f\"Specificity:          {specificity:.4f}\")\n",
    "print(confusion_matrix(Y_TRUES, Y_PREDS))\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(Y_TRUES, 'o', color='blue', alpha=.25, markersize=8, label='Ground-truth')\n",
    "# plt.plot(Y_PREDS, 'o', color='red', alpha=.25, markersize=8, label='Prediction')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10779 11\n",
      "1 10617 173\n",
      "2 10488 302\n",
      "3 10599 191\n",
      "4 10604 186\n",
      "5 10767 23\n",
      "6 10437 353\n",
      "7 10541 249\n",
      "8 10500 290\n",
      "9 10611 179\n",
      "10 10771 19\n",
      "11 10560 230\n",
      "12 10532 258\n",
      "13 10716 74\n",
      "14 10524 266\n",
      "15 10548 242\n",
      "16 10665 125\n",
      "17 10508 282\n",
      "18 10668 122\n",
      "19 10570 220\n",
      "20 10588 202\n",
      "21 10521 269\n",
      "22 10627 163\n",
      "23 10600 190\n",
      "24 10771 19\n",
      "25 10520 270\n",
      "26 10495 295\n",
      "27 10520 270\n",
      "28 10605 185\n",
      "29 10616 174\n",
      "30 10576 214\n",
      "31 10556 234\n",
      "32 10687 103\n",
      "33 10560 230\n",
      "34 10625 165\n",
      "35 10751 39\n",
      "36 10525 265\n",
      "37 10436 354\n",
      "38 10621 169\n",
      "39 10463 327\n",
      "40 10524 266\n",
      "41 10584 206\n",
      "42 10632 158\n",
      "43 10520 270\n",
      "44 10728 62\n",
      "45 10699 91\n",
      "46 10763 27\n",
      "47 10580 210\n",
      "48 10672 118\n",
      "49 10528 262\n",
      "50 10627 163\n",
      "51 10541 249\n",
      "52 10502 288\n",
      "53 10575 215\n",
      "54 10532 258\n",
      "55 10775 15\n",
      "AUC-ROC:    0.6371\n",
      "AUC-PR:     0.3090\n",
      "Accuracy:   0.9838\n",
      "Precision:  0.3276\n",
      "Recall:     0.2815\n",
      "F1 Score:   0.3028\n",
      "Sensitivity (Recall): 0.2815\n",
      "Specificity:          0.9927\n",
      "[[10577    78]\n",
      " [   97    38]]\n"
     ]
    }
   ],
   "source": [
    "# DT\n",
    "\n",
    "Y_TRUES = np.empty([0])\n",
    "Y_PROBS = np.empty([0])\n",
    "Y_PREDS = np.empty([0])\n",
    "\n",
    "\n",
    "cv = LeavePGroupsOut(n_groups=1)\n",
    "for train_idx, test_idx in cv.split(x, y, groups=p):\n",
    "    participant = np.unique(p[test_idx])[0]\n",
    "\n",
    "\n",
    "# cv = KFold(n_splits=x.shape[0], shuffle=True, random_state=seed)\n",
    "# for fold, (train_idx, test_idx) in enumerate(cv.split(x), start=1):\n",
    "#     participant = np.unique(p[test_idx])[0]\n",
    "\n",
    "# cv = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "# for fold, (train_idx, test_idx) in enumerate(cv.split(x), start=1):\n",
    "#     participant = fold\n",
    "\n",
    "    x_train, x_test = x[train_idx], x[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    print(participant, x_train.shape[0], x_test.shape[0])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    normalizer = MinMaxScaler()\n",
    "    normalizer.fit(x_train)\n",
    "    x_train = normalizer.transform(x_train)\n",
    "    x_test = normalizer.transform(x_test)\n",
    "\n",
    "    # Oversample class 1\n",
    "\n",
    "    # ros = RandomOverSampler(sampling_strategy=.1, random_state=seed)\n",
    "    # x_train, y_train = ros.fit_resample(x_train, y_train)\n",
    "\n",
    "    # # 1. Check original class distribution\n",
    "    # print(\"Original class distribution:\", Counter(y_train))\n",
    "    # # 2. Define the minority class (adjust if needed)\n",
    "    # minority_class = 1  # change this if your minority class label is different\n",
    "    # current_minority_count = sum(y_train == minority_class)\n",
    "    # # 3. Define desired new total count for the minority class (10x)\n",
    "    # target_minority_count = current_minority_count * 10\n",
    "    # # 4. Setup SMOTE with custom sampling strategy\n",
    "    # smote = SMOTE(sampling_strategy={minority_class: target_minority_count}, random_state=seed)\n",
    "    # # 5. Fit and resample\n",
    "    # x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "    # # 6. Confirm new distribution\n",
    "    # print(\"Resampled class distribution:\", Counter(y_train))\n",
    "\n",
    "\n",
    "    # ----- LightGBM\n",
    "    # train_data = lgb.Dataset(x_train, label=y_train)\n",
    "    # test_data = lgb.Dataset(x_test, label=y_test, reference=train_data)\n",
    "    # params = {\n",
    "    #     'verbose': -1,  # 👈 turn off training output\n",
    "    #     'objective': 'binary',\n",
    "    #     'metric': 'binary_logloss',  # or 'auc' if you prefer\n",
    "    #     'num_leaves': 64,\n",
    "    #     'learning_rate': 0.01,\n",
    "    #     'n_estimators': 100,\n",
    "    #     # 'is_unbalance': True  # Automatically balances positive and negative classes\n",
    "    #     # 'scale_pos_weight': (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    # }\n",
    "    # bst = lgb.train(params, train_data, valid_sets=[train_data, test_data])\n",
    "    # y_probs = bst.predict(x_test, num_iteration=bst.best_iteration)\n",
    "\n",
    "\n",
    "    # ----- Transformer\n",
    "    # model = TabPFNClassifier(\n",
    "    #     device='cuda',\n",
    "    #     random_state=seed,\n",
    "    #     n_estimators = 4,\n",
    "    #     ignore_pretraining_limits=True)\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- Gradient Boosting Classifier\n",
    "    # model = GradientBoostingClassifier(\n",
    "    #     n_estimators=100,  # match LightGBM\n",
    "    #     learning_rate=0.001,  # match LightGBM\n",
    "    #     max_depth=4,  # similar to LightGBM default tree depth\n",
    "    #     subsample=1.0,  # default\n",
    "    #     random_state=seed\n",
    "    # )\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "    \n",
    "\n",
    "    # ----- XGBoost\n",
    "    # model = XGBClassifier()\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- Random Forest\n",
    "    # model = RandomForestClassifier(\n",
    "    #     n_estimators=250,  # number of trees\n",
    "    #     max_depth=None,  # let the trees grow fully\n",
    "    #     random_state=seed\n",
    "    # )\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- SVM\n",
    "    # model = SVC(kernel='rbf', probability=True, random_state=seed)\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- DT\n",
    "    model = DecisionTreeClassifier(criterion=\"log_loss\", max_depth=512, random_state=seed)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_probs = model.predict(x_test)\n",
    "\n",
    "\n",
    "\n",
    "    Y_TRUES = np.append(Y_TRUES, y_test)\n",
    "    Y_PROBS = np.append(Y_PROBS, y_probs)\n",
    "    Y_PREDS = np.append(Y_PREDS, (y_probs >= .5).astype(int))\n",
    "\n",
    "\n",
    "indx = Y_TRUES.argsort()\n",
    "Y_TRUES = Y_TRUES[indx]\n",
    "Y_PROBS = Y_PROBS[indx]\n",
    "Y_PREDS = Y_PREDS[indx]\n",
    "\n",
    "\n",
    "# Evaluation metrics\n",
    "auc_roc = roc_auc_score(Y_TRUES, Y_PROBS)\n",
    "precision, recall, _ = precision_recall_curve(Y_TRUES, Y_PROBS)\n",
    "auc_pr = auc(recall, precision)\n",
    "acc = accuracy_score(Y_TRUES, Y_PREDS)\n",
    "pre = precision_score(Y_TRUES, Y_PREDS)\n",
    "rec = recall_score(Y_TRUES, Y_PREDS)\n",
    "f1 = f1_score(Y_TRUES, Y_PREDS)\n",
    "\n",
    "# Confusion matrix: [ [TN, FP], [FN, TP] ]\n",
    "tn, fp, fn, tp = confusion_matrix(Y_TRUES, Y_PREDS).ravel()\n",
    "\n",
    "# Sensitivity = Recall = TP / (TP + FN)\n",
    "sensitivity = tp / (tp + fn)\n",
    "\n",
    "# Specificity = TN / (TN + FP)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# Print results\n",
    "print(f\"AUC-ROC:    {auc_roc:.4f}\")\n",
    "print(f\"AUC-PR:     {auc_pr:.4f}\")\n",
    "print(f\"Accuracy:   {acc:.4f}\")\n",
    "print(f\"Precision:  {pre:.4f}\")\n",
    "print(f\"Recall:     {rec:.4f}\")\n",
    "print(f\"F1 Score:   {f1:.4f}\")\n",
    "print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "print(f\"Specificity:          {specificity:.4f}\")\n",
    "print(confusion_matrix(Y_TRUES, Y_PREDS))\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(Y_TRUES, 'o', color='blue', alpha=.25, markersize=8, label='Ground-truth')\n",
    "# plt.plot(Y_PREDS, 'o', color='red', alpha=.25, markersize=8, label='Prediction')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10779 11\n",
      "Iteration 1, loss = 0.22359075\n",
      "Validation score: 0.987013\n",
      "Iteration 2, loss = 0.07409667\n",
      "Validation score: 0.987013\n",
      "Iteration 3, loss = 0.05419803\n",
      "Validation score: 0.987013\n",
      "Iteration 4, loss = 0.04736082\n",
      "Validation score: 0.987013\n",
      "Iteration 5, loss = 0.04526200\n",
      "Validation score: 0.987013\n",
      "Iteration 6, loss = 0.04457983\n",
      "Validation score: 0.987013\n",
      "Iteration 7, loss = 0.04392281\n",
      "Validation score: 0.987013\n",
      "Iteration 8, loss = 0.04302360\n",
      "Validation score: 0.987013\n",
      "Iteration 9, loss = 0.04293123\n",
      "Validation score: 0.987013\n",
      "Iteration 10, loss = 0.04338872\n",
      "Validation score: 0.987013\n",
      "Iteration 11, loss = 0.04240246\n",
      "Validation score: 0.987013\n",
      "Iteration 12, loss = 0.04298408\n",
      "Validation score: 0.987013\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "1 10617 173\n",
      "Iteration 1, loss = 0.22521033\n",
      "Validation score: 0.987759\n",
      "Iteration 2, loss = 0.07557132\n",
      "Validation score: 0.987759\n",
      "Iteration 3, loss = 0.05631219\n",
      "Validation score: 0.987759\n",
      "Iteration 4, loss = 0.04912314\n",
      "Validation score: 0.987759\n",
      "Iteration 5, loss = 0.04646625\n",
      "Validation score: 0.986817\n",
      "Iteration 6, loss = 0.04610225\n",
      "Validation score: 0.987759\n",
      "Iteration 7, loss = 0.04595231\n",
      "Validation score: 0.986817\n",
      "Iteration 8, loss = 0.04520345\n",
      "Validation score: 0.985876\n",
      "Iteration 9, loss = 0.04474785\n",
      "Validation score: 0.985876\n",
      "Iteration 10, loss = 0.04427101\n",
      "Validation score: 0.987759\n",
      "Iteration 11, loss = 0.04417197\n",
      "Validation score: 0.986817\n",
      "Iteration 12, loss = 0.04397762\n",
      "Validation score: 0.985876\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "2 10488 302\n",
      "Iteration 1, loss = 0.22767101\n",
      "Validation score: 0.987607\n",
      "Iteration 2, loss = 0.07794769\n",
      "Validation score: 0.987607\n",
      "Iteration 3, loss = 0.05698580\n",
      "Validation score: 0.987607\n",
      "Iteration 4, loss = 0.04864520\n",
      "Validation score: 0.987607\n",
      "Iteration 5, loss = 0.04610000\n",
      "Validation score: 0.987607\n",
      "Iteration 6, loss = 0.04549393\n",
      "Validation score: 0.987607\n",
      "Iteration 7, loss = 0.04428578\n",
      "Validation score: 0.987607\n",
      "Iteration 8, loss = 0.04406085\n",
      "Validation score: 0.987607\n",
      "Iteration 9, loss = 0.04354513\n",
      "Validation score: 0.987607\n",
      "Iteration 10, loss = 0.04269821\n",
      "Validation score: 0.987607\n",
      "Iteration 11, loss = 0.04169664\n",
      "Validation score: 0.987607\n",
      "Iteration 12, loss = 0.04197791\n",
      "Validation score: 0.987607\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "3 10599 191\n",
      "Iteration 1, loss = 0.21973647\n",
      "Validation score: 0.988679\n",
      "Iteration 2, loss = 0.07291508\n",
      "Validation score: 0.988679\n",
      "Iteration 3, loss = 0.05376950\n",
      "Validation score: 0.988679\n",
      "Iteration 4, loss = 0.04639793\n",
      "Validation score: 0.988679\n",
      "Iteration 5, loss = 0.04387640\n",
      "Validation score: 0.988679\n",
      "Iteration 6, loss = 0.04234230\n",
      "Validation score: 0.988679\n",
      "Iteration 7, loss = 0.04211487\n",
      "Validation score: 0.988679\n",
      "Iteration 8, loss = 0.04081208\n",
      "Validation score: 0.988679\n",
      "Iteration 9, loss = 0.04103377\n",
      "Validation score: 0.988679\n",
      "Iteration 10, loss = 0.04018621\n",
      "Validation score: 0.989623\n",
      "Iteration 11, loss = 0.03960046\n",
      "Validation score: 0.988679\n",
      "Iteration 12, loss = 0.03934185\n",
      "Validation score: 0.988679\n",
      "Iteration 13, loss = 0.03829209\n",
      "Validation score: 0.989623\n",
      "Iteration 14, loss = 0.03757386\n",
      "Validation score: 0.989623\n",
      "Iteration 15, loss = 0.03814896\n",
      "Validation score: 0.989623\n",
      "Iteration 16, loss = 0.03733012\n",
      "Validation score: 0.989623\n",
      "Iteration 17, loss = 0.03656965\n",
      "Validation score: 0.988679\n",
      "Iteration 18, loss = 0.03836407\n",
      "Validation score: 0.986792\n",
      "Iteration 19, loss = 0.03623860\n",
      "Validation score: 0.987736\n",
      "Iteration 20, loss = 0.03517358\n",
      "Validation score: 0.988679\n",
      "Iteration 21, loss = 0.03482838\n",
      "Validation score: 0.988679\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "4 10604 186\n",
      "Iteration 1, loss = 0.22732756\n",
      "Validation score: 0.986805\n",
      "Iteration 2, loss = 0.07517352\n",
      "Validation score: 0.986805\n",
      "Iteration 3, loss = 0.05584503\n",
      "Validation score: 0.986805\n",
      "Iteration 4, loss = 0.04814469\n",
      "Validation score: 0.986805\n",
      "Iteration 5, loss = 0.04559324\n",
      "Validation score: 0.986805\n",
      "Iteration 6, loss = 0.04460278\n",
      "Validation score: 0.986805\n",
      "Iteration 7, loss = 0.04462732\n",
      "Validation score: 0.986805\n",
      "Iteration 8, loss = 0.04422507\n",
      "Validation score: 0.986805\n",
      "Iteration 9, loss = 0.04378327\n",
      "Validation score: 0.986805\n",
      "Iteration 10, loss = 0.04413176\n",
      "Validation score: 0.986805\n",
      "Iteration 11, loss = 0.04289775\n",
      "Validation score: 0.986805\n",
      "Iteration 12, loss = 0.04262200\n",
      "Validation score: 0.986805\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "5 10767 23\n",
      "Iteration 1, loss = 0.22423582\n",
      "Validation score: 0.987001\n",
      "Iteration 2, loss = 0.07504788\n",
      "Validation score: 0.987001\n",
      "Iteration 3, loss = 0.05425307\n",
      "Validation score: 0.987001\n",
      "Iteration 4, loss = 0.04627552\n",
      "Validation score: 0.987001\n",
      "Iteration 5, loss = 0.04392320\n",
      "Validation score: 0.987001\n",
      "Iteration 6, loss = 0.04332134\n",
      "Validation score: 0.987001\n",
      "Iteration 7, loss = 0.04275266\n",
      "Validation score: 0.987001\n",
      "Iteration 8, loss = 0.04211012\n",
      "Validation score: 0.987001\n",
      "Iteration 9, loss = 0.04230859\n",
      "Validation score: 0.987001\n",
      "Iteration 10, loss = 0.04146595\n",
      "Validation score: 0.987001\n",
      "Iteration 11, loss = 0.04087312\n",
      "Validation score: 0.987001\n",
      "Iteration 12, loss = 0.04023743\n",
      "Validation score: 0.987001\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "6 10437 353\n",
      "Iteration 1, loss = 0.22736745\n",
      "Validation score: 0.988506\n",
      "Iteration 2, loss = 0.07537860\n",
      "Validation score: 0.988506\n",
      "Iteration 3, loss = 0.05667522\n",
      "Validation score: 0.988506\n",
      "Iteration 4, loss = 0.04873113\n",
      "Validation score: 0.988506\n",
      "Iteration 5, loss = 0.04523755\n",
      "Validation score: 0.988506\n",
      "Iteration 6, loss = 0.04403088\n",
      "Validation score: 0.988506\n",
      "Iteration 7, loss = 0.04332334\n",
      "Validation score: 0.988506\n",
      "Iteration 8, loss = 0.04271226\n",
      "Validation score: 0.988506\n",
      "Iteration 9, loss = 0.04289318\n",
      "Validation score: 0.988506\n",
      "Iteration 10, loss = 0.04279593\n",
      "Validation score: 0.988506\n",
      "Iteration 11, loss = 0.04243196\n",
      "Validation score: 0.988506\n",
      "Iteration 12, loss = 0.04191760\n",
      "Validation score: 0.988506\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "7 10541 249\n",
      "Iteration 1, loss = 0.22758350\n",
      "Validation score: 0.986730\n",
      "Iteration 2, loss = 0.07633266\n",
      "Validation score: 0.986730\n",
      "Iteration 3, loss = 0.05610391\n",
      "Validation score: 0.986730\n",
      "Iteration 4, loss = 0.04813329\n",
      "Validation score: 0.986730\n",
      "Iteration 5, loss = 0.04569774\n",
      "Validation score: 0.986730\n",
      "Iteration 6, loss = 0.04508539\n",
      "Validation score: 0.986730\n",
      "Iteration 7, loss = 0.04429032\n",
      "Validation score: 0.985782\n",
      "Iteration 8, loss = 0.04477019\n",
      "Validation score: 0.986730\n",
      "Iteration 9, loss = 0.04343999\n",
      "Validation score: 0.986730\n",
      "Iteration 10, loss = 0.04388769\n",
      "Validation score: 0.986730\n",
      "Iteration 11, loss = 0.04305474\n",
      "Validation score: 0.986730\n",
      "Iteration 12, loss = 0.04292283\n",
      "Validation score: 0.986730\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "8 10500 290\n",
      "Iteration 1, loss = 0.22750268\n",
      "Validation score: 0.987619\n",
      "Iteration 2, loss = 0.07597311\n",
      "Validation score: 0.987619\n",
      "Iteration 3, loss = 0.05649674\n",
      "Validation score: 0.987619\n",
      "Iteration 4, loss = 0.04887120\n",
      "Validation score: 0.987619\n",
      "Iteration 5, loss = 0.04680428\n",
      "Validation score: 0.987619\n",
      "Iteration 6, loss = 0.04614134\n",
      "Validation score: 0.987619\n",
      "Iteration 7, loss = 0.04527605\n",
      "Validation score: 0.987619\n",
      "Iteration 8, loss = 0.04493385\n",
      "Validation score: 0.987619\n",
      "Iteration 9, loss = 0.04421926\n",
      "Validation score: 0.987619\n",
      "Iteration 10, loss = 0.04430609\n",
      "Validation score: 0.987619\n",
      "Iteration 11, loss = 0.04495155\n",
      "Validation score: 0.987619\n",
      "Iteration 12, loss = 0.04342119\n",
      "Validation score: 0.985714\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "9 10611 179\n",
      "Iteration 1, loss = 0.22105502\n",
      "Validation score: 0.986817\n",
      "Iteration 2, loss = 0.07939670\n",
      "Validation score: 0.986817\n",
      "Iteration 3, loss = 0.05770629\n",
      "Validation score: 0.986817\n",
      "Iteration 4, loss = 0.04973892\n",
      "Validation score: 0.986817\n",
      "Iteration 5, loss = 0.04663444\n",
      "Validation score: 0.986817\n",
      "Iteration 6, loss = 0.04556260\n",
      "Validation score: 0.986817\n",
      "Iteration 7, loss = 0.04475968\n",
      "Validation score: 0.986817\n",
      "Iteration 8, loss = 0.04418943\n",
      "Validation score: 0.986817\n",
      "Iteration 9, loss = 0.04479965\n",
      "Validation score: 0.986817\n",
      "Iteration 10, loss = 0.04363103\n",
      "Validation score: 0.986817\n",
      "Iteration 11, loss = 0.04319035\n",
      "Validation score: 0.986817\n",
      "Iteration 12, loss = 0.04274353\n",
      "Validation score: 0.986817\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "10 10771 19\n",
      "Iteration 1, loss = 0.22361785\n",
      "Validation score: 0.987013\n",
      "Iteration 2, loss = 0.07482025\n",
      "Validation score: 0.987013\n",
      "Iteration 3, loss = 0.05239670\n",
      "Validation score: 0.987013\n",
      "Iteration 4, loss = 0.04589728\n",
      "Validation score: 0.987013\n",
      "Iteration 5, loss = 0.04517565\n",
      "Validation score: 0.987013\n",
      "Iteration 6, loss = 0.04327339\n",
      "Validation score: 0.987013\n",
      "Iteration 7, loss = 0.04243470\n",
      "Validation score: 0.987013\n",
      "Iteration 8, loss = 0.04170586\n",
      "Validation score: 0.987013\n",
      "Iteration 9, loss = 0.04110979\n",
      "Validation score: 0.987013\n",
      "Iteration 10, loss = 0.04104477\n",
      "Validation score: 0.987013\n",
      "Iteration 11, loss = 0.04017662\n",
      "Validation score: 0.987941\n",
      "Iteration 12, loss = 0.04106442\n",
      "Validation score: 0.987941\n",
      "Iteration 13, loss = 0.03959727\n",
      "Validation score: 0.987941\n",
      "Iteration 14, loss = 0.03891511\n",
      "Validation score: 0.987013\n",
      "Iteration 15, loss = 0.03831343\n",
      "Validation score: 0.987013\n",
      "Iteration 16, loss = 0.03966018\n",
      "Validation score: 0.987941\n",
      "Iteration 17, loss = 0.03835055\n",
      "Validation score: 0.987941\n",
      "Iteration 18, loss = 0.03806664\n",
      "Validation score: 0.987941\n",
      "Iteration 19, loss = 0.03711388\n",
      "Validation score: 0.987013\n",
      "Iteration 20, loss = 0.03757105\n",
      "Validation score: 0.987941\n",
      "Iteration 21, loss = 0.03680492\n",
      "Validation score: 0.986085\n",
      "Iteration 22, loss = 0.03662702\n",
      "Validation score: 0.987941\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "11 10560 230\n",
      "Iteration 1, loss = 0.22761937\n",
      "Validation score: 0.987689\n",
      "Iteration 2, loss = 0.07662942\n",
      "Validation score: 0.987689\n",
      "Iteration 3, loss = 0.05745223\n",
      "Validation score: 0.987689\n",
      "Iteration 4, loss = 0.04909300\n",
      "Validation score: 0.987689\n",
      "Iteration 5, loss = 0.04697268\n",
      "Validation score: 0.987689\n",
      "Iteration 6, loss = 0.04586719\n",
      "Validation score: 0.987689\n",
      "Iteration 7, loss = 0.04641238\n",
      "Validation score: 0.987689\n",
      "Iteration 8, loss = 0.04448851\n",
      "Validation score: 0.987689\n",
      "Iteration 9, loss = 0.04437289\n",
      "Validation score: 0.987689\n",
      "Iteration 10, loss = 0.04384232\n",
      "Validation score: 0.987689\n",
      "Iteration 11, loss = 0.04283637\n",
      "Validation score: 0.987689\n",
      "Iteration 12, loss = 0.04286693\n",
      "Validation score: 0.987689\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "12 10532 258\n",
      "Iteration 1, loss = 0.22466861\n",
      "Validation score: 0.986717\n",
      "Iteration 2, loss = 0.07642667\n",
      "Validation score: 0.986717\n",
      "Iteration 3, loss = 0.05641585\n",
      "Validation score: 0.986717\n",
      "Iteration 4, loss = 0.04906229\n",
      "Validation score: 0.986717\n",
      "Iteration 5, loss = 0.04679432\n",
      "Validation score: 0.986717\n",
      "Iteration 6, loss = 0.04660970\n",
      "Validation score: 0.986717\n",
      "Iteration 7, loss = 0.04581858\n",
      "Validation score: 0.986717\n",
      "Iteration 8, loss = 0.04582857\n",
      "Validation score: 0.986717\n",
      "Iteration 9, loss = 0.04468776\n",
      "Validation score: 0.986717\n",
      "Iteration 10, loss = 0.04513833\n",
      "Validation score: 0.986717\n",
      "Iteration 11, loss = 0.04404519\n",
      "Validation score: 0.986717\n",
      "Iteration 12, loss = 0.04388895\n",
      "Validation score: 0.986717\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "13 10716 74\n",
      "Iteration 1, loss = 0.23018223\n",
      "Validation score: 0.986940\n",
      "Iteration 2, loss = 0.07446841\n",
      "Validation score: 0.986940\n",
      "Iteration 3, loss = 0.05592164\n",
      "Validation score: 0.986940\n",
      "Iteration 4, loss = 0.04821509\n",
      "Validation score: 0.986940\n",
      "Iteration 5, loss = 0.04626422\n",
      "Validation score: 0.986940\n",
      "Iteration 6, loss = 0.04564010\n",
      "Validation score: 0.986940\n",
      "Iteration 7, loss = 0.04443268\n",
      "Validation score: 0.986940\n",
      "Iteration 8, loss = 0.04474760\n",
      "Validation score: 0.986940\n",
      "Iteration 9, loss = 0.04341990\n",
      "Validation score: 0.986940\n",
      "Iteration 10, loss = 0.04282511\n",
      "Validation score: 0.986940\n",
      "Iteration 11, loss = 0.04322894\n",
      "Validation score: 0.986940\n",
      "Iteration 12, loss = 0.04214138\n",
      "Validation score: 0.986940\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "14 10524 266\n",
      "Iteration 1, loss = 0.23284413\n",
      "Validation score: 0.987654\n",
      "Iteration 2, loss = 0.07736409\n",
      "Validation score: 0.987654\n",
      "Iteration 3, loss = 0.05773053\n",
      "Validation score: 0.987654\n",
      "Iteration 4, loss = 0.04982059\n",
      "Validation score: 0.987654\n",
      "Iteration 5, loss = 0.04703655\n",
      "Validation score: 0.987654\n",
      "Iteration 6, loss = 0.04642111\n",
      "Validation score: 0.987654\n",
      "Iteration 7, loss = 0.04627558\n",
      "Validation score: 0.987654\n",
      "Iteration 8, loss = 0.04523459\n",
      "Validation score: 0.987654\n",
      "Iteration 9, loss = 0.04438599\n",
      "Validation score: 0.987654\n",
      "Iteration 10, loss = 0.04450727\n",
      "Validation score: 0.987654\n",
      "Iteration 11, loss = 0.04537667\n",
      "Validation score: 0.987654\n",
      "Iteration 12, loss = 0.04375558\n",
      "Validation score: 0.987654\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "15 10548 242\n",
      "Iteration 1, loss = 0.23180316\n",
      "Validation score: 0.987678\n",
      "Iteration 2, loss = 0.07936943\n",
      "Validation score: 0.987678\n",
      "Iteration 3, loss = 0.05643508\n",
      "Validation score: 0.987678\n",
      "Iteration 4, loss = 0.04903093\n",
      "Validation score: 0.987678\n",
      "Iteration 5, loss = 0.04627390\n",
      "Validation score: 0.987678\n",
      "Iteration 6, loss = 0.04558087\n",
      "Validation score: 0.987678\n",
      "Iteration 7, loss = 0.04557528\n",
      "Validation score: 0.987678\n",
      "Iteration 8, loss = 0.04453410\n",
      "Validation score: 0.987678\n",
      "Iteration 9, loss = 0.04448003\n",
      "Validation score: 0.987678\n",
      "Iteration 10, loss = 0.04445249\n",
      "Validation score: 0.987678\n",
      "Iteration 11, loss = 0.04354752\n",
      "Validation score: 0.987678\n",
      "Iteration 12, loss = 0.04335381\n",
      "Validation score: 0.987678\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "16 10665 125\n",
      "Iteration 1, loss = 0.22934638\n",
      "Validation score: 0.987816\n",
      "Iteration 2, loss = 0.07559412\n",
      "Validation score: 0.987816\n",
      "Iteration 3, loss = 0.05669433\n",
      "Validation score: 0.987816\n",
      "Iteration 4, loss = 0.05007770\n",
      "Validation score: 0.987816\n",
      "Iteration 5, loss = 0.04693170\n",
      "Validation score: 0.987816\n",
      "Iteration 6, loss = 0.04578626\n",
      "Validation score: 0.987816\n",
      "Iteration 7, loss = 0.04557961\n",
      "Validation score: 0.987816\n",
      "Iteration 8, loss = 0.04484440\n",
      "Validation score: 0.987816\n",
      "Iteration 9, loss = 0.04464717\n",
      "Validation score: 0.987816\n",
      "Iteration 10, loss = 0.04521251\n",
      "Validation score: 0.987816\n",
      "Iteration 11, loss = 0.04375268\n",
      "Validation score: 0.987816\n",
      "Iteration 12, loss = 0.04324400\n",
      "Validation score: 0.987816\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "17 10508 282\n",
      "Iteration 1, loss = 0.22748731\n",
      "Validation score: 0.986679\n",
      "Iteration 2, loss = 0.07778573\n",
      "Validation score: 0.986679\n",
      "Iteration 3, loss = 0.05671729\n",
      "Validation score: 0.986679\n",
      "Iteration 4, loss = 0.04905350\n",
      "Validation score: 0.986679\n",
      "Iteration 5, loss = 0.04706439\n",
      "Validation score: 0.986679\n",
      "Iteration 6, loss = 0.04585826\n",
      "Validation score: 0.986679\n",
      "Iteration 7, loss = 0.04536436\n",
      "Validation score: 0.986679\n",
      "Iteration 8, loss = 0.04474110\n",
      "Validation score: 0.986679\n",
      "Iteration 9, loss = 0.04527511\n",
      "Validation score: 0.986679\n",
      "Iteration 10, loss = 0.04442644\n",
      "Validation score: 0.986679\n",
      "Iteration 11, loss = 0.04481285\n",
      "Validation score: 0.986679\n",
      "Iteration 12, loss = 0.04389519\n",
      "Validation score: 0.986679\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "18 10668 122\n",
      "Iteration 1, loss = 0.22274644\n",
      "Validation score: 0.986879\n",
      "Iteration 2, loss = 0.07856502\n",
      "Validation score: 0.986879\n",
      "Iteration 3, loss = 0.05774853\n",
      "Validation score: 0.986879\n",
      "Iteration 4, loss = 0.04899176\n",
      "Validation score: 0.986879\n",
      "Iteration 5, loss = 0.04747618\n",
      "Validation score: 0.986879\n",
      "Iteration 6, loss = 0.04683448\n",
      "Validation score: 0.986879\n",
      "Iteration 7, loss = 0.04580912\n",
      "Validation score: 0.986879\n",
      "Iteration 8, loss = 0.04483784\n",
      "Validation score: 0.986879\n",
      "Iteration 9, loss = 0.04416348\n",
      "Validation score: 0.986879\n",
      "Iteration 10, loss = 0.04383391\n",
      "Validation score: 0.986879\n",
      "Iteration 11, loss = 0.04354727\n",
      "Validation score: 0.986879\n",
      "Iteration 12, loss = 0.04354537\n",
      "Validation score: 0.986879\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "19 10570 220\n",
      "Iteration 1, loss = 0.22552498\n",
      "Validation score: 0.987701\n",
      "Iteration 2, loss = 0.07496857\n",
      "Validation score: 0.987701\n",
      "Iteration 3, loss = 0.05527252\n",
      "Validation score: 0.987701\n",
      "Iteration 4, loss = 0.04827744\n",
      "Validation score: 0.987701\n",
      "Iteration 5, loss = 0.04666705\n",
      "Validation score: 0.986755\n",
      "Iteration 6, loss = 0.04627871\n",
      "Validation score: 0.987701\n",
      "Iteration 7, loss = 0.04511566\n",
      "Validation score: 0.986755\n",
      "Iteration 8, loss = 0.04496885\n",
      "Validation score: 0.987701\n",
      "Iteration 9, loss = 0.04365089\n",
      "Validation score: 0.987701\n",
      "Iteration 10, loss = 0.04332962\n",
      "Validation score: 0.987701\n",
      "Iteration 11, loss = 0.04332922\n",
      "Validation score: 0.985809\n",
      "Iteration 12, loss = 0.04216070\n",
      "Validation score: 0.987701\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "20 10588 202\n",
      "Iteration 1, loss = 0.22627007\n",
      "Validation score: 0.986780\n",
      "Iteration 2, loss = 0.07500231\n",
      "Validation score: 0.986780\n",
      "Iteration 3, loss = 0.05817346\n",
      "Validation score: 0.986780\n",
      "Iteration 4, loss = 0.05082528\n",
      "Validation score: 0.986780\n",
      "Iteration 5, loss = 0.04849028\n",
      "Validation score: 0.986780\n",
      "Iteration 6, loss = 0.04688378\n",
      "Validation score: 0.986780\n",
      "Iteration 7, loss = 0.04643508\n",
      "Validation score: 0.986780\n",
      "Iteration 8, loss = 0.04594092\n",
      "Validation score: 0.986780\n",
      "Iteration 9, loss = 0.04586012\n",
      "Validation score: 0.986780\n",
      "Iteration 10, loss = 0.04476964\n",
      "Validation score: 0.986780\n",
      "Iteration 11, loss = 0.04462922\n",
      "Validation score: 0.986780\n",
      "Iteration 12, loss = 0.04381544\n",
      "Validation score: 0.986780\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "21 10521 269\n",
      "Iteration 1, loss = 0.22741828\n",
      "Validation score: 0.987654\n",
      "Iteration 2, loss = 0.07790387\n",
      "Validation score: 0.987654\n",
      "Iteration 3, loss = 0.05739359\n",
      "Validation score: 0.987654\n",
      "Iteration 4, loss = 0.04844085\n",
      "Validation score: 0.987654\n",
      "Iteration 5, loss = 0.04642357\n",
      "Validation score: 0.987654\n",
      "Iteration 6, loss = 0.04781641\n",
      "Validation score: 0.987654\n",
      "Iteration 7, loss = 0.04443787\n",
      "Validation score: 0.987654\n",
      "Iteration 8, loss = 0.04401646\n",
      "Validation score: 0.987654\n",
      "Iteration 9, loss = 0.04378196\n",
      "Validation score: 0.987654\n",
      "Iteration 10, loss = 0.04327659\n",
      "Validation score: 0.987654\n",
      "Iteration 11, loss = 0.04312580\n",
      "Validation score: 0.987654\n",
      "Iteration 12, loss = 0.04244276\n",
      "Validation score: 0.987654\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "22 10627 163\n",
      "Iteration 1, loss = 0.22631478\n",
      "Validation score: 0.986830\n",
      "Iteration 2, loss = 0.07534627\n",
      "Validation score: 0.986830\n",
      "Iteration 3, loss = 0.05399475\n",
      "Validation score: 0.986830\n",
      "Iteration 4, loss = 0.04717724\n",
      "Validation score: 0.986830\n",
      "Iteration 5, loss = 0.04567835\n",
      "Validation score: 0.986830\n",
      "Iteration 6, loss = 0.04424774\n",
      "Validation score: 0.986830\n",
      "Iteration 7, loss = 0.04574133\n",
      "Validation score: 0.985889\n",
      "Iteration 8, loss = 0.04358883\n",
      "Validation score: 0.984948\n",
      "Iteration 9, loss = 0.04335730\n",
      "Validation score: 0.985889\n",
      "Iteration 10, loss = 0.04366024\n",
      "Validation score: 0.982126\n",
      "Iteration 11, loss = 0.04345333\n",
      "Validation score: 0.985889\n",
      "Iteration 12, loss = 0.04235941\n",
      "Validation score: 0.985889\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "23 10600 190\n",
      "Iteration 1, loss = 0.21948461\n",
      "Validation score: 0.990566\n",
      "Iteration 2, loss = 0.06348599\n",
      "Validation score: 0.990566\n",
      "Iteration 3, loss = 0.04703253\n",
      "Validation score: 0.990566\n",
      "Iteration 4, loss = 0.03856717\n",
      "Validation score: 0.990566\n",
      "Iteration 5, loss = 0.03545072\n",
      "Validation score: 0.990566\n",
      "Iteration 6, loss = 0.03456923\n",
      "Validation score: 0.990566\n",
      "Iteration 7, loss = 0.03311498\n",
      "Validation score: 0.990566\n",
      "Iteration 8, loss = 0.03328244\n",
      "Validation score: 0.990566\n",
      "Iteration 9, loss = 0.03268783\n",
      "Validation score: 0.990566\n",
      "Iteration 10, loss = 0.03258719\n",
      "Validation score: 0.990566\n",
      "Iteration 11, loss = 0.03189965\n",
      "Validation score: 0.990566\n",
      "Iteration 12, loss = 0.03210697\n",
      "Validation score: 0.991509\n",
      "Iteration 13, loss = 0.03146525\n",
      "Validation score: 0.990566\n",
      "Iteration 14, loss = 0.03164070\n",
      "Validation score: 0.990566\n",
      "Iteration 15, loss = 0.03112760\n",
      "Validation score: 0.990566\n",
      "Iteration 16, loss = 0.03136757\n",
      "Validation score: 0.990566\n",
      "Iteration 17, loss = 0.03024876\n",
      "Validation score: 0.990566\n",
      "Iteration 18, loss = 0.03045857\n",
      "Validation score: 0.990566\n",
      "Iteration 19, loss = 0.02997758\n",
      "Validation score: 0.990566\n",
      "Iteration 20, loss = 0.02962099\n",
      "Validation score: 0.990566\n",
      "Iteration 21, loss = 0.02930700\n",
      "Validation score: 0.990566\n",
      "Iteration 22, loss = 0.02959074\n",
      "Validation score: 0.990566\n",
      "Iteration 23, loss = 0.02929198\n",
      "Validation score: 0.990566\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "24 10771 19\n",
      "Iteration 1, loss = 0.22385204\n",
      "Validation score: 0.987013\n",
      "Iteration 2, loss = 0.07469063\n",
      "Validation score: 0.987013\n",
      "Iteration 3, loss = 0.05391718\n",
      "Validation score: 0.987013\n",
      "Iteration 4, loss = 0.04638681\n",
      "Validation score: 0.987013\n",
      "Iteration 5, loss = 0.04519073\n",
      "Validation score: 0.987013\n",
      "Iteration 6, loss = 0.04351634\n",
      "Validation score: 0.987013\n",
      "Iteration 7, loss = 0.04256739\n",
      "Validation score: 0.987013\n",
      "Iteration 8, loss = 0.04192275\n",
      "Validation score: 0.987013\n",
      "Iteration 9, loss = 0.04144970\n",
      "Validation score: 0.987013\n",
      "Iteration 10, loss = 0.04131678\n",
      "Validation score: 0.987013\n",
      "Iteration 11, loss = 0.04050652\n",
      "Validation score: 0.987941\n",
      "Iteration 12, loss = 0.04108071\n",
      "Validation score: 0.987013\n",
      "Iteration 13, loss = 0.04007661\n",
      "Validation score: 0.987941\n",
      "Iteration 14, loss = 0.03922356\n",
      "Validation score: 0.987013\n",
      "Iteration 15, loss = 0.03856917\n",
      "Validation score: 0.987013\n",
      "Iteration 16, loss = 0.03959853\n",
      "Validation score: 0.987941\n",
      "Iteration 17, loss = 0.03860704\n",
      "Validation score: 0.987013\n",
      "Iteration 18, loss = 0.03823933\n",
      "Validation score: 0.987941\n",
      "Iteration 19, loss = 0.03715081\n",
      "Validation score: 0.987013\n",
      "Iteration 20, loss = 0.03775047\n",
      "Validation score: 0.987013\n",
      "Iteration 21, loss = 0.03702009\n",
      "Validation score: 0.984230\n",
      "Iteration 22, loss = 0.03668051\n",
      "Validation score: 0.987013\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "25 10520 270\n",
      "Iteration 1, loss = 0.22829587\n",
      "Validation score: 0.987643\n",
      "Iteration 2, loss = 0.07253802\n",
      "Validation score: 0.987643\n",
      "Iteration 3, loss = 0.05501251\n",
      "Validation score: 0.987643\n",
      "Iteration 4, loss = 0.04830663\n",
      "Validation score: 0.987643\n",
      "Iteration 5, loss = 0.04581111\n",
      "Validation score: 0.987643\n",
      "Iteration 6, loss = 0.04537064\n",
      "Validation score: 0.987643\n",
      "Iteration 7, loss = 0.04568087\n",
      "Validation score: 0.987643\n",
      "Iteration 8, loss = 0.04444826\n",
      "Validation score: 0.987643\n",
      "Iteration 9, loss = 0.04415843\n",
      "Validation score: 0.987643\n",
      "Iteration 10, loss = 0.04324453\n",
      "Validation score: 0.987643\n",
      "Iteration 11, loss = 0.04322114\n",
      "Validation score: 0.987643\n",
      "Iteration 12, loss = 0.04306793\n",
      "Validation score: 0.987643\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "26 10495 295\n",
      "Iteration 1, loss = 0.22256417\n",
      "Validation score: 0.987619\n",
      "Iteration 2, loss = 0.07823694\n",
      "Validation score: 0.987619\n",
      "Iteration 3, loss = 0.05748707\n",
      "Validation score: 0.987619\n",
      "Iteration 4, loss = 0.04891734\n",
      "Validation score: 0.987619\n",
      "Iteration 5, loss = 0.04705074\n",
      "Validation score: 0.987619\n",
      "Iteration 6, loss = 0.04547070\n",
      "Validation score: 0.987619\n",
      "Iteration 7, loss = 0.04538610\n",
      "Validation score: 0.987619\n",
      "Iteration 8, loss = 0.04510744\n",
      "Validation score: 0.987619\n",
      "Iteration 9, loss = 0.04461279\n",
      "Validation score: 0.987619\n",
      "Iteration 10, loss = 0.04442189\n",
      "Validation score: 0.987619\n",
      "Iteration 11, loss = 0.04342583\n",
      "Validation score: 0.987619\n",
      "Iteration 12, loss = 0.04330432\n",
      "Validation score: 0.987619\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "27 10520 270\n",
      "Iteration 1, loss = 0.22708186\n",
      "Validation score: 0.987643\n",
      "Iteration 2, loss = 0.07810856\n",
      "Validation score: 0.987643\n",
      "Iteration 3, loss = 0.05868057\n",
      "Validation score: 0.987643\n",
      "Iteration 4, loss = 0.04996469\n",
      "Validation score: 0.987643\n",
      "Iteration 5, loss = 0.04771166\n",
      "Validation score: 0.987643\n",
      "Iteration 6, loss = 0.04909188\n",
      "Validation score: 0.987643\n",
      "Iteration 7, loss = 0.04584249\n",
      "Validation score: 0.987643\n",
      "Iteration 8, loss = 0.04545432\n",
      "Validation score: 0.987643\n",
      "Iteration 9, loss = 0.04461397\n",
      "Validation score: 0.987643\n",
      "Iteration 10, loss = 0.04452225\n",
      "Validation score: 0.987643\n",
      "Iteration 11, loss = 0.04432689\n",
      "Validation score: 0.987643\n",
      "Iteration 12, loss = 0.04384958\n",
      "Validation score: 0.987643\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "28 10605 185\n",
      "Iteration 1, loss = 0.22636121\n",
      "Validation score: 0.988690\n",
      "Iteration 2, loss = 0.07199567\n",
      "Validation score: 0.988690\n",
      "Iteration 3, loss = 0.05385939\n",
      "Validation score: 0.988690\n",
      "Iteration 4, loss = 0.04646291\n",
      "Validation score: 0.988690\n",
      "Iteration 5, loss = 0.04357438\n",
      "Validation score: 0.988690\n",
      "Iteration 6, loss = 0.04393831\n",
      "Validation score: 0.988690\n",
      "Iteration 7, loss = 0.04195474\n",
      "Validation score: 0.988690\n",
      "Iteration 8, loss = 0.04228522\n",
      "Validation score: 0.988690\n",
      "Iteration 9, loss = 0.04130980\n",
      "Validation score: 0.988690\n",
      "Iteration 10, loss = 0.04048886\n",
      "Validation score: 0.988690\n",
      "Iteration 11, loss = 0.04076782\n",
      "Validation score: 0.988690\n",
      "Iteration 12, loss = 0.04032485\n",
      "Validation score: 0.988690\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "29 10616 174\n",
      "Iteration 1, loss = 0.22782415\n",
      "Validation score: 0.987759\n",
      "Iteration 2, loss = 0.07569070\n",
      "Validation score: 0.987759\n",
      "Iteration 3, loss = 0.05622919\n",
      "Validation score: 0.987759\n",
      "Iteration 4, loss = 0.04835769\n",
      "Validation score: 0.987759\n",
      "Iteration 5, loss = 0.04495331\n",
      "Validation score: 0.987759\n",
      "Iteration 6, loss = 0.04512870\n",
      "Validation score: 0.987759\n",
      "Iteration 7, loss = 0.04449435\n",
      "Validation score: 0.987759\n",
      "Iteration 8, loss = 0.04345135\n",
      "Validation score: 0.987759\n",
      "Iteration 9, loss = 0.04312681\n",
      "Validation score: 0.987759\n",
      "Iteration 10, loss = 0.04245060\n",
      "Validation score: 0.987759\n",
      "Iteration 11, loss = 0.04312899\n",
      "Validation score: 0.987759\n",
      "Iteration 12, loss = 0.04183041\n",
      "Validation score: 0.987759\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "30 10576 214\n",
      "Iteration 1, loss = 0.23009099\n",
      "Validation score: 0.987713\n",
      "Iteration 2, loss = 0.07742843\n",
      "Validation score: 0.987713\n",
      "Iteration 3, loss = 0.05744550\n",
      "Validation score: 0.987713\n",
      "Iteration 4, loss = 0.04941936\n",
      "Validation score: 0.987713\n",
      "Iteration 5, loss = 0.04645300\n",
      "Validation score: 0.987713\n",
      "Iteration 6, loss = 0.04561417\n",
      "Validation score: 0.987713\n",
      "Iteration 7, loss = 0.04481961\n",
      "Validation score: 0.987713\n",
      "Iteration 8, loss = 0.04425175\n",
      "Validation score: 0.987713\n",
      "Iteration 9, loss = 0.04470928\n",
      "Validation score: 0.987713\n",
      "Iteration 10, loss = 0.04411266\n",
      "Validation score: 0.987713\n",
      "Iteration 11, loss = 0.04307763\n",
      "Validation score: 0.987713\n",
      "Iteration 12, loss = 0.04267200\n",
      "Validation score: 0.987713\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "31 10556 234\n",
      "Iteration 1, loss = 0.23165452\n",
      "Validation score: 0.987689\n",
      "Iteration 2, loss = 0.07264441\n",
      "Validation score: 0.987689\n",
      "Iteration 3, loss = 0.05372795\n",
      "Validation score: 0.987689\n",
      "Iteration 4, loss = 0.04715079\n",
      "Validation score: 0.987689\n",
      "Iteration 5, loss = 0.04492531\n",
      "Validation score: 0.987689\n",
      "Iteration 6, loss = 0.04501456\n",
      "Validation score: 0.987689\n",
      "Iteration 7, loss = 0.04421988\n",
      "Validation score: 0.987689\n",
      "Iteration 8, loss = 0.04352736\n",
      "Validation score: 0.987689\n",
      "Iteration 9, loss = 0.04311541\n",
      "Validation score: 0.987689\n",
      "Iteration 10, loss = 0.04286758\n",
      "Validation score: 0.986742\n",
      "Iteration 11, loss = 0.04195719\n",
      "Validation score: 0.987689\n",
      "Iteration 12, loss = 0.04178305\n",
      "Validation score: 0.987689\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "32 10687 103\n",
      "Iteration 1, loss = 0.22626099\n",
      "Validation score: 0.986904\n",
      "Iteration 2, loss = 0.07690988\n",
      "Validation score: 0.986904\n",
      "Iteration 3, loss = 0.05707438\n",
      "Validation score: 0.986904\n",
      "Iteration 4, loss = 0.04934173\n",
      "Validation score: 0.986904\n",
      "Iteration 5, loss = 0.04617280\n",
      "Validation score: 0.986904\n",
      "Iteration 6, loss = 0.04513129\n",
      "Validation score: 0.986904\n",
      "Iteration 7, loss = 0.04499653\n",
      "Validation score: 0.986904\n",
      "Iteration 8, loss = 0.04381080\n",
      "Validation score: 0.986904\n",
      "Iteration 9, loss = 0.04475582\n",
      "Validation score: 0.986904\n",
      "Iteration 10, loss = 0.04266497\n",
      "Validation score: 0.986904\n",
      "Iteration 11, loss = 0.04318436\n",
      "Validation score: 0.986904\n",
      "Iteration 12, loss = 0.04226293\n",
      "Validation score: 0.987839\n",
      "Iteration 13, loss = 0.04179380\n",
      "Validation score: 0.986904\n",
      "Iteration 14, loss = 0.04194919\n",
      "Validation score: 0.986904\n",
      "Iteration 15, loss = 0.04077791\n",
      "Validation score: 0.986904\n",
      "Iteration 16, loss = 0.04034420\n",
      "Validation score: 0.986904\n",
      "Iteration 17, loss = 0.04072564\n",
      "Validation score: 0.986904\n",
      "Iteration 18, loss = 0.04017811\n",
      "Validation score: 0.987839\n",
      "Iteration 19, loss = 0.03972285\n",
      "Validation score: 0.986904\n",
      "Iteration 20, loss = 0.03868802\n",
      "Validation score: 0.985968\n",
      "Iteration 21, loss = 0.03839607\n",
      "Validation score: 0.986904\n",
      "Iteration 22, loss = 0.03905750\n",
      "Validation score: 0.986904\n",
      "Iteration 23, loss = 0.03787891\n",
      "Validation score: 0.986904\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "33 10560 230\n",
      "Iteration 1, loss = 0.22775767\n",
      "Validation score: 0.987689\n",
      "Iteration 2, loss = 0.07669730\n",
      "Validation score: 0.987689\n",
      "Iteration 3, loss = 0.05646582\n",
      "Validation score: 0.987689\n",
      "Iteration 4, loss = 0.04832148\n",
      "Validation score: 0.987689\n",
      "Iteration 5, loss = 0.04684374\n",
      "Validation score: 0.987689\n",
      "Iteration 6, loss = 0.04571477\n",
      "Validation score: 0.987689\n",
      "Iteration 7, loss = 0.04611279\n",
      "Validation score: 0.987689\n",
      "Iteration 8, loss = 0.04448315\n",
      "Validation score: 0.987689\n",
      "Iteration 9, loss = 0.04428658\n",
      "Validation score: 0.987689\n",
      "Iteration 10, loss = 0.04369143\n",
      "Validation score: 0.987689\n",
      "Iteration 11, loss = 0.04305567\n",
      "Validation score: 0.987689\n",
      "Iteration 12, loss = 0.04313682\n",
      "Validation score: 0.987689\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "34 10625 165\n",
      "Iteration 1, loss = 0.22675372\n",
      "Validation score: 0.986830\n",
      "Iteration 2, loss = 0.07720642\n",
      "Validation score: 0.986830\n",
      "Iteration 3, loss = 0.05770439\n",
      "Validation score: 0.986830\n",
      "Iteration 4, loss = 0.05090362\n",
      "Validation score: 0.986830\n",
      "Iteration 5, loss = 0.04749754\n",
      "Validation score: 0.986830\n",
      "Iteration 6, loss = 0.04617658\n",
      "Validation score: 0.986830\n",
      "Iteration 7, loss = 0.04623069\n",
      "Validation score: 0.986830\n",
      "Iteration 8, loss = 0.04589447\n",
      "Validation score: 0.986830\n",
      "Iteration 9, loss = 0.04527731\n",
      "Validation score: 0.986830\n",
      "Iteration 10, loss = 0.04452512\n",
      "Validation score: 0.986830\n",
      "Iteration 11, loss = 0.04364769\n",
      "Validation score: 0.986830\n",
      "Iteration 12, loss = 0.04328283\n",
      "Validation score: 0.986830\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "35 10751 39\n",
      "Iteration 1, loss = 0.22589944\n",
      "Validation score: 0.986989\n",
      "Iteration 2, loss = 0.07370884\n",
      "Validation score: 0.986989\n",
      "Iteration 3, loss = 0.05519787\n",
      "Validation score: 0.986989\n",
      "Iteration 4, loss = 0.04851546\n",
      "Validation score: 0.986989\n",
      "Iteration 5, loss = 0.04591401\n",
      "Validation score: 0.986989\n",
      "Iteration 6, loss = 0.04509237\n",
      "Validation score: 0.986989\n",
      "Iteration 7, loss = 0.04379840\n",
      "Validation score: 0.986989\n",
      "Iteration 8, loss = 0.04342351\n",
      "Validation score: 0.986989\n",
      "Iteration 9, loss = 0.04302213\n",
      "Validation score: 0.986989\n",
      "Iteration 10, loss = 0.04219180\n",
      "Validation score: 0.986989\n",
      "Iteration 11, loss = 0.04205278\n",
      "Validation score: 0.986989\n",
      "Iteration 12, loss = 0.04248365\n",
      "Validation score: 0.986989\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "36 10525 265\n",
      "Iteration 1, loss = 0.22866577\n",
      "Validation score: 0.986705\n",
      "Iteration 2, loss = 0.07772927\n",
      "Validation score: 0.986705\n",
      "Iteration 3, loss = 0.05708880\n",
      "Validation score: 0.986705\n",
      "Iteration 4, loss = 0.05052909\n",
      "Validation score: 0.986705\n",
      "Iteration 5, loss = 0.04708751\n",
      "Validation score: 0.986705\n",
      "Iteration 6, loss = 0.04638223\n",
      "Validation score: 0.986705\n",
      "Iteration 7, loss = 0.04533237\n",
      "Validation score: 0.986705\n",
      "Iteration 8, loss = 0.04657047\n",
      "Validation score: 0.986705\n",
      "Iteration 9, loss = 0.04488982\n",
      "Validation score: 0.986705\n",
      "Iteration 10, loss = 0.04483064\n",
      "Validation score: 0.986705\n",
      "Iteration 11, loss = 0.04425180\n",
      "Validation score: 0.986705\n",
      "Iteration 12, loss = 0.04435978\n",
      "Validation score: 0.986705\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "37 10436 354\n",
      "Iteration 1, loss = 0.23277754\n",
      "Validation score: 0.987548\n",
      "Iteration 2, loss = 0.07644258\n",
      "Validation score: 0.987548\n",
      "Iteration 3, loss = 0.05669683\n",
      "Validation score: 0.987548\n",
      "Iteration 4, loss = 0.04810346\n",
      "Validation score: 0.987548\n",
      "Iteration 5, loss = 0.04647442\n",
      "Validation score: 0.987548\n",
      "Iteration 6, loss = 0.04474166\n",
      "Validation score: 0.987548\n",
      "Iteration 7, loss = 0.04507841\n",
      "Validation score: 0.987548\n",
      "Iteration 8, loss = 0.04453090\n",
      "Validation score: 0.987548\n",
      "Iteration 9, loss = 0.04337050\n",
      "Validation score: 0.987548\n",
      "Iteration 10, loss = 0.04321681\n",
      "Validation score: 0.987548\n",
      "Iteration 11, loss = 0.04258466\n",
      "Validation score: 0.987548\n",
      "Iteration 12, loss = 0.04208808\n",
      "Validation score: 0.987548\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "38 10621 169\n",
      "Iteration 1, loss = 0.22515605\n",
      "Validation score: 0.988711\n",
      "Iteration 2, loss = 0.07190095\n",
      "Validation score: 0.988711\n",
      "Iteration 3, loss = 0.05272954\n",
      "Validation score: 0.988711\n",
      "Iteration 4, loss = 0.04452344\n",
      "Validation score: 0.988711\n",
      "Iteration 5, loss = 0.04232386\n",
      "Validation score: 0.988711\n",
      "Iteration 6, loss = 0.04199123\n",
      "Validation score: 0.988711\n",
      "Iteration 7, loss = 0.04109605\n",
      "Validation score: 0.988711\n",
      "Iteration 8, loss = 0.04063080\n",
      "Validation score: 0.988711\n",
      "Iteration 9, loss = 0.04067920\n",
      "Validation score: 0.988711\n",
      "Iteration 10, loss = 0.04041122\n",
      "Validation score: 0.988711\n",
      "Iteration 11, loss = 0.03942796\n",
      "Validation score: 0.988711\n",
      "Iteration 12, loss = 0.03909319\n",
      "Validation score: 0.987770\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "39 10463 327\n",
      "Iteration 1, loss = 0.23057139\n",
      "Validation score: 0.986628\n",
      "Iteration 2, loss = 0.07809728\n",
      "Validation score: 0.986628\n",
      "Iteration 3, loss = 0.05887156\n",
      "Validation score: 0.986628\n",
      "Iteration 4, loss = 0.05135763\n",
      "Validation score: 0.986628\n",
      "Iteration 5, loss = 0.04867474\n",
      "Validation score: 0.986628\n",
      "Iteration 6, loss = 0.04733687\n",
      "Validation score: 0.986628\n",
      "Iteration 7, loss = 0.04659632\n",
      "Validation score: 0.986628\n",
      "Iteration 8, loss = 0.04790370\n",
      "Validation score: 0.986628\n",
      "Iteration 9, loss = 0.04658747\n",
      "Validation score: 0.986628\n",
      "Iteration 10, loss = 0.04529242\n",
      "Validation score: 0.986628\n",
      "Iteration 11, loss = 0.04574175\n",
      "Validation score: 0.986628\n",
      "Iteration 12, loss = 0.04601964\n",
      "Validation score: 0.986628\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "40 10524 266\n",
      "Iteration 1, loss = 0.23316528\n",
      "Validation score: 0.986705\n",
      "Iteration 2, loss = 0.07666280\n",
      "Validation score: 0.986705\n",
      "Iteration 3, loss = 0.05813396\n",
      "Validation score: 0.986705\n",
      "Iteration 4, loss = 0.04992261\n",
      "Validation score: 0.986705\n",
      "Iteration 5, loss = 0.04733174\n",
      "Validation score: 0.986705\n",
      "Iteration 6, loss = 0.04655170\n",
      "Validation score: 0.986705\n",
      "Iteration 7, loss = 0.04628204\n",
      "Validation score: 0.986705\n",
      "Iteration 8, loss = 0.04546207\n",
      "Validation score: 0.986705\n",
      "Iteration 9, loss = 0.04463535\n",
      "Validation score: 0.986705\n",
      "Iteration 10, loss = 0.04521873\n",
      "Validation score: 0.986705\n",
      "Iteration 11, loss = 0.04443087\n",
      "Validation score: 0.986705\n",
      "Iteration 12, loss = 0.04409734\n",
      "Validation score: 0.986705\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "41 10584 206\n",
      "Iteration 1, loss = 0.22999280\n",
      "Validation score: 0.986780\n",
      "Iteration 2, loss = 0.07613087\n",
      "Validation score: 0.986780\n",
      "Iteration 3, loss = 0.05716986\n",
      "Validation score: 0.986780\n",
      "Iteration 4, loss = 0.04959125\n",
      "Validation score: 0.986780\n",
      "Iteration 5, loss = 0.04809821\n",
      "Validation score: 0.986780\n",
      "Iteration 6, loss = 0.04727183\n",
      "Validation score: 0.986780\n",
      "Iteration 7, loss = 0.04649694\n",
      "Validation score: 0.986780\n",
      "Iteration 8, loss = 0.04576272\n",
      "Validation score: 0.986780\n",
      "Iteration 9, loss = 0.04596688\n",
      "Validation score: 0.986780\n",
      "Iteration 10, loss = 0.04502269\n",
      "Validation score: 0.985836\n",
      "Iteration 11, loss = 0.04499474\n",
      "Validation score: 0.986780\n",
      "Iteration 12, loss = 0.04382391\n",
      "Validation score: 0.986780\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "42 10632 158\n",
      "Iteration 1, loss = 0.22275973\n",
      "Validation score: 0.986842\n",
      "Iteration 2, loss = 0.07706377\n",
      "Validation score: 0.986842\n",
      "Iteration 3, loss = 0.05596062\n",
      "Validation score: 0.986842\n",
      "Iteration 4, loss = 0.04791520\n",
      "Validation score: 0.986842\n",
      "Iteration 5, loss = 0.04661767\n",
      "Validation score: 0.986842\n",
      "Iteration 6, loss = 0.04534071\n",
      "Validation score: 0.986842\n",
      "Iteration 7, loss = 0.04418867\n",
      "Validation score: 0.986842\n",
      "Iteration 8, loss = 0.04420100\n",
      "Validation score: 0.986842\n",
      "Iteration 9, loss = 0.04407606\n",
      "Validation score: 0.986842\n",
      "Iteration 10, loss = 0.04485185\n",
      "Validation score: 0.985902\n",
      "Iteration 11, loss = 0.04275063\n",
      "Validation score: 0.985902\n",
      "Iteration 12, loss = 0.04264862\n",
      "Validation score: 0.986842\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "43 10520 270\n",
      "Iteration 1, loss = 0.22780608\n",
      "Validation score: 0.987643\n",
      "Iteration 2, loss = 0.07780342\n",
      "Validation score: 0.987643\n",
      "Iteration 3, loss = 0.05820826\n",
      "Validation score: 0.987643\n",
      "Iteration 4, loss = 0.04953106\n",
      "Validation score: 0.987643\n",
      "Iteration 5, loss = 0.04762031\n",
      "Validation score: 0.987643\n",
      "Iteration 6, loss = 0.04869238\n",
      "Validation score: 0.987643\n",
      "Iteration 7, loss = 0.04570247\n",
      "Validation score: 0.987643\n",
      "Iteration 8, loss = 0.04509837\n",
      "Validation score: 0.987643\n",
      "Iteration 9, loss = 0.04446823\n",
      "Validation score: 0.987643\n",
      "Iteration 10, loss = 0.04438366\n",
      "Validation score: 0.987643\n",
      "Iteration 11, loss = 0.04415087\n",
      "Validation score: 0.987643\n",
      "Iteration 12, loss = 0.04389789\n",
      "Validation score: 0.987643\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "44 10728 62\n",
      "Iteration 1, loss = 0.22614627\n",
      "Validation score: 0.986952\n",
      "Iteration 2, loss = 0.07554513\n",
      "Validation score: 0.986952\n",
      "Iteration 3, loss = 0.05669429\n",
      "Validation score: 0.986952\n",
      "Iteration 4, loss = 0.04907980\n",
      "Validation score: 0.986952\n",
      "Iteration 5, loss = 0.04700382\n",
      "Validation score: 0.986952\n",
      "Iteration 6, loss = 0.04580588\n",
      "Validation score: 0.986952\n",
      "Iteration 7, loss = 0.04544679\n",
      "Validation score: 0.986952\n",
      "Iteration 8, loss = 0.04445727\n",
      "Validation score: 0.986952\n",
      "Iteration 9, loss = 0.04525502\n",
      "Validation score: 0.986952\n",
      "Iteration 10, loss = 0.04414076\n",
      "Validation score: 0.986952\n",
      "Iteration 11, loss = 0.04387545\n",
      "Validation score: 0.986952\n",
      "Iteration 12, loss = 0.04350150\n",
      "Validation score: 0.986952\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "45 10699 91\n",
      "Iteration 1, loss = 0.22360377\n",
      "Validation score: 0.987850\n",
      "Iteration 2, loss = 0.07366880\n",
      "Validation score: 0.987850\n",
      "Iteration 3, loss = 0.05447741\n",
      "Validation score: 0.987850\n",
      "Iteration 4, loss = 0.04791406\n",
      "Validation score: 0.987850\n",
      "Iteration 5, loss = 0.04551481\n",
      "Validation score: 0.987850\n",
      "Iteration 6, loss = 0.04475359\n",
      "Validation score: 0.987850\n",
      "Iteration 7, loss = 0.04449650\n",
      "Validation score: 0.986916\n",
      "Iteration 8, loss = 0.04357643\n",
      "Validation score: 0.986916\n",
      "Iteration 9, loss = 0.04366102\n",
      "Validation score: 0.987850\n",
      "Iteration 10, loss = 0.04323414\n",
      "Validation score: 0.987850\n",
      "Iteration 11, loss = 0.04212546\n",
      "Validation score: 0.985981\n",
      "Iteration 12, loss = 0.04209113\n",
      "Validation score: 0.986916\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "46 10763 27\n",
      "Iteration 1, loss = 0.22635695\n",
      "Validation score: 0.987001\n",
      "Iteration 2, loss = 0.07531592\n",
      "Validation score: 0.987001\n",
      "Iteration 3, loss = 0.05568590\n",
      "Validation score: 0.987001\n",
      "Iteration 4, loss = 0.04730363\n",
      "Validation score: 0.987001\n",
      "Iteration 5, loss = 0.04576004\n",
      "Validation score: 0.987001\n",
      "Iteration 6, loss = 0.04366956\n",
      "Validation score: 0.987001\n",
      "Iteration 7, loss = 0.04308286\n",
      "Validation score: 0.987001\n",
      "Iteration 8, loss = 0.04229773\n",
      "Validation score: 0.987001\n",
      "Iteration 9, loss = 0.04314059\n",
      "Validation score: 0.987001\n",
      "Iteration 10, loss = 0.04172612\n",
      "Validation score: 0.986072\n",
      "Iteration 11, loss = 0.04065716\n",
      "Validation score: 0.987001\n",
      "Iteration 12, loss = 0.04098351\n",
      "Validation score: 0.986072\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "47 10580 210\n",
      "Iteration 1, loss = 0.22532377\n",
      "Validation score: 0.987713\n",
      "Iteration 2, loss = 0.07552833\n",
      "Validation score: 0.987713\n",
      "Iteration 3, loss = 0.05665355\n",
      "Validation score: 0.987713\n",
      "Iteration 4, loss = 0.04914830\n",
      "Validation score: 0.987713\n",
      "Iteration 5, loss = 0.04613802\n",
      "Validation score: 0.987713\n",
      "Iteration 6, loss = 0.04519514\n",
      "Validation score: 0.987713\n",
      "Iteration 7, loss = 0.04413178\n",
      "Validation score: 0.987713\n",
      "Iteration 8, loss = 0.04359209\n",
      "Validation score: 0.987713\n",
      "Iteration 9, loss = 0.04274136\n",
      "Validation score: 0.987713\n",
      "Iteration 10, loss = 0.04286026\n",
      "Validation score: 0.987713\n",
      "Iteration 11, loss = 0.04252225\n",
      "Validation score: 0.987713\n",
      "Iteration 12, loss = 0.04185711\n",
      "Validation score: 0.987713\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "48 10672 118\n",
      "Iteration 1, loss = 0.23139363\n",
      "Validation score: 0.986891\n",
      "Iteration 2, loss = 0.07405808\n",
      "Validation score: 0.986891\n",
      "Iteration 3, loss = 0.05623900\n",
      "Validation score: 0.986891\n",
      "Iteration 4, loss = 0.04877460\n",
      "Validation score: 0.986891\n",
      "Iteration 5, loss = 0.05072293\n",
      "Validation score: 0.986891\n",
      "Iteration 6, loss = 0.04604981\n",
      "Validation score: 0.986891\n",
      "Iteration 7, loss = 0.04629024\n",
      "Validation score: 0.986891\n",
      "Iteration 8, loss = 0.04516590\n",
      "Validation score: 0.986891\n",
      "Iteration 9, loss = 0.04548985\n",
      "Validation score: 0.986891\n",
      "Iteration 10, loss = 0.04384139\n",
      "Validation score: 0.986891\n",
      "Iteration 11, loss = 0.04346701\n",
      "Validation score: 0.986891\n",
      "Iteration 12, loss = 0.04343258\n",
      "Validation score: 0.985955\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "49 10528 262\n",
      "Iteration 1, loss = 0.23043005\n",
      "Validation score: 0.987654\n",
      "Iteration 2, loss = 0.07513933\n",
      "Validation score: 0.987654\n",
      "Iteration 3, loss = 0.05641772\n",
      "Validation score: 0.987654\n",
      "Iteration 4, loss = 0.04868201\n",
      "Validation score: 0.987654\n",
      "Iteration 5, loss = 0.04601619\n",
      "Validation score: 0.987654\n",
      "Iteration 6, loss = 0.04571449\n",
      "Validation score: 0.987654\n",
      "Iteration 7, loss = 0.04421288\n",
      "Validation score: 0.987654\n",
      "Iteration 8, loss = 0.04407995\n",
      "Validation score: 0.987654\n",
      "Iteration 9, loss = 0.04373909\n",
      "Validation score: 0.987654\n",
      "Iteration 10, loss = 0.04329389\n",
      "Validation score: 0.987654\n",
      "Iteration 11, loss = 0.04418836\n",
      "Validation score: 0.987654\n",
      "Iteration 12, loss = 0.04235891\n",
      "Validation score: 0.987654\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "50 10627 163\n",
      "Iteration 1, loss = 0.22621014\n",
      "Validation score: 0.987770\n",
      "Iteration 2, loss = 0.07557292\n",
      "Validation score: 0.987770\n",
      "Iteration 3, loss = 0.05427200\n",
      "Validation score: 0.987770\n",
      "Iteration 4, loss = 0.04745902\n",
      "Validation score: 0.987770\n",
      "Iteration 5, loss = 0.04558340\n",
      "Validation score: 0.987770\n",
      "Iteration 6, loss = 0.04437592\n",
      "Validation score: 0.987770\n",
      "Iteration 7, loss = 0.04493370\n",
      "Validation score: 0.987770\n",
      "Iteration 8, loss = 0.04376559\n",
      "Validation score: 0.987770\n",
      "Iteration 9, loss = 0.04326249\n",
      "Validation score: 0.987770\n",
      "Iteration 10, loss = 0.04309072\n",
      "Validation score: 0.987770\n",
      "Iteration 11, loss = 0.04253845\n",
      "Validation score: 0.987770\n",
      "Iteration 12, loss = 0.04235874\n",
      "Validation score: 0.987770\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "51 10541 249\n",
      "Iteration 1, loss = 0.22602361\n",
      "Validation score: 0.987678\n",
      "Iteration 2, loss = 0.07331011\n",
      "Validation score: 0.987678\n",
      "Iteration 3, loss = 0.05389536\n",
      "Validation score: 0.987678\n",
      "Iteration 4, loss = 0.04686705\n",
      "Validation score: 0.987678\n",
      "Iteration 5, loss = 0.04466757\n",
      "Validation score: 0.987678\n",
      "Iteration 6, loss = 0.04374851\n",
      "Validation score: 0.987678\n",
      "Iteration 7, loss = 0.04327236\n",
      "Validation score: 0.987678\n",
      "Iteration 8, loss = 0.04288897\n",
      "Validation score: 0.987678\n",
      "Iteration 9, loss = 0.04212172\n",
      "Validation score: 0.987678\n",
      "Iteration 10, loss = 0.04246771\n",
      "Validation score: 0.987678\n",
      "Iteration 11, loss = 0.04154687\n",
      "Validation score: 0.987678\n",
      "Iteration 12, loss = 0.04136728\n",
      "Validation score: 0.987678\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "52 10502 288\n",
      "Iteration 1, loss = 0.22466551\n",
      "Validation score: 0.987631\n",
      "Iteration 2, loss = 0.07635845\n",
      "Validation score: 0.987631\n",
      "Iteration 3, loss = 0.05655209\n",
      "Validation score: 0.987631\n",
      "Iteration 4, loss = 0.04921354\n",
      "Validation score: 0.987631\n",
      "Iteration 5, loss = 0.04738717\n",
      "Validation score: 0.987631\n",
      "Iteration 6, loss = 0.04503315\n",
      "Validation score: 0.987631\n",
      "Iteration 7, loss = 0.04476074\n",
      "Validation score: 0.987631\n",
      "Iteration 8, loss = 0.04392540\n",
      "Validation score: 0.987631\n",
      "Iteration 9, loss = 0.04350958\n",
      "Validation score: 0.987631\n",
      "Iteration 10, loss = 0.04412042\n",
      "Validation score: 0.987631\n",
      "Iteration 11, loss = 0.04301106\n",
      "Validation score: 0.987631\n",
      "Iteration 12, loss = 0.04247400\n",
      "Validation score: 0.987631\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "53 10575 215\n",
      "Iteration 1, loss = 0.23098438\n",
      "Validation score: 0.987713\n",
      "Iteration 2, loss = 0.07580356\n",
      "Validation score: 0.987713\n",
      "Iteration 3, loss = 0.05719307\n",
      "Validation score: 0.987713\n",
      "Iteration 4, loss = 0.04904629\n",
      "Validation score: 0.987713\n",
      "Iteration 5, loss = 0.04688102\n",
      "Validation score: 0.987713\n",
      "Iteration 6, loss = 0.04547715\n",
      "Validation score: 0.987713\n",
      "Iteration 7, loss = 0.04701842\n",
      "Validation score: 0.987713\n",
      "Iteration 8, loss = 0.04440087\n",
      "Validation score: 0.987713\n",
      "Iteration 9, loss = 0.04440940\n",
      "Validation score: 0.987713\n",
      "Iteration 10, loss = 0.04442911\n",
      "Validation score: 0.987713\n",
      "Iteration 11, loss = 0.04296072\n",
      "Validation score: 0.987713\n",
      "Iteration 12, loss = 0.04271796\n",
      "Validation score: 0.987713\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "54 10532 258\n",
      "Iteration 1, loss = 0.22436651\n",
      "Validation score: 0.986717\n",
      "Iteration 2, loss = 0.07621532\n",
      "Validation score: 0.986717\n",
      "Iteration 3, loss = 0.05552007\n",
      "Validation score: 0.986717\n",
      "Iteration 4, loss = 0.04820371\n",
      "Validation score: 0.986717\n",
      "Iteration 5, loss = 0.04624802\n",
      "Validation score: 0.986717\n",
      "Iteration 6, loss = 0.04640225\n",
      "Validation score: 0.986717\n",
      "Iteration 7, loss = 0.04558389\n",
      "Validation score: 0.986717\n",
      "Iteration 8, loss = 0.04579690\n",
      "Validation score: 0.986717\n",
      "Iteration 9, loss = 0.04488475\n",
      "Validation score: 0.986717\n",
      "Iteration 10, loss = 0.04474166\n",
      "Validation score: 0.986717\n",
      "Iteration 11, loss = 0.04401716\n",
      "Validation score: 0.986717\n",
      "Iteration 12, loss = 0.04365114\n",
      "Validation score: 0.985769\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "55 10775 15\n",
      "Iteration 1, loss = 0.22605499\n",
      "Validation score: 0.987013\n",
      "Iteration 2, loss = 0.07422459\n",
      "Validation score: 0.987013\n",
      "Iteration 3, loss = 0.05414112\n",
      "Validation score: 0.987013\n",
      "Iteration 4, loss = 0.04597523\n",
      "Validation score: 0.987013\n",
      "Iteration 5, loss = 0.04503089\n",
      "Validation score: 0.987013\n",
      "Iteration 6, loss = 0.04319642\n",
      "Validation score: 0.987013\n",
      "Iteration 7, loss = 0.04276550\n",
      "Validation score: 0.987013\n",
      "Iteration 8, loss = 0.04179368\n",
      "Validation score: 0.987013\n",
      "Iteration 9, loss = 0.04160509\n",
      "Validation score: 0.987013\n",
      "Iteration 10, loss = 0.04113506\n",
      "Validation score: 0.987013\n",
      "Iteration 11, loss = 0.04134562\n",
      "Validation score: 0.987013\n",
      "Iteration 12, loss = 0.04089192\n",
      "Validation score: 0.985158\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "AUC-ROC:    0.3258\n",
      "AUC-PR:     0.0568\n",
      "Accuracy:   0.9872\n",
      "Precision:  0.3636\n",
      "Recall:     0.0296\n",
      "F1 Score:   0.0548\n",
      "Sensitivity (Recall): 0.0296\n",
      "Specificity:          0.9993\n",
      "[[10648     7]\n",
      " [  131     4]]\n"
     ]
    }
   ],
   "source": [
    "# MLP\n",
    "\n",
    "Y_TRUES = np.empty([0])\n",
    "Y_PROBS = np.empty([0])\n",
    "Y_PREDS = np.empty([0])\n",
    "\n",
    "\n",
    "cv = LeavePGroupsOut(n_groups=1)\n",
    "for train_idx, test_idx in cv.split(x, y, groups=p):\n",
    "    participant = np.unique(p[test_idx])[0]\n",
    "\n",
    "\n",
    "# cv = KFold(n_splits=x.shape[0], shuffle=True, random_state=seed)\n",
    "# for fold, (train_idx, test_idx) in enumerate(cv.split(x), start=1):\n",
    "#     participant = np.unique(p[test_idx])[0]\n",
    "\n",
    "# cv = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "# for fold, (train_idx, test_idx) in enumerate(cv.split(x), start=1):\n",
    "#     participant = fold\n",
    "\n",
    "    x_train, x_test = x[train_idx], x[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    print(participant, x_train.shape[0], x_test.shape[0])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    normalizer = MinMaxScaler()\n",
    "    normalizer.fit(x_train)\n",
    "    x_train = normalizer.transform(x_train)\n",
    "    x_test = normalizer.transform(x_test)\n",
    "\n",
    "    # Oversample class 1\n",
    "\n",
    "    # ros = RandomOverSampler(sampling_strategy=.1, random_state=seed)\n",
    "    # x_train, y_train = ros.fit_resample(x_train, y_train)\n",
    "\n",
    "    # # 1. Check original class distribution\n",
    "    # print(\"Original class distribution:\", Counter(y_train))\n",
    "    # # 2. Define the minority class (adjust if needed)\n",
    "    # minority_class = 1  # change this if your minority class label is different\n",
    "    # current_minority_count = sum(y_train == minority_class)\n",
    "    # # 3. Define desired new total count for the minority class (10x)\n",
    "    # target_minority_count = current_minority_count * 10\n",
    "    # # 4. Setup SMOTE with custom sampling strategy\n",
    "    # smote = SMOTE(sampling_strategy={minority_class: target_minority_count}, random_state=seed)\n",
    "    # # 5. Fit and resample\n",
    "    # x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "    # # 6. Confirm new distribution\n",
    "    # print(\"Resampled class distribution:\", Counter(y_train))\n",
    "\n",
    "\n",
    "    # ----- LightGBM\n",
    "    # train_data = lgb.Dataset(x_train, label=y_train)\n",
    "    # test_data = lgb.Dataset(x_test, label=y_test, reference=train_data)\n",
    "    # params = {\n",
    "    #     'verbose': -1,  # 👈 turn off training output\n",
    "    #     'objective': 'binary',\n",
    "    #     'metric': 'binary_logloss',  # or 'auc' if you prefer\n",
    "    #     'num_leaves': 64,\n",
    "    #     'learning_rate': 0.01,\n",
    "    #     'n_estimators': 100,\n",
    "    #     # 'is_unbalance': True  # Automatically balances positive and negative classes\n",
    "    #     # 'scale_pos_weight': (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    # }\n",
    "    # bst = lgb.train(params, train_data, valid_sets=[train_data, test_data])\n",
    "    # y_probs = bst.predict(x_test, num_iteration=bst.best_iteration)\n",
    "\n",
    "\n",
    "    # ----- Transformer\n",
    "    # model = TabPFNClassifier(\n",
    "    #     device='cuda',\n",
    "    #     random_state=seed,\n",
    "    #     n_estimators = 4,\n",
    "    #     ignore_pretraining_limits=True)\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- Gradient Boosting Classifier\n",
    "    # model = GradientBoostingClassifier(\n",
    "    #     n_estimators=100,  # match LightGBM\n",
    "    #     learning_rate=0.001,  # match LightGBM\n",
    "    #     max_depth=4,  # similar to LightGBM default tree depth\n",
    "    #     subsample=1.0,  # default\n",
    "    #     random_state=seed\n",
    "    # )\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "    \n",
    "\n",
    "    # ----- XGBoost\n",
    "    # model = XGBClassifier()\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- Random Forest\n",
    "    # model = RandomForestClassifier(\n",
    "    #     n_estimators=250,  # number of trees\n",
    "    #     max_depth=None,  # let the trees grow fully\n",
    "    #     random_state=seed\n",
    "    # )\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- SVM\n",
    "    # model = SVC(kernel='rbf', probability=True, random_state=seed)\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- DT\n",
    "    # model = DecisionTreeClassifier(criterion=\"log_loss\", max_depth=512, random_state=seed)\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict(x_test)\n",
    "\n",
    "\n",
    "    # MLP\n",
    "    model = MLPClassifier(\n",
    "        hidden_layer_sizes=(128, 64),    # 2 hidden layers: 64 and 32 neurons\n",
    "        activation='relu',              # good default: 'relu'\n",
    "        solver='adam',                  # optimizer\n",
    "        alpha=0.0001,                   # L2 regularization\n",
    "        learning_rate_init=0.001,\n",
    "        max_iter=500,\n",
    "        early_stopping=True,\n",
    "        random_state=seed,\n",
    "        verbose=True,\n",
    "    )\n",
    "    model.fit(x_train, y_train)    \n",
    "    y_probs = model.predict_proba(x_test)[:, 1]\n",
    "    y_preds = model.predict(x_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Y_TRUES = np.append(Y_TRUES, y_test)\n",
    "    Y_PROBS = np.append(Y_PROBS, y_probs)\n",
    "    Y_PREDS = np.append(Y_PREDS, y_preds)\n",
    "\n",
    "\n",
    "indx = Y_TRUES.argsort()\n",
    "Y_TRUES = Y_TRUES[indx]\n",
    "Y_PROBS = Y_PROBS[indx]\n",
    "Y_PREDS = Y_PREDS[indx]\n",
    "\n",
    "\n",
    "# Evaluation metrics\n",
    "auc_roc = roc_auc_score(Y_TRUES, Y_PROBS)\n",
    "precision, recall, _ = precision_recall_curve(Y_TRUES, Y_PROBS)\n",
    "auc_pr = auc(recall, precision)\n",
    "acc = accuracy_score(Y_TRUES, Y_PREDS)\n",
    "pre = precision_score(Y_TRUES, Y_PREDS)\n",
    "rec = recall_score(Y_TRUES, Y_PREDS)\n",
    "f1 = f1_score(Y_TRUES, Y_PREDS)\n",
    "\n",
    "# Confusion matrix: [ [TN, FP], [FN, TP] ]\n",
    "tn, fp, fn, tp = confusion_matrix(Y_TRUES, Y_PREDS).ravel()\n",
    "\n",
    "# Sensitivity = Recall = TP / (TP + FN)\n",
    "sensitivity = tp / (tp + fn)\n",
    "\n",
    "# Specificity = TN / (TN + FP)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# Print results\n",
    "print(f\"AUC-ROC:    {auc_roc:.4f}\")\n",
    "print(f\"AUC-PR:     {auc_pr:.4f}\")\n",
    "print(f\"Accuracy:   {acc:.4f}\")\n",
    "print(f\"Precision:  {pre:.4f}\")\n",
    "print(f\"Recall:     {rec:.4f}\")\n",
    "print(f\"F1 Score:   {f1:.4f}\")\n",
    "print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "print(f\"Specificity:          {specificity:.4f}\")\n",
    "print(confusion_matrix(Y_TRUES, Y_PREDS))\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(Y_TRUES, 'o', color='blue', alpha=.25, markersize=8, label='Ground-truth')\n",
    "# plt.plot(Y_PREDS, 'o', color='red', alpha=.25, markersize=8, label='Prediction')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 8632 2158\n",
      "2 8632 2158\n",
      "3 8632 2158\n",
      "4 8632 2158\n",
      "5 8632 2158\n",
      "AUC-ROC:    0.9154\n",
      "AUC-PR:     0.2614\n",
      "Accuracy:   0.8911\n",
      "Precision:  0.0860\n",
      "Recall:     0.8000\n",
      "F1 Score:   0.1553\n",
      "Sensitivity (Recall): 0.8000\n",
      "Specificity:          0.8923\n",
      "[[9507 1148]\n",
      " [  27  108]]\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Y_TRUES = np.empty([0])\n",
    "Y_PROBS = np.empty([0])\n",
    "Y_PREDS = np.empty([0])\n",
    "\n",
    "\n",
    "# cv = LeavePGroupsOut(n_groups=1)\n",
    "# for train_idx, test_idx in cv.split(x, y, groups=p):\n",
    "#     participant = np.unique(p[test_idx])[0]\n",
    "\n",
    "\n",
    "# cv = KFold(n_splits=x.shape[0], shuffle=True, random_state=seed)\n",
    "# for fold, (train_idx, test_idx) in enumerate(cv.split(x), start=1):\n",
    "#     participant = np.unique(p[test_idx])[0]\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "for fold, (train_idx, test_idx) in enumerate(cv.split(x), start=1):\n",
    "    participant = fold\n",
    "\n",
    "    x_train, x_test = x[train_idx], x[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    print(participant, x_train.shape[0], x_test.shape[0])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    normalizer = MinMaxScaler()\n",
    "    normalizer.fit(x_train)\n",
    "    x_train = normalizer.transform(x_train)\n",
    "    x_test = normalizer.transform(x_test)\n",
    "\n",
    "    # Oversample class 1\n",
    "\n",
    "    # ros = RandomOverSampler(sampling_strategy=.1, random_state=seed)\n",
    "    # x_train, y_train = ros.fit_resample(x_train, y_train)\n",
    "\n",
    "    # 1. Check original class distribution\n",
    "    # print(\"Original class distribution:\", Counter(y_train))\n",
    "    # # 2. Define the minority class (adjust if needed)\n",
    "    # minority_class = 1  # change this if your minority class label is different\n",
    "    # current_minority_count = sum(y_train == minority_class)\n",
    "    # # 3. Define desired new total count for the minority class (10x)\n",
    "    # target_minority_count = current_minority_count * 10\n",
    "    # # 4. Setup SMOTE with custom sampling strategy\n",
    "    # smote = SMOTE(sampling_strategy={minority_class: target_minority_count}, random_state=seed)\n",
    "    # # 5. Fit and resample\n",
    "    # x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "    # # 6. Confirm new distribution\n",
    "    # print(\"Resampled class distribution:\", Counter(y_train))\n",
    "\n",
    "\n",
    "    # Undersample class 0\n",
    "    # reduction_percent = 99\n",
    "    # print(\"Original class distribution:\", Counter(y_train))    \n",
    "    # class_0_idx = np.where(y_train == 0)[0]\n",
    "    # class_1_idx = np.where(y_train == 1)[0]\n",
    "    # # How many class 0 samples to keep\n",
    "    # n_keep = int(len(class_0_idx) * (1 - reduction_percent / 100.0))\n",
    "    # sampled_0_idx = np.random.choice(class_0_idx, size=n_keep, replace=False)\n",
    "    # # Combine sampled class 0 with all class 1\n",
    "    # final_idx = np.concatenate([sampled_0_idx, class_1_idx])\n",
    "    # x_train = x_train[final_idx]\n",
    "    # y_train =  y_train[final_idx]\n",
    "    # print(\"Resampled class distribution:\", Counter(y_train))\n",
    "\n",
    "\n",
    "    # ----- LightGBM\n",
    "    # train_data = lgb.Dataset(x_train, label=y_train)\n",
    "    # test_data = lgb.Dataset(x_test, label=y_test, reference=train_data)\n",
    "    # params = {\n",
    "    #     'verbose': -1,  # 👈 turn off training output\n",
    "    #     'objective': 'binary',\n",
    "    #     'metric': 'binary_logloss',  # or 'auc' if you prefer\n",
    "    #     'num_leaves': 64,\n",
    "    #     'learning_rate': 0.01,\n",
    "    #     'n_estimators': 100,\n",
    "    #     # 'is_unbalance': True  # Automatically balances positive and negative classes\n",
    "    #     # 'scale_pos_weight': (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    # }\n",
    "    # bst = lgb.train(params, train_data, valid_sets=[train_data, test_data])\n",
    "    # y_probs = bst.predict(x_test, num_iteration=bst.best_iteration)\n",
    "\n",
    "\n",
    "    # ----- Transformer\n",
    "    # model = TabPFNClassifier(\n",
    "    #     device='cuda',\n",
    "    #     random_state=seed,\n",
    "    #     n_estimators = 4,\n",
    "    #     ignore_pretraining_limits=True)\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- Gradient Boosting Classifier\n",
    "    # model = GradientBoostingClassifier(\n",
    "    #     n_estimators=100,  # match LightGBM\n",
    "    #     learning_rate=0.001,  # match LightGBM\n",
    "    #     max_depth=4,  # similar to LightGBM default tree depth\n",
    "    #     subsample=1.0,  # default\n",
    "    #     random_state=seed\n",
    "    # )\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "    \n",
    "\n",
    "    # ----- XGBoost\n",
    "    # model = XGBClassifier()\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- Random Forest\n",
    "    # model = RandomForestClassifier(\n",
    "    #     n_estimators=250,  # number of trees\n",
    "    #     max_depth=None,  # let the trees grow fully\n",
    "    #     random_state=seed\n",
    "    # )\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- SVM\n",
    "    # model = SVC(kernel='rbf', probability=True, random_state=seed)\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- DT\n",
    "    # model = DecisionTreeClassifier(criterion=\"log_loss\", max_depth=512, random_state=seed)\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict(x_test)\n",
    "\n",
    "\n",
    "    # MLP\n",
    "    # model = MLPClassifier(\n",
    "    #     hidden_layer_sizes=(128, 64),    # 2 hidden layers: 64 and 32 neurons\n",
    "    #     activation='relu',              # good default: 'relu'\n",
    "    #     solver='adam',                  # optimizer\n",
    "    #     alpha=0.0001,                   # L2 regularization\n",
    "    #     learning_rate_init=0.001,\n",
    "    #     max_iter=500,\n",
    "    #     early_stopping=True,\n",
    "    #     random_state=seed,\n",
    "    #     verbose=True,\n",
    "    # )\n",
    "    # model.fit(x_train, y_train)    \n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "    # y_preds = model.predict(x_test)\n",
    "\n",
    "\n",
    "    # LR\n",
    "    model = LogisticRegression(\n",
    "    penalty='l2',               # regularization (default)\n",
    "    C=1.0,                      # inverse of regularization strength\n",
    "    solver='lbfgs',             # optimizer (good default for small/medium data)\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced',\n",
    "    random_state=seed)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_probs = model.predict_proba(x_test)[:, 1]\n",
    "    y_preds = model.predict(x_test)\n",
    "\n",
    "\n",
    "\n",
    "    Y_TRUES = np.append(Y_TRUES, y_test)\n",
    "    Y_PROBS = np.append(Y_PROBS, y_probs)\n",
    "    Y_PREDS = np.append(Y_PREDS, y_preds)\n",
    "\n",
    "\n",
    "indx = Y_TRUES.argsort()\n",
    "Y_TRUES = Y_TRUES[indx]\n",
    "Y_PROBS = Y_PROBS[indx]\n",
    "Y_PREDS = Y_PREDS[indx]\n",
    "\n",
    "\n",
    "# Evaluation metrics\n",
    "auc_roc = roc_auc_score(Y_TRUES, Y_PROBS)\n",
    "precision, recall, _ = precision_recall_curve(Y_TRUES, Y_PROBS)\n",
    "auc_pr = auc(recall, precision)\n",
    "acc = accuracy_score(Y_TRUES, Y_PREDS)\n",
    "pre = precision_score(Y_TRUES, Y_PREDS)\n",
    "rec = recall_score(Y_TRUES, Y_PREDS)\n",
    "f1 = f1_score(Y_TRUES, Y_PREDS)\n",
    "\n",
    "# Confusion matrix: [ [TN, FP], [FN, TP] ]\n",
    "tn, fp, fn, tp = confusion_matrix(Y_TRUES, Y_PREDS).ravel()\n",
    "\n",
    "# Sensitivity = Recall = TP / (TP + FN)\n",
    "sensitivity = tp / (tp + fn)\n",
    "\n",
    "# Specificity = TN / (TN + FP)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# Print results\n",
    "print(f\"AUC-ROC:    {auc_roc:.4f}\")\n",
    "print(f\"AUC-PR:     {auc_pr:.4f}\")\n",
    "print(f\"Accuracy:   {acc:.4f}\")\n",
    "print(f\"Precision:  {pre:.4f}\")\n",
    "print(f\"Recall:     {rec:.4f}\")\n",
    "print(f\"F1 Score:   {f1:.4f}\")\n",
    "print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "print(f\"Specificity:          {specificity:.4f}\")\n",
    "print(confusion_matrix(Y_TRUES, Y_PREDS))\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(Y_TRUES, 'o', color='blue', alpha=.25, markersize=8, label='Ground-truth')\n",
    "# plt.plot(Y_PREDS, 'o', color='red', alpha=.25, markersize=8, label='Prediction')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 8632 2158\n",
      "2 2 8632 2158\n",
      "3 3 8632 2158\n",
      "4 4 8632 2158\n",
      "5 5 8632 2158\n",
      "AUC-ROC:    0.8988\n",
      "AUC-PR:     0.4455\n",
      "Accuracy:   0.7080\n",
      "Precision:  0.0374\n",
      "Recall:     0.9037\n",
      "F1 Score:   0.0719\n",
      "Sensitivity (Recall): 0.9037\n",
      "Specificity:          0.7055\n",
      "[[7517 3138]\n",
      " [  13  122]]\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "\n",
    "Y_TRUES = np.empty([0])\n",
    "Y_PROBS = np.empty([0])\n",
    "Y_PREDS = np.empty([0])\n",
    "\n",
    "\n",
    "# cv = LeavePGroupsOut(n_groups=1)\n",
    "# for train_idx, test_idx in cv.split(x, y, groups=p):\n",
    "#     participant = np.unique(p[test_idx])[0]\n",
    "\n",
    "\n",
    "# cv = KFold(n_splits=x.shape[0], shuffle=True, random_state=seed)\n",
    "# for fold, (train_idx, test_idx) in enumerate(cv.split(x), start=1):\n",
    "#     participant = np.unique(p[test_idx])[0]\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "for fold, (train_idx, test_idx) in enumerate(cv.split(x), start=1):\n",
    "    participant = fold\n",
    "\n",
    "    x_train, x_test = x[train_idx], x[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    print(fold, participant, x_train.shape[0], x_test.shape[0])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    normalizer = MinMaxScaler()\n",
    "    normalizer.fit(x_train)\n",
    "    x_train = normalizer.transform(x_train)\n",
    "    x_test = normalizer.transform(x_test)\n",
    "\n",
    "    # Oversample class 1\n",
    "\n",
    "    # ros = RandomOverSampler(sampling_strategy=.1, random_state=seed)\n",
    "    # x_train, y_train = ros.fit_resample(x_train, y_train)\n",
    "\n",
    "    # # 1. Check original class distribution\n",
    "    # print(\"Original class distribution:\", Counter(y_train))\n",
    "    # # 2. Define the minority class (adjust if needed)\n",
    "    # minority_class = 1  # change this if your minority class label is different\n",
    "    # current_minority_count = sum(y_train == minority_class)\n",
    "    # # 3. Define desired new total count for the minority class (10x)\n",
    "    # target_minority_count = current_minority_count * 10\n",
    "    # # 4. Setup SMOTE with custom sampling strategy\n",
    "    # smote = SMOTE(sampling_strategy={minority_class: target_minority_count}, random_state=seed)\n",
    "    # # 5. Fit and resample\n",
    "    # x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "    # # 6. Confirm new distribution\n",
    "    # print(\"Resampled class distribution:\", Counter(y_train))\n",
    "\n",
    "\n",
    "    # Undersample class 0\n",
    "    # reduction_percent = 99\n",
    "    # print(\"Original class distribution:\", Counter(y_train))    \n",
    "    # class_0_idx = np.where(y_train == 0)[0]\n",
    "    # class_1_idx = np.where(y_train == 1)[0]\n",
    "    # # How many class 0 samples to keep\n",
    "    # n_keep = int(len(class_0_idx) * (1 - reduction_percent / 100.0))\n",
    "    # sampled_0_idx = np.random.choice(class_0_idx, size=n_keep, replace=False)\n",
    "    # # Combine sampled class 0 with all class 1\n",
    "    # final_idx = np.concatenate([sampled_0_idx, class_1_idx])\n",
    "    # x_train = x_train[final_idx]\n",
    "    # y_train =  y_train[final_idx]\n",
    "    # print(\"Resampled class distribution:\", Counter(y_train))\n",
    "\n",
    "\n",
    "    # ----- LightGBM\n",
    "    # train_data = lgb.Dataset(x_train, label=y_train)\n",
    "    # test_data = lgb.Dataset(x_test, label=y_test, reference=train_data)\n",
    "    # params = {\n",
    "    #     'verbose': -1,  # 👈 turn off training output\n",
    "    #     'objective': 'binary',\n",
    "    #     'metric': 'binary_logloss',  # or 'auc' if you prefer\n",
    "    #     'num_leaves': 64,\n",
    "    #     'learning_rate': 0.01,\n",
    "    #     'n_estimators': 100,\n",
    "    #     # 'is_unbalance': True  # Automatically balances positive and negative classes\n",
    "    #     # 'scale_pos_weight': (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    # }\n",
    "    # bst = lgb.train(params, train_data, valid_sets=[train_data, test_data])\n",
    "    # y_probs = bst.predict(x_test, num_iteration=bst.best_iteration)\n",
    "\n",
    "\n",
    "    # ----- Transformer\n",
    "    # model = TabPFNClassifier(\n",
    "    #     device='cuda',\n",
    "    #     random_state=seed,\n",
    "    #     n_estimators = 4,\n",
    "    #     ignore_pretraining_limits=True)\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- Gradient Boosting Classifier\n",
    "    # model = GradientBoostingClassifier(\n",
    "    #     n_estimators=100,  # match LightGBM\n",
    "    #     learning_rate=0.001,  # match LightGBM\n",
    "    #     max_depth=4,  # similar to LightGBM default tree depth\n",
    "    #     subsample=1.0,  # default\n",
    "    #     random_state=seed\n",
    "    # )\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "    \n",
    "\n",
    "    # ----- XGBoost\n",
    "    # model = XGBClassifier()\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- Random Forest\n",
    "    # model = RandomForestClassifier(\n",
    "    #     n_estimators=250,  # number of trees\n",
    "    #     max_depth=None,  # let the trees grow fully\n",
    "    #     random_state=seed\n",
    "    # )\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- SVM\n",
    "    # model = SVC(kernel='rbf', probability=True, random_state=seed)\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- DT\n",
    "    # model = DecisionTreeClassifier(criterion=\"log_loss\", max_depth=512, random_state=seed)\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict(x_test)\n",
    "\n",
    "\n",
    "    # MLP\n",
    "    # model = MLPClassifier(\n",
    "    #     hidden_layer_sizes=(128, 64),    # 2 hidden layers: 64 and 32 neurons\n",
    "    #     activation='relu',              # good default: 'relu'\n",
    "    #     solver='adam',                  # optimizer\n",
    "    #     alpha=0.0001,                   # L2 regularization\n",
    "    #     learning_rate_init=0.001,\n",
    "    #     max_iter=500,\n",
    "    #     early_stopping=True,\n",
    "    #     random_state=seed,\n",
    "    #     verbose=True,\n",
    "    # )\n",
    "    # model.fit(x_train, y_train)    \n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "    # y_preds = model.predict(x_test)\n",
    "\n",
    "\n",
    "    # LR\n",
    "    # model = LogisticRegression(\n",
    "    # penalty='l2',               # regularization (default)\n",
    "    # C=1.0,                      # inverse of regularization strength\n",
    "    # solver='lbfgs',             # optimizer (good default for small/medium data)\n",
    "    # max_iter=1000,\n",
    "    # class_weight='balanced',\n",
    "    # random_state=seed)\n",
    "    # model.fit(x_train, y_train)    \n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "    # y_preds = model.predict(x_test)\n",
    "\n",
    "\n",
    "    # NB\n",
    "    model = GaussianNB()\n",
    "    model.fit(x_train,\n",
    "              y_train,\n",
    "              sample_weight=compute_sample_weight(class_weight='balanced', y=y_train),\n",
    "              )\n",
    "    y_probs = model.predict_proba(x_test)[:, 1]\n",
    "    y_preds = model.predict(x_test)\n",
    "\n",
    "\n",
    "\n",
    "    Y_TRUES = np.append(Y_TRUES, y_test)\n",
    "    Y_PROBS = np.append(Y_PROBS, y_probs)\n",
    "    Y_PREDS = np.append(Y_PREDS, y_preds)\n",
    "\n",
    "\n",
    "indx = Y_TRUES.argsort()\n",
    "Y_TRUES = Y_TRUES[indx]\n",
    "Y_PROBS = Y_PROBS[indx]\n",
    "Y_PREDS = Y_PREDS[indx]\n",
    "\n",
    "\n",
    "# Evaluation metrics\n",
    "auc_roc = roc_auc_score(Y_TRUES, Y_PROBS)\n",
    "precision, recall, _ = precision_recall_curve(Y_TRUES, Y_PROBS)\n",
    "auc_pr = auc(recall, precision)\n",
    "acc = accuracy_score(Y_TRUES, Y_PREDS)\n",
    "pre = precision_score(Y_TRUES, Y_PREDS)\n",
    "rec = recall_score(Y_TRUES, Y_PREDS)\n",
    "f1 = f1_score(Y_TRUES, Y_PREDS)\n",
    "\n",
    "# Confusion matrix: [ [TN, FP], [FN, TP] ]\n",
    "tn, fp, fn, tp = confusion_matrix(Y_TRUES, Y_PREDS).ravel()\n",
    "\n",
    "# Sensitivity = Recall = TP / (TP + FN)\n",
    "sensitivity = tp / (tp + fn)\n",
    "\n",
    "# Specificity = TN / (TN + FP)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# Print results\n",
    "print(f\"AUC-ROC:    {auc_roc:.4f}\")\n",
    "print(f\"AUC-PR:     {auc_pr:.4f}\")\n",
    "print(f\"Accuracy:   {acc:.4f}\")\n",
    "print(f\"Precision:  {pre:.4f}\")\n",
    "print(f\"Recall:     {rec:.4f}\")\n",
    "print(f\"F1 Score:   {f1:.4f}\")\n",
    "print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "print(f\"Specificity:          {specificity:.4f}\")\n",
    "print(confusion_matrix(Y_TRUES, Y_PREDS))\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(Y_TRUES, 'o', color='blue', alpha=.25, markersize=8, label='Ground-truth')\n",
    "# plt.plot(Y_PREDS, 'o', color='red', alpha=.25, markersize=8, label='Prediction')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ali-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
