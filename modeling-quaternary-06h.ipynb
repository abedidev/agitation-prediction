{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3080 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, confusion_matrix, f1_score, accuracy_score, \\\n",
    "    precision_score, recall_score, average_precision_score\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import KFold, LeavePGroupsOut\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tabpfn import TabPFNClassifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "import shap\n",
    "# from catboost import CatBoostClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "# matplotlib.use('TkAgg')\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler  # install via: pip install imbalanced-learn\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "from scipy.stats import skew\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "seed = 69\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "label = 'agitation-four'       # quaternary     prediction\n",
    "weighted = True                # weighted      unweighted\n",
    "kfold = False                   # kfold         lopo\n",
    "\n",
    "results_dir = '/home/ali/PycharmProjects/tihm/results/06h-quaternary-prediction-lopo-weighted-ctx-sta-phy'\n",
    "\n",
    "os.makedirs(results_dir, exist_ok=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/home/ali/PycharmProjects/tihm/dataset'\n",
    "\n",
    "dataset = pd.read_csv(os.path.join(root, 'data-06h.csv'))\n",
    "# dataset_12h = pd.read_csv(os.path.join(root, 'data-12h.csv'))\n",
    "# dataset_24h = pd.read_csv(os.path.join(root, 'data-24h.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "135.0\n",
      "135.0\n",
      "(array([  117,   358,   528,   532,   536,   537,   552,   564,   568,\n",
      "         572,   597,   661,   669,   923,   935,   940,   944,   963,\n",
      "         967,   983,  1039,  1044,  1055,  1074,  1662,  2660,  2986,\n",
      "        3010,  3163,  4157,  4261,  4439,  4442,  4454,  4460,  4464,\n",
      "        4467,  4468,  4470,  4471,  4472,  4474,  4476,  4480,  4482,\n",
      "        4483,  4484,  4487,  4488,  4491,  4492,  4495,  4504,  4506,\n",
      "        4507,  4515,  4519,  4520,  4530,  4532,  4555,  4571,  4579,\n",
      "        4593,  4692,  4740,  4741,  4748,  4764,  4772,  4792,  4891,\n",
      "        5066,  5067,  5195,  5386,  5503,  5504,  5523,  5543,  5564,\n",
      "        5567,  5587,  5591,  5600,  5622,  5626,  5649,  5827,  5828,\n",
      "        6034,  6120,  6152,  6156,  6164,  6185,  6204,  7192,  7197,\n",
      "        7241,  7384,  7493,  7501,  7502,  7509,  7517,  7518,  7525,\n",
      "        7533,  7534,  7537,  7541,  7568,  8661,  8976,  8977,  9070,\n",
      "        9074,  9087,  9094,  9103,  9186,  9426,  9470,  9514,  9526,\n",
      "        9660,  9891,  9915,  9935, 10128, 10148, 10257, 10262, 10368]),)\n",
      "(array([  116,   357,   527,   531,   535,   536,   551,   563,   567,\n",
      "         571,   596,   660,   668,   922,   934,   939,   943,   962,\n",
      "         966,   982,  1038,  1043,  1054,  1073,  1661,  2659,  2985,\n",
      "        3009,  3162,  4156,  4260,  4438,  4441,  4453,  4459,  4463,\n",
      "        4466,  4467,  4469,  4470,  4471,  4473,  4475,  4479,  4481,\n",
      "        4482,  4483,  4486,  4487,  4490,  4491,  4494,  4503,  4505,\n",
      "        4506,  4514,  4518,  4519,  4529,  4531,  4554,  4570,  4578,\n",
      "        4592,  4691,  4739,  4740,  4747,  4763,  4771,  4791,  4890,\n",
      "        5065,  5066,  5194,  5385,  5502,  5503,  5522,  5542,  5563,\n",
      "        5566,  5586,  5590,  5599,  5621,  5625,  5648,  5826,  5827,\n",
      "        6033,  6119,  6151,  6155,  6163,  6184,  6203,  7191,  7196,\n",
      "        7240,  7383,  7492,  7500,  7501,  7508,  7516,  7517,  7524,\n",
      "        7532,  7533,  7536,  7540,  7567,  8660,  8975,  8976,  9069,\n",
      "        9073,  9086,  9093,  9102,  9185,  9425,  9469,  9513,  9525,\n",
      "        9659,  9890,  9914,  9934, 10127, 10147, 10256, 10261, 10367]),)\n",
      "1755 135\n",
      "1752 135\n",
      "1400 117 89 18\n",
      "['id' 'date' '6h' 'back-door' 'bathroom' 'bedroom' 'fridge-door'\n",
      " 'front-door' 'hallway' 'kitchen' 'lounge' 'total-events'\n",
      " 'unique-locations' 'active-location-ratio' 'private-to-public-ratio'\n",
      " 'location-entropy' 'location-dominance-ratio' 'back-and-forth-count'\n",
      " 'num-transitions' 'back-door-count-max' 'back-door-count-mean'\n",
      " 'back-door-count-std' 'back-door-count-sum' 'bathroom-count-max'\n",
      " 'bathroom-count-mean' 'bathroom-count-std' 'bathroom-count-sum'\n",
      " 'bedroom-count-max' 'bedroom-count-mean' 'bedroom-count-std'\n",
      " 'bedroom-count-sum' 'fridge-door-count-max' 'fridge-door-count-mean'\n",
      " 'fridge-door-count-std' 'fridge-door-count-sum' 'front-door-count-max'\n",
      " 'front-door-count-mean' 'front-door-count-std' 'front-door-count-sum'\n",
      " 'hallway-count-max' 'hallway-count-mean' 'hallway-count-std'\n",
      " 'hallway-count-sum' 'kitchen-count-max' 'kitchen-count-mean'\n",
      " 'kitchen-count-std' 'kitchen-count-sum' 'lounge-count-max'\n",
      " 'lounge-count-mean' 'lounge-count-std' 'lounge-count-sum'\n",
      " 'body-temperature' 'body-weight' 'diastolic-blood-pressure' 'heart-rate'\n",
      " 'muscle-mass' 'systolic-blood-pressure' 'total-body-water'\n",
      " 'skin-temperature' 'blood-pressure' 'agitation' 'body-water' 'pulse'\n",
      " 'weight' 'body-temperature-label' 'age' 'sex' 'agitation-next'\n",
      " 'agitation-four']\n"
     ]
    }
   ],
   "source": [
    "# display(dataset)\n",
    "\n",
    "print(dataset['agitation'].equals(dataset['agitation-next']))\n",
    "\n",
    "print(np.sum(dataset['agitation']))\n",
    "print(np.sum(dataset['agitation-next']))\n",
    "\n",
    "print(np.where(dataset['agitation'] == 1))\n",
    "print(np.where(dataset['agitation-next'] == 1))\n",
    "\n",
    "print(np.sum(dataset['agitation'] == 0),\n",
    "      np.sum(dataset['agitation'] == 1))\n",
    "\n",
    "print(np.sum(dataset['agitation-next'] == 0),\n",
    "      np.sum(dataset['agitation-next'] == 1))\n",
    "\n",
    "print(np.sum(dataset['agitation-four'] == 0),\n",
    "      np.sum(dataset['agitation-four'] == 1),\n",
    "      np.sum(dataset['agitation-four'] == 2),\n",
    "      np.sum(dataset['agitation-four'] == 3))\n",
    "\n",
    "\n",
    "# display(dataset)\n",
    "print(dataset.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of participants: 56\n",
      "Total number of participants with at least one agitation episode: 27\n",
      "Total number of participants with no agitation episodes: 29\n",
      "Total number of agitation episodes: 135\n"
     ]
    }
   ],
   "source": [
    "# Agitation Statistics\n",
    "\n",
    "temp = dataset.copy()\n",
    "# temp = dataset_12h\n",
    "\n",
    "temp['agitation'].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "print(f\"Total number of participants: {temp['id'].nunique()}\")\n",
    "print(f\"Total number of participants with at least one agitation episode: {temp[temp['agitation'] == 1]['id'].nunique()}\")\n",
    "print(f\"Total number of participants with no agitation episodes: {temp['id'].nunique() - temp[temp['agitation'] == 1]['id'].nunique()}\")\n",
    "print(f\"Total number of agitation episodes: {(temp['agitation'] == 1).sum()}\")\n",
    "\n",
    "\n",
    "ids, total_counts = np.unique(temp['id'], return_counts=True)\n",
    "agitated_ids, agitation_counts = np.unique(temp[temp['agitation'] == 1]['id'], return_counts=True)\n",
    "agitation_map = dict(zip(agitated_ids, agitation_counts))\n",
    "unique_dates = temp.groupby('id')['date'].nunique().reindex(ids, fill_value=0).values\n",
    "participant_summary_df = pd.DataFrame({\n",
    "    'participant_id': ids,\n",
    "    'six_hour_sample_count': total_counts,\n",
    "    'unique_dates': unique_dates,\n",
    "    'agitation_episodes': [agitation_map.get(pid, 0) for pid in ids]\n",
    "})\n",
    "participant_summary_df = participant_summary_df.sort_values(by='agitation_episodes', ascending=False).reset_index(drop=True)\n",
    "# display(participant_summary_df)\n",
    "\n",
    "\n",
    "values, counts = np.unique(temp.loc[temp['agitation'] == 1, '6h'], return_counts=True)\n",
    "# values, counts = np.unique(temp.loc[temp['agitation'] == 1, '12h'], return_counts=True)\n",
    "\n",
    "agitation_temporal_df = pd.DataFrame({\n",
    "    '6h_time_block': values,\n",
    "    'agitation_episode_count': counts\n",
    "})\n",
    "# display(agitation_temporal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 32 8 8 2 6\n"
     ]
    }
   ],
   "source": [
    "# 03\n",
    "columns_indices = ['id', 'date', '6h']\n",
    "\n",
    "# 08\n",
    "columns_count = ['back-door', 'bathroom', 'bedroom', 'fridge-door',\n",
    "                 'front-door', 'hallway', 'kitchen', 'lounge']\n",
    "\n",
    "# 08\n",
    "columns_contextual = ['total-events', 'unique-locations', 'active-location-ratio', 'private-to-public-ratio',\n",
    "                      'location-entropy', 'location-dominance-ratio', 'back-and-forth-count', 'num-transitions']\n",
    "\n",
    "# 32\n",
    "columns_statistical = ['back-door-count-max', 'back-door-count-mean', 'back-door-count-std', 'back-door-count-sum',\n",
    "                       'bathroom-count-max', 'bathroom-count-mean', 'bathroom-count-std', 'bathroom-count-sum',\n",
    "                       'bedroom-count-max', 'bedroom-count-mean', 'bedroom-count-std', 'bedroom-count-sum',\n",
    "                       'fridge-door-count-max', 'fridge-door-count-mean', 'fridge-door-count-std', 'fridge-door-count-sum',\n",
    "                       'front-door-count-max', 'front-door-count-mean', 'front-door-count-std', 'front-door-count-sum',\n",
    "                       'hallway-count-max', 'hallway-count-mean', 'hallway-count-std', 'hallway-count-sum',\n",
    "                       'kitchen-count-max', 'kitchen-count-mean', 'kitchen-count-std','kitchen-count-sum',\n",
    "                       'lounge-count-max', 'lounge-count-mean', 'lounge-count-std','lounge-count-sum']\n",
    "\n",
    "# 08\n",
    "columns_physiology = ['body-temperature', 'body-weight', 'diastolic-blood-pressure', 'heart-rate',\n",
    "                      'muscle-mass', 'systolic-blood-pressure', 'total-body-water', 'skin-temperature']\n",
    "\n",
    "# 06\n",
    "columns_labels = ['blood-pressure', 'agitation', 'body-water', 'pulse', 'weight', 'body-temperature-label']\n",
    "\n",
    "# 02\n",
    "columns_demographics = ['age', 'sex']\n",
    "\n",
    "# 02\n",
    "columns_prediction = ['agitation-next', 'agitation-four']\n",
    "\n",
    "print(len(columns_count),\n",
    "      len(columns_statistical),\n",
    "      len(columns_contextual),\n",
    "      len(columns_physiology),\n",
    "      len(columns_demographics),\n",
    "      len(columns_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'date', '6h', 'back-door', 'bathroom', 'bedroom', 'fridge-door', 'front-door', 'hallway', 'kitchen', 'lounge', 'total-events', 'unique-locations', 'active-location-ratio', 'private-to-public-ratio', 'location-entropy', 'location-dominance-ratio', 'back-and-forth-count', 'num-transitions', 'back-door-count-max', 'back-door-count-mean', 'back-door-count-std', 'back-door-count-sum', 'bathroom-count-max', 'bathroom-count-mean', 'bathroom-count-std', 'bathroom-count-sum', 'bedroom-count-max', 'bedroom-count-mean', 'bedroom-count-std', 'bedroom-count-sum', 'fridge-door-count-max', 'fridge-door-count-mean', 'fridge-door-count-std', 'fridge-door-count-sum', 'front-door-count-max', 'front-door-count-mean', 'front-door-count-std', 'front-door-count-sum', 'hallway-count-max', 'hallway-count-mean', 'hallway-count-std', 'hallway-count-sum', 'kitchen-count-max', 'kitchen-count-mean', 'kitchen-count-std', 'kitchen-count-sum', 'lounge-count-max', 'lounge-count-mean', 'lounge-count-std', 'lounge-count-sum', 'body-temperature', 'body-weight', 'diastolic-blood-pressure', 'heart-rate', 'muscle-mass', 'systolic-blood-pressure', 'total-body-water', 'skin-temperature', 'blood-pressure', 'agitation', 'body-water', 'pulse', 'weight', 'body-temperature-label', 'age', 'sex', 'agitation-next', 'agitation-four']\n",
      "['id', 'date', '6h', 'back-door', 'bathroom', 'bedroom', 'fridge-door', 'front-door', 'hallway', 'kitchen', 'lounge', 'total-events', 'unique-locations', 'active-location-ratio', 'private-to-public-ratio', 'location-entropy', 'location-dominance-ratio', 'back-and-forth-count', 'num-transitions', 'back-door-count-max', 'back-door-count-mean', 'back-door-count-std', 'back-door-count-sum', 'bathroom-count-max', 'bathroom-count-mean', 'bathroom-count-std', 'bathroom-count-sum', 'bedroom-count-max', 'bedroom-count-mean', 'bedroom-count-std', 'bedroom-count-sum', 'fridge-door-count-max', 'fridge-door-count-mean', 'fridge-door-count-std', 'fridge-door-count-sum', 'front-door-count-max', 'front-door-count-mean', 'front-door-count-std', 'front-door-count-sum', 'hallway-count-max', 'hallway-count-mean', 'hallway-count-std', 'hallway-count-sum', 'kitchen-count-max', 'kitchen-count-mean', 'kitchen-count-std', 'kitchen-count-sum', 'lounge-count-max', 'lounge-count-mean', 'lounge-count-std', 'lounge-count-sum', 'body-temperature', 'body-weight', 'diastolic-blood-pressure', 'heart-rate', 'muscle-mass', 'systolic-blood-pressure', 'total-body-water', 'skin-temperature', 'blood-pressure', 'agitation', 'body-water', 'pulse', 'weight', 'body-temperature-label', 'age', 'sex', 'agitation-next', 'agitation-four']\n",
      "True\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "[0 1 2 3]\n",
      "349\n",
      "['id', 'date', '6h', 'total-events', 'unique-locations', 'active-location-ratio', 'private-to-public-ratio', 'location-entropy', 'location-dominance-ratio', 'back-and-forth-count', 'num-transitions', 'back-door-count-max', 'back-door-count-mean', 'back-door-count-std', 'back-door-count-sum', 'bathroom-count-max', 'bathroom-count-mean', 'bathroom-count-std', 'bathroom-count-sum', 'bedroom-count-max', 'bedroom-count-mean', 'bedroom-count-std', 'bedroom-count-sum', 'fridge-door-count-max', 'fridge-door-count-mean', 'fridge-door-count-std', 'fridge-door-count-sum', 'front-door-count-max', 'front-door-count-mean', 'front-door-count-std', 'front-door-count-sum', 'hallway-count-max', 'hallway-count-mean', 'hallway-count-std', 'hallway-count-sum', 'kitchen-count-max', 'kitchen-count-mean', 'kitchen-count-std', 'kitchen-count-sum', 'lounge-count-max', 'lounge-count-mean', 'lounge-count-std', 'lounge-count-sum', 'body-temperature', 'body-weight', 'diastolic-blood-pressure', 'heart-rate', 'muscle-mass', 'systolic-blood-pressure', 'total-body-water', 'skin-temperature']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'y: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], shape=(10790,))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'p: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([ 0,  0,  0, ..., 55, 55, 55], shape=(10790,))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'dataset: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>6h</th>\n",
       "      <th>total-events</th>\n",
       "      <th>unique-locations</th>\n",
       "      <th>active-location-ratio</th>\n",
       "      <th>private-to-public-ratio</th>\n",
       "      <th>location-entropy</th>\n",
       "      <th>location-dominance-ratio</th>\n",
       "      <th>back-and-forth-count</th>\n",
       "      <th>...</th>\n",
       "      <th>lounge-count-std</th>\n",
       "      <th>lounge-count-sum</th>\n",
       "      <th>body-temperature</th>\n",
       "      <th>body-weight</th>\n",
       "      <th>diastolic-blood-pressure</th>\n",
       "      <th>heart-rate</th>\n",
       "      <th>muscle-mass</th>\n",
       "      <th>systolic-blood-pressure</th>\n",
       "      <th>total-body-water</th>\n",
       "      <th>skin-temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0697d</td>\n",
       "      <td>2019-06-28</td>\n",
       "      <td>00-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0697d</td>\n",
       "      <td>2019-06-28</td>\n",
       "      <td>06-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0697d</td>\n",
       "      <td>2019-06-28</td>\n",
       "      <td>12-18</td>\n",
       "      <td>221.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.1691</td>\n",
       "      <td>2.7340</td>\n",
       "      <td>0.2579</td>\n",
       "      <td>35.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.6999</td>\n",
       "      <td>49.0</td>\n",
       "      <td>36.1864</td>\n",
       "      <td>86.3</td>\n",
       "      <td>82.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>64.5</td>\n",
       "      <td>165.0</td>\n",
       "      <td>50.6</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0697d</td>\n",
       "      <td>2019-06-28</td>\n",
       "      <td>18-24</td>\n",
       "      <td>101.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.0889</td>\n",
       "      <td>1.8916</td>\n",
       "      <td>0.4851</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.8130</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0697d</td>\n",
       "      <td>2019-06-29</td>\n",
       "      <td>00-06</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10785</th>\n",
       "      <td>fd100</td>\n",
       "      <td>2019-06-29</td>\n",
       "      <td>12-18</td>\n",
       "      <td>182.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.2283</td>\n",
       "      <td>2.5224</td>\n",
       "      <td>0.3077</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4828</td>\n",
       "      <td>46.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10786</th>\n",
       "      <td>fd100</td>\n",
       "      <td>2019-06-29</td>\n",
       "      <td>18-24</td>\n",
       "      <td>114.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.3671</td>\n",
       "      <td>2.4755</td>\n",
       "      <td>0.2982</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.7583</td>\n",
       "      <td>34.0</td>\n",
       "      <td>36.2400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10787</th>\n",
       "      <td>fd100</td>\n",
       "      <td>2019-06-30</td>\n",
       "      <td>00-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10788</th>\n",
       "      <td>fd100</td>\n",
       "      <td>2019-06-30</td>\n",
       "      <td>06-12</td>\n",
       "      <td>146.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.5465</td>\n",
       "      <td>2.5379</td>\n",
       "      <td>0.2603</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.4683</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10789</th>\n",
       "      <td>fd100</td>\n",
       "      <td>2019-06-30</td>\n",
       "      <td>12-18</td>\n",
       "      <td>127.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.3636</td>\n",
       "      <td>2.1376</td>\n",
       "      <td>0.3937</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.9766</td>\n",
       "      <td>32.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10790 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id        date     6h  total-events  unique-locations  \\\n",
       "0      0697d  2019-06-28  00-06           0.0               0.0   \n",
       "1      0697d  2019-06-28  06-12           0.0               0.0   \n",
       "2      0697d  2019-06-28  12-18         221.0               8.0   \n",
       "3      0697d  2019-06-28  18-24         101.0               7.0   \n",
       "4      0697d  2019-06-29  00-06           3.0               1.0   \n",
       "...      ...         ...    ...           ...               ...   \n",
       "10785  fd100  2019-06-29  12-18         182.0               7.0   \n",
       "10786  fd100  2019-06-29  18-24         114.0               7.0   \n",
       "10787  fd100  2019-06-30  00-06           0.0               0.0   \n",
       "10788  fd100  2019-06-30  06-12         146.0               7.0   \n",
       "10789  fd100  2019-06-30  12-18         127.0               7.0   \n",
       "\n",
       "       active-location-ratio  private-to-public-ratio  location-entropy  \\\n",
       "0                      0.000                   1.0000            0.0000   \n",
       "1                      0.000                   1.0000            0.0000   \n",
       "2                      1.000                   0.1691            2.7340   \n",
       "3                      0.875                   0.0889            1.8916   \n",
       "4                      0.125                   1.0000           -0.0000   \n",
       "...                      ...                      ...               ...   \n",
       "10785                  0.875                   0.2283            2.5224   \n",
       "10786                  0.875                   0.3671            2.4755   \n",
       "10787                  0.000                   1.0000            0.0000   \n",
       "10788                  0.875                   0.5465            2.5379   \n",
       "10789                  0.875                   0.3636            2.1376   \n",
       "\n",
       "       location-dominance-ratio  back-and-forth-count  ...  lounge-count-std  \\\n",
       "0                        0.0000                   0.0  ...            0.0000   \n",
       "1                        0.0000                   0.0  ...            0.0000   \n",
       "2                        0.2579                  35.0  ...            4.6999   \n",
       "3                        0.4851                  22.0  ...            3.8130   \n",
       "4                        1.0000                   0.0  ...            0.0000   \n",
       "...                         ...                   ...  ...               ...   \n",
       "10785                    0.3077                  31.0  ...            4.4828   \n",
       "10786                    0.2982                  19.0  ...            3.7583   \n",
       "10787                    0.0000                   0.0  ...            0.0000   \n",
       "10788                    0.2603                  10.0  ...            3.4683   \n",
       "10789                    0.3937                   8.0  ...            2.9766   \n",
       "\n",
       "       lounge-count-sum  body-temperature  body-weight  \\\n",
       "0                   0.0               NaN          NaN   \n",
       "1                   0.0               NaN          NaN   \n",
       "2                  49.0           36.1864         86.3   \n",
       "3                  31.0               NaN          NaN   \n",
       "4                   0.0               NaN          NaN   \n",
       "...                 ...               ...          ...   \n",
       "10785              46.0               NaN          NaN   \n",
       "10786              34.0           36.2400          NaN   \n",
       "10787               0.0               NaN          NaN   \n",
       "10788              31.0               NaN          NaN   \n",
       "10789              32.0               NaN          NaN   \n",
       "\n",
       "       diastolic-blood-pressure  heart-rate  muscle-mass  \\\n",
       "0                           NaN         NaN          NaN   \n",
       "1                           NaN         NaN          NaN   \n",
       "2                          82.0        42.0         64.5   \n",
       "3                           NaN         NaN          NaN   \n",
       "4                           NaN         NaN          NaN   \n",
       "...                         ...         ...          ...   \n",
       "10785                       NaN         NaN          NaN   \n",
       "10786                       NaN         NaN          NaN   \n",
       "10787                       NaN         NaN          NaN   \n",
       "10788                       NaN         NaN          NaN   \n",
       "10789                       NaN         NaN          NaN   \n",
       "\n",
       "       systolic-blood-pressure  total-body-water  skin-temperature  \n",
       "0                          NaN               NaN               NaN  \n",
       "1                          NaN               NaN               NaN  \n",
       "2                        165.0              50.6               NaN  \n",
       "3                          NaN               NaN               NaN  \n",
       "4                          NaN               NaN               NaN  \n",
       "...                        ...               ...               ...  \n",
       "10785                      NaN               NaN               NaN  \n",
       "10786                      NaN               NaN               NaN  \n",
       "10787                      NaN               NaN               NaN  \n",
       "10788                      NaN               NaN               NaN  \n",
       "10789                      NaN               NaN               NaN  \n",
       "\n",
       "[10790 rows x 51 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "columns = columns_indices + columns_count + columns_contextual + columns_statistical + columns_physiology + columns_labels + columns_demographics + columns_prediction\n",
    "print(dataset.columns.to_list())\n",
    "print(columns)\n",
    "\n",
    "print(columns == dataset.columns.to_list())\n",
    "\n",
    "\n",
    "y = np.array(dataset[[label]]).squeeze()\n",
    "\n",
    "if label == 'agitation' or label == 'agitation-next':\n",
    "    y[y == -1] = 0\n",
    "    y[y >= 1] = 1\n",
    "elif label == 'agitation-four':\n",
    "    y[y == -1] = 0\n",
    "    y[y == -10] = 0\n",
    "\n",
    "ids = np.array(dataset['id']).squeeze()\n",
    "p = np.unique(ids, return_inverse=True)[1]\n",
    "\n",
    "print(np.isnan(y).sum())\n",
    "print(np.isnan(p).sum())\n",
    "\n",
    "y = np.nan_to_num(y, nan=0)\n",
    "\n",
    "print(np.isnan(y).sum())\n",
    "print(np.isnan(p).sum())\n",
    "\n",
    "print(np.unique(y))\n",
    "print(y.sum())\n",
    "\n",
    "\n",
    "# To Drop\n",
    "dataset.drop(\n",
    "    \n",
    "    columns_count +\n",
    "    \n",
    "\n",
    "    # columns_contextual +\n",
    "\n",
    "    # columns_statistical +\n",
    "    \n",
    "    # columns_physiology +\n",
    "\n",
    "\n",
    "    columns_labels +\n",
    "    columns_demographics + \n",
    "    columns_prediction\n",
    "\n",
    "    , axis=1, inplace=True)\n",
    "\n",
    "print(dataset.columns.to_list())\n",
    "\n",
    "display('y: ', y)\n",
    "display('p: ', p)\n",
    "display('dataset: ', dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89869\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Imputation\n",
    "import utils_data\n",
    "importlib.reload(utils_data)\n",
    "from utils_data import hierarchical_imputation\n",
    "\n",
    "\n",
    "\n",
    "print(dataset.isna().sum().sum())\n",
    "\n",
    "dataset = hierarchical_imputation(dataset)\n",
    "\n",
    "print(dataset.isna().sum().sum())\n",
    "\n",
    "dataset.drop(\n",
    "    columns_indices,\n",
    "    axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10790, 48)\n",
      "(10790,)\n",
      "[0 1 2 3]\n",
      "['total-events' 'unique-locations' 'active-location-ratio'\n",
      " 'private-to-public-ratio' 'location-entropy' 'location-dominance-ratio'\n",
      " 'back-and-forth-count' 'num-transitions' 'back-door-count-max'\n",
      " 'back-door-count-mean' 'back-door-count-std' 'back-door-count-sum'\n",
      " 'bathroom-count-max' 'bathroom-count-mean' 'bathroom-count-std'\n",
      " 'bathroom-count-sum' 'bedroom-count-max' 'bedroom-count-mean'\n",
      " 'bedroom-count-std' 'bedroom-count-sum' 'fridge-door-count-max'\n",
      " 'fridge-door-count-mean' 'fridge-door-count-std' 'fridge-door-count-sum'\n",
      " 'front-door-count-max' 'front-door-count-mean' 'front-door-count-std'\n",
      " 'front-door-count-sum' 'hallway-count-max' 'hallway-count-mean'\n",
      " 'hallway-count-std' 'hallway-count-sum' 'kitchen-count-max'\n",
      " 'kitchen-count-mean' 'kitchen-count-std' 'kitchen-count-sum'\n",
      " 'lounge-count-max' 'lounge-count-mean' 'lounge-count-std'\n",
      " 'lounge-count-sum' 'body-temperature' 'body-weight'\n",
      " 'diastolic-blood-pressure' 'heart-rate' 'muscle-mass'\n",
      " 'systolic-blood-pressure' 'total-body-water' 'skin-temperature']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total-events</th>\n",
       "      <th>unique-locations</th>\n",
       "      <th>active-location-ratio</th>\n",
       "      <th>private-to-public-ratio</th>\n",
       "      <th>location-entropy</th>\n",
       "      <th>location-dominance-ratio</th>\n",
       "      <th>back-and-forth-count</th>\n",
       "      <th>num-transitions</th>\n",
       "      <th>back-door-count-max</th>\n",
       "      <th>back-door-count-mean</th>\n",
       "      <th>...</th>\n",
       "      <th>lounge-count-std</th>\n",
       "      <th>lounge-count-sum</th>\n",
       "      <th>body-temperature</th>\n",
       "      <th>body-weight</th>\n",
       "      <th>diastolic-blood-pressure</th>\n",
       "      <th>heart-rate</th>\n",
       "      <th>muscle-mass</th>\n",
       "      <th>systolic-blood-pressure</th>\n",
       "      <th>total-body-water</th>\n",
       "      <th>skin-temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.5675</td>\n",
       "      <td>69.1708</td>\n",
       "      <td>75.6146</td>\n",
       "      <td>72.1250</td>\n",
       "      <td>47.5853</td>\n",
       "      <td>136.4271</td>\n",
       "      <td>49.8206</td>\n",
       "      <td>34.2843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.5675</td>\n",
       "      <td>69.1708</td>\n",
       "      <td>75.6146</td>\n",
       "      <td>72.1250</td>\n",
       "      <td>47.5853</td>\n",
       "      <td>136.4271</td>\n",
       "      <td>49.8206</td>\n",
       "      <td>34.2843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>221.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.1691</td>\n",
       "      <td>2.7340</td>\n",
       "      <td>0.2579</td>\n",
       "      <td>35.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.5455</td>\n",
       "      <td>...</td>\n",
       "      <td>4.6999</td>\n",
       "      <td>49.0</td>\n",
       "      <td>36.1864</td>\n",
       "      <td>86.3000</td>\n",
       "      <td>82.0000</td>\n",
       "      <td>42.0000</td>\n",
       "      <td>64.5000</td>\n",
       "      <td>165.0000</td>\n",
       "      <td>50.6000</td>\n",
       "      <td>34.2843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.0889</td>\n",
       "      <td>1.8916</td>\n",
       "      <td>0.4851</td>\n",
       "      <td>22.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0909</td>\n",
       "      <td>...</td>\n",
       "      <td>3.8130</td>\n",
       "      <td>31.0</td>\n",
       "      <td>36.5675</td>\n",
       "      <td>69.1708</td>\n",
       "      <td>75.6146</td>\n",
       "      <td>72.1250</td>\n",
       "      <td>47.5853</td>\n",
       "      <td>136.4271</td>\n",
       "      <td>49.8206</td>\n",
       "      <td>34.2843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.5425</td>\n",
       "      <td>71.7159</td>\n",
       "      <td>74.1458</td>\n",
       "      <td>76.0469</td>\n",
       "      <td>48.6600</td>\n",
       "      <td>130.7188</td>\n",
       "      <td>51.7600</td>\n",
       "      <td>34.5600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10785</th>\n",
       "      <td>182.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.2283</td>\n",
       "      <td>2.5224</td>\n",
       "      <td>0.3077</td>\n",
       "      <td>31.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4828</td>\n",
       "      <td>46.0</td>\n",
       "      <td>36.5425</td>\n",
       "      <td>71.7159</td>\n",
       "      <td>74.1458</td>\n",
       "      <td>76.0469</td>\n",
       "      <td>48.6600</td>\n",
       "      <td>130.7188</td>\n",
       "      <td>51.7600</td>\n",
       "      <td>34.5600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10786</th>\n",
       "      <td>114.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.3671</td>\n",
       "      <td>2.4755</td>\n",
       "      <td>0.2982</td>\n",
       "      <td>19.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.7583</td>\n",
       "      <td>34.0</td>\n",
       "      <td>36.2400</td>\n",
       "      <td>71.7159</td>\n",
       "      <td>74.1458</td>\n",
       "      <td>76.0469</td>\n",
       "      <td>48.6600</td>\n",
       "      <td>130.7188</td>\n",
       "      <td>51.7600</td>\n",
       "      <td>34.5600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10787</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.7593</td>\n",
       "      <td>68.9053</td>\n",
       "      <td>73.7692</td>\n",
       "      <td>70.3526</td>\n",
       "      <td>46.2964</td>\n",
       "      <td>134.0000</td>\n",
       "      <td>51.1929</td>\n",
       "      <td>34.6478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10788</th>\n",
       "      <td>146.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.5465</td>\n",
       "      <td>2.5379</td>\n",
       "      <td>0.2603</td>\n",
       "      <td>10.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.4683</td>\n",
       "      <td>31.0</td>\n",
       "      <td>36.7593</td>\n",
       "      <td>68.9053</td>\n",
       "      <td>73.7692</td>\n",
       "      <td>70.3526</td>\n",
       "      <td>46.2964</td>\n",
       "      <td>134.0000</td>\n",
       "      <td>51.1929</td>\n",
       "      <td>34.6478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10789</th>\n",
       "      <td>127.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.3636</td>\n",
       "      <td>2.1376</td>\n",
       "      <td>0.3937</td>\n",
       "      <td>8.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.9766</td>\n",
       "      <td>32.0</td>\n",
       "      <td>36.7593</td>\n",
       "      <td>68.9053</td>\n",
       "      <td>73.7692</td>\n",
       "      <td>70.3526</td>\n",
       "      <td>46.2964</td>\n",
       "      <td>134.0000</td>\n",
       "      <td>51.1929</td>\n",
       "      <td>34.6478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10790 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       total-events  unique-locations  active-location-ratio  \\\n",
       "0               0.0               0.0                  0.000   \n",
       "1               0.0               0.0                  0.000   \n",
       "2             221.0               8.0                  1.000   \n",
       "3             101.0               7.0                  0.875   \n",
       "4               3.0               1.0                  0.125   \n",
       "...             ...               ...                    ...   \n",
       "10785         182.0               7.0                  0.875   \n",
       "10786         114.0               7.0                  0.875   \n",
       "10787           0.0               0.0                  0.000   \n",
       "10788         146.0               7.0                  0.875   \n",
       "10789         127.0               7.0                  0.875   \n",
       "\n",
       "       private-to-public-ratio  location-entropy  location-dominance-ratio  \\\n",
       "0                       1.0000            0.0000                    0.0000   \n",
       "1                       1.0000            0.0000                    0.0000   \n",
       "2                       0.1691            2.7340                    0.2579   \n",
       "3                       0.0889            1.8916                    0.4851   \n",
       "4                       1.0000           -0.0000                    1.0000   \n",
       "...                        ...               ...                       ...   \n",
       "10785                   0.2283            2.5224                    0.3077   \n",
       "10786                   0.3671            2.4755                    0.2982   \n",
       "10787                   1.0000            0.0000                    0.0000   \n",
       "10788                   0.5465            2.5379                    0.2603   \n",
       "10789                   0.3636            2.1376                    0.3937   \n",
       "\n",
       "       back-and-forth-count  num-transitions  back-door-count-max  \\\n",
       "0                       0.0              0.0                  0.0   \n",
       "1                       0.0              0.0                  0.0   \n",
       "2                      35.0            155.0                 11.0   \n",
       "3                      22.0             61.0                  1.0   \n",
       "4                       0.0              1.0                  0.0   \n",
       "...                     ...              ...                  ...   \n",
       "10785                  31.0            139.0                  0.0   \n",
       "10786                  19.0             92.0                  0.0   \n",
       "10787                   0.0              0.0                  0.0   \n",
       "10788                  10.0            110.0                  0.0   \n",
       "10789                   8.0             66.0                  0.0   \n",
       "\n",
       "       back-door-count-mean  ...  lounge-count-std  lounge-count-sum  \\\n",
       "0                    0.0000  ...            0.0000               0.0   \n",
       "1                    0.0000  ...            0.0000               0.0   \n",
       "2                    0.5455  ...            4.6999              49.0   \n",
       "3                    0.0909  ...            3.8130              31.0   \n",
       "4                    0.0000  ...            0.0000               0.0   \n",
       "...                     ...  ...               ...               ...   \n",
       "10785                0.0000  ...            4.4828              46.0   \n",
       "10786                0.0000  ...            3.7583              34.0   \n",
       "10787                0.0000  ...            0.0000               0.0   \n",
       "10788                0.0000  ...            3.4683              31.0   \n",
       "10789                0.0000  ...            2.9766              32.0   \n",
       "\n",
       "       body-temperature  body-weight  diastolic-blood-pressure  heart-rate  \\\n",
       "0               36.5675      69.1708                   75.6146     72.1250   \n",
       "1               36.5675      69.1708                   75.6146     72.1250   \n",
       "2               36.1864      86.3000                   82.0000     42.0000   \n",
       "3               36.5675      69.1708                   75.6146     72.1250   \n",
       "4               36.5425      71.7159                   74.1458     76.0469   \n",
       "...                 ...          ...                       ...         ...   \n",
       "10785           36.5425      71.7159                   74.1458     76.0469   \n",
       "10786           36.2400      71.7159                   74.1458     76.0469   \n",
       "10787           36.7593      68.9053                   73.7692     70.3526   \n",
       "10788           36.7593      68.9053                   73.7692     70.3526   \n",
       "10789           36.7593      68.9053                   73.7692     70.3526   \n",
       "\n",
       "       muscle-mass  systolic-blood-pressure  total-body-water  \\\n",
       "0          47.5853                 136.4271           49.8206   \n",
       "1          47.5853                 136.4271           49.8206   \n",
       "2          64.5000                 165.0000           50.6000   \n",
       "3          47.5853                 136.4271           49.8206   \n",
       "4          48.6600                 130.7188           51.7600   \n",
       "...            ...                      ...               ...   \n",
       "10785      48.6600                 130.7188           51.7600   \n",
       "10786      48.6600                 130.7188           51.7600   \n",
       "10787      46.2964                 134.0000           51.1929   \n",
       "10788      46.2964                 134.0000           51.1929   \n",
       "10789      46.2964                 134.0000           51.1929   \n",
       "\n",
       "       skin-temperature  \n",
       "0               34.2843  \n",
       "1               34.2843  \n",
       "2               34.2843  \n",
       "3               34.2843  \n",
       "4               34.5600  \n",
       "...                 ...  \n",
       "10785           34.5600  \n",
       "10786           34.5600  \n",
       "10787           34.6478  \n",
       "10788           34.6478  \n",
       "10789           34.6478  \n",
       "\n",
       "[10790 rows x 48 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.array(dataset)\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "np.save(os.path.join('/home/ali/PycharmProjects/tihm/xyp', 'x.npy'), x)\n",
    "np.save(os.path.join('/home/ali/PycharmProjects/tihm/xyp', 'y.npy'), y)\n",
    "np.save(os.path.join('/home/ali/PycharmProjects/tihm/xyp', 'p.npy'), p)\n",
    "\n",
    "\n",
    "print(np.unique(y))\n",
    "\n",
    "feature_names = dataset.columns.values\n",
    "print(feature_names)\n",
    "display(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10779 11\n",
      "1 10617 173\n",
      "2 10488 302\n",
      "3 10599 191\n",
      "4 10604 186\n",
      "5 10767 23\n",
      "6 10437 353\n",
      "7 10541 249\n",
      "8 10500 290\n",
      "9 10611 179\n",
      "10 10771 19\n",
      "11 10560 230\n",
      "12 10532 258\n",
      "13 10716 74\n",
      "14 10524 266\n",
      "15 10548 242\n",
      "16 10665 125\n",
      "17 10508 282\n",
      "18 10668 122\n",
      "19 10570 220\n",
      "20 10588 202\n",
      "21 10521 269\n",
      "22 10627 163\n",
      "23 10600 190\n",
      "24 10771 19\n",
      "25 10520 270\n",
      "26 10495 295\n",
      "27 10520 270\n",
      "28 10605 185\n",
      "29 10616 174\n",
      "30 10576 214\n",
      "31 10556 234\n",
      "32 10687 103\n",
      "33 10560 230\n",
      "34 10625 165\n",
      "35 10751 39\n",
      "36 10525 265\n",
      "37 10436 354\n",
      "38 10621 169\n",
      "39 10463 327\n",
      "40 10524 266\n",
      "41 10584 206\n",
      "42 10632 158\n",
      "43 10520 270\n",
      "44 10728 62\n",
      "45 10699 91\n",
      "46 10763 27\n",
      "47 10580 210\n",
      "48 10672 118\n",
      "49 10528 262\n",
      "50 10627 163\n",
      "51 10541 249\n",
      "52 10502 288\n",
      "53 10575 215\n",
      "54 10532 258\n",
      "55 10775 15\n",
      "[[10563     3     0     0]\n",
      " [  117     0     0     0]\n",
      " [   89     0     0     0]\n",
      " [   18     0     0     0]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auc-roc-macro</th>\n",
       "      <th>auc-roc-weighted</th>\n",
       "      <th>auc-pr-macro</th>\n",
       "      <th>auc-pr-weighted</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>cm-0</th>\n",
       "      <th>cm-1</th>\n",
       "      <th>cm-2</th>\n",
       "      <th>cm-3</th>\n",
       "      <th>...</th>\n",
       "      <th>cm-6</th>\n",
       "      <th>cm-7</th>\n",
       "      <th>cm-8</th>\n",
       "      <th>cm-9</th>\n",
       "      <th>cm-10</th>\n",
       "      <th>cm-11</th>\n",
       "      <th>cm-12</th>\n",
       "      <th>cm-13</th>\n",
       "      <th>cm-14</th>\n",
       "      <th>cm-15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8657</td>\n",
       "      <td>0.8434</td>\n",
       "      <td>0.3027</td>\n",
       "      <td>0.9719</td>\n",
       "      <td>0.9790</td>\n",
       "      <td>[10563 3 0 0]\\n[117 0 0 0]\\n[89 0 0 0]\\n[18 0 ...</td>\n",
       "      <td>10563</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  auc-roc-macro auc-roc-weighted auc-pr-macro auc-pr-weighted accuracy  \\\n",
       "0        0.8657           0.8434       0.3027          0.9719   0.9790   \n",
       "\n",
       "                                    confusion_matrix   cm-0 cm-1 cm-2 cm-3  \\\n",
       "0  [10563 3 0 0]\\n[117 0 0 0]\\n[89 0 0 0]\\n[18 0 ...  10563    3    0    0   \n",
       "\n",
       "   ... cm-6 cm-7 cm-8 cm-9 cm-10 cm-11 cm-12 cm-13 cm-14 cm-15  \n",
       "0  ...    0    0   89    0     0     0    18     0     0     0  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# LightGBM\n",
    "\n",
    "name = 'LightGBM'.lower()\n",
    "\n",
    "Y_TRUES = np.empty([0])\n",
    "Y_PROBS = []\n",
    "Y_PREDS = np.empty([0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Explainability\n",
    "# importances_list = []\n",
    "# shap_list = []\n",
    "# x_test_list = []\n",
    "\n",
    "\n",
    "if kfold:\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    split_iterator = cv.split(x)\n",
    "else:\n",
    "    cv = LeavePGroupsOut(n_groups=1)\n",
    "    split_iterator = cv.split(x, y, groups=p)\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(split_iterator, start=1):\n",
    "    participant = i if kfold else np.unique(p[test_idx])[0]\n",
    "\n",
    "\n",
    "\n",
    "    x_train, x_test = x[train_idx], x[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    print(participant, x_train.shape[0], x_test.shape[0])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    normalizer = MinMaxScaler()\n",
    "    normalizer.fit(x_train)\n",
    "    x_train = normalizer.transform(x_train)\n",
    "    x_test = normalizer.transform(x_test)\n",
    "\n",
    "    # Oversample class 1\n",
    "\n",
    "    # ros = RandomOverSampler(sampling_strategy=.1, random_state=seed)\n",
    "    # x_train, y_train = ros.fit_resample(x_train, y_train)\n",
    "\n",
    "    # 1. Check original class distribution\n",
    "    # print(\"Original class distribution:\", Counter(y_train))\n",
    "    # # 2. Define the minority class (adjust if needed)\n",
    "    # minority_class = 1  # change this if your minority class label is different\n",
    "    # current_minority_count = sum(y_train == minority_class)\n",
    "    # # 3. Define desired new total count for the minority class (10x)\n",
    "    # target_minority_count = current_minority_count * 10\n",
    "    # # 4. Setup SMOTE with custom sampling strategy\n",
    "    # smote = SMOTE(sampling_strategy={minority_class: target_minority_count}, random_state=seed)\n",
    "    # # 5. Fit and resample\n",
    "    # x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "    # # 6. Confirm new distribution\n",
    "    # print(\"Resampled class distribution:\", Counter(y_train))\n",
    "\n",
    "\n",
    "\n",
    "    # ----- LightGBM\n",
    "    train_data = lgb.Dataset(x_train, label=y_train)\n",
    "    test_data = lgb.Dataset(x_test, label=y_test, reference=train_data)\n",
    "    params = {\n",
    "        'verbose': -1,  # 👈 turn off training output\n",
    "        'objective': 'multiclass',\n",
    "        'num_class': 4,  # Specify number of classes\n",
    "        'metric': 'multi_logloss',  # or 'auc' if you prefer\n",
    "        'num_leaves': 64,\n",
    "        'learning_rate': 0.01,\n",
    "        'n_estimators': 100,\n",
    "        \n",
    "        'is_unbalance': weighted  # Automatically balances positive and negative classes\n",
    "\n",
    "        # 'scale_pos_weight': (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    }\n",
    "    model = lgb.train(params, train_data, valid_sets=[train_data, test_data])\n",
    "    y_probs = model.predict(x_test, num_iteration=model.best_iteration)\n",
    "\n",
    "\n",
    "    Y_TRUES = np.append(Y_TRUES, y_test)\n",
    "    Y_PROBS.append(y_probs)\n",
    "    Y_PREDS = np.append(Y_PREDS, np.argmax(y_probs, axis=1))\n",
    "\n",
    "\n",
    "\n",
    "    # Explainability\n",
    "    # importances = model.feature_importance(importance_type='gain')\n",
    "    # importances = model.feature_importance(importance_type='split')\n",
    "    # importances_list.append(importances)\n",
    "\n",
    "\n",
    "    # explainer = shap.TreeExplainer(model)\n",
    "    # shap_values = explainer.shap_values(x_test)\n",
    "    # shap_list.append(shap_values)\n",
    "    # x_test_list.append(x_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Y_PROBS = np.concatenate(Y_PROBS, axis=0)\n",
    "\n",
    "\n",
    "indx = Y_TRUES.argsort()\n",
    "Y_TRUES = Y_TRUES[indx]\n",
    "Y_PROBS = Y_PROBS[indx]\n",
    "Y_PREDS = Y_PREDS[indx]\n",
    "\n",
    "\n",
    "# Evaluation metrics (multiclass)\n",
    "auc_roc_macro = roc_auc_score(Y_TRUES, Y_PROBS, multi_class='ovr', average='macro')\n",
    "auc_roc_weighted = roc_auc_score(Y_TRUES, Y_PROBS, multi_class='ovr', average='weighted')\n",
    "auc_pr_macro = average_precision_score(Y_TRUES, Y_PROBS, average='macro')\n",
    "auc_pr_weighted = average_precision_score(Y_TRUES, Y_PROBS, average='weighted')\n",
    "acc = accuracy_score(Y_TRUES, Y_PREDS)\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(Y_TRUES, Y_PREDS)\n",
    "print(cm)\n",
    "cm_str = '\\n'.join(['[' + ' '.join(map(str, row)) + ']' for row in cm])\n",
    "\n",
    "cm_flat = cm.flatten()\n",
    "\n",
    "results = {\n",
    "    'auc-roc-macro': f\"{auc_roc_macro:.4f}\",\n",
    "    'auc-roc-weighted': f\"{auc_roc_weighted:.4f}\",\n",
    "    'auc-pr-macro': f\"{auc_pr_macro:.4f}\",\n",
    "    'auc-pr-weighted': f\"{auc_pr_weighted:.4f}\",\n",
    "    'accuracy': f\"{acc:.4f}\",\n",
    "    'confusion_matrix': cm_str\n",
    "}\n",
    "\n",
    "for i, val in enumerate(cm_flat):\n",
    "    results[f'cm-{i}'] = str(int(val))\n",
    "\n",
    "results = pd.DataFrame([results])\n",
    "results.to_csv(os.path.join(results_dir, name + '.csv'), index=False)\n",
    "display(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Explainability\n",
    "\n",
    "# print(len(feature_names), np.vstack(importances_list).mean(axis=0).round(4).shape, x.shape, y.shape, x_test.shape, y_test.shape)\n",
    "\n",
    "# importance_df = pd.DataFrame({\n",
    "#     'feature': feature_names,\n",
    "#     'importance': np.vstack(importances_list).mean(axis=0).round(4)\n",
    "# }).sort_values(by='importance', ascending=False)\n",
    "# display(importance_df)\n",
    "# importance_df.to_csv('importance.csv', index=False)\n",
    "\n",
    "\n",
    "# # Create DataFrame for summary\n",
    "# shap_df = pd.DataFrame({\n",
    "#     'feature': feature_names,\n",
    "#     'mean_abs_shap': np.abs(np.vstack(shap_list)).mean(axis=0).round(4)\n",
    "# }).sort_values(by='mean_abs_shap', ascending=False)\n",
    "# shap_df.to_csv('importance-shap.csv', index=False)\n",
    "\n",
    "# print(np.vstack(x_test_list).shape)\n",
    "\n",
    "# shap.summary_plot(shap_values, pd.DataFrame(x_test, columns=feature_names), max_display=16)\n",
    "# shap.summary_plot(np.vstack(shap_list), pd.DataFrame(np.vstack(x_test_list), columns=feature_names), max_display=16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10563     3     0     0]\n",
      " [  117     0     0     0]\n",
      " [   89     0     0     0]\n",
      " [   18     0     0     0]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auc-roc-macro</th>\n",
       "      <th>auc-roc-weighted</th>\n",
       "      <th>auc-pr-macro</th>\n",
       "      <th>auc-pr-weighted</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>cm-0</th>\n",
       "      <th>cm-1</th>\n",
       "      <th>cm-2</th>\n",
       "      <th>cm-3</th>\n",
       "      <th>...</th>\n",
       "      <th>cm-6</th>\n",
       "      <th>cm-7</th>\n",
       "      <th>cm-8</th>\n",
       "      <th>cm-9</th>\n",
       "      <th>cm-10</th>\n",
       "      <th>cm-11</th>\n",
       "      <th>cm-12</th>\n",
       "      <th>cm-13</th>\n",
       "      <th>cm-14</th>\n",
       "      <th>cm-15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8657</td>\n",
       "      <td>0.8434</td>\n",
       "      <td>0.3027</td>\n",
       "      <td>0.9719</td>\n",
       "      <td>0.9790</td>\n",
       "      <td>[10563 3 0 0]\\n[117 0 0 0]\\n[89 0 0 0]\\n[18 0 ...</td>\n",
       "      <td>10563</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  auc-roc-macro auc-roc-weighted auc-pr-macro auc-pr-weighted accuracy  \\\n",
       "0        0.8657           0.8434       0.3027          0.9719   0.9790   \n",
       "\n",
       "                                    confusion_matrix   cm-0 cm-1 cm-2 cm-3  \\\n",
       "0  [10563 3 0 0]\\n[117 0 0 0]\\n[89 0 0 0]\\n[18 0 ...  10563    3    0    0   \n",
       "\n",
       "   ... cm-6 cm-7 cm-8 cm-9 cm-10 cm-11 cm-12 cm-13 cm-14 cm-15  \n",
       "0  ...    0    0   89    0     0     0    18     0     0     0  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Transformer\n",
    "\n",
    "if not weighted:\n",
    "\n",
    "    name = 'Transformer'.lower()\n",
    "\n",
    "    Y_TRUES = np.empty([0])\n",
    "    Y_PROBS = []\n",
    "    Y_PREDS = np.empty([0])\n",
    "\n",
    "\n",
    "    if kfold:\n",
    "        cv = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "        split_iterator = cv.split(x)\n",
    "    else:\n",
    "        cv = LeavePGroupsOut(n_groups=1)\n",
    "        split_iterator = cv.split(x, y, groups=p)\n",
    "\n",
    "    for i, (train_idx, test_idx) in enumerate(split_iterator, start=1):\n",
    "        participant = i if kfold else np.unique(p[test_idx])[0]\n",
    "\n",
    "        x_train, x_test = x[train_idx], x[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        print(participant, x_train.shape[0], x_test.shape[0])\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(x_train)\n",
    "        x_train = scaler.transform(x_train)\n",
    "        x_test = scaler.transform(x_test)\n",
    "\n",
    "        normalizer = MinMaxScaler()\n",
    "        normalizer.fit(x_train)\n",
    "        x_train = normalizer.transform(x_train)\n",
    "        x_test = normalizer.transform(x_test)\n",
    "\n",
    "        # Oversample class 1\n",
    "\n",
    "        # ros = RandomOverSampler(sampling_strategy=.1, random_state=seed)\n",
    "        # x_train, y_train = ros.fit_resample(x_train, y_train)\n",
    "\n",
    "        # # 1. Check original class distribution\n",
    "        # print(\"Original class distribution:\", Counter(y_train))\n",
    "        # # 2. Define the minority class (adjust if needed)\n",
    "        # minority_class = 1  # change this if your minority class label is different\n",
    "        # current_minority_count = sum(y_train == minority_class)\n",
    "        # # 3. Define desired new total count for the minority class (10x)\n",
    "        # target_minority_count = current_minority_count * 10\n",
    "        # # 4. Setup SMOTE with custom sampling strategy\n",
    "        # smote = SMOTE(sampling_strategy={minority_class: target_minority_count}, random_state=seed)\n",
    "        # # 5. Fit and resample\n",
    "        # x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "        # # 6. Confirm new distribution\n",
    "        # print(\"Resampled class distribution:\", Counter(y_train))\n",
    "\n",
    "\n",
    "\n",
    "        # ----- LightGBM\n",
    "        # train_data = lgb.Dataset(x_train, label=y_train)\n",
    "        # test_data = lgb.Dataset(x_test, label=y_test, reference=train_data)\n",
    "        # params = {\n",
    "        #     'verbose': -1,  # 👈 turn off training output\n",
    "        #     'objective': 'binary',\n",
    "        #     'metric': 'binary_logloss',  # or 'auc' if you prefer\n",
    "        #     'num_leaves': 64,\n",
    "        #     'learning_rate': 0.01,\n",
    "        #     'n_estimators': 100,\n",
    "        #     # 'is_unbalance': True  # Automatically balances positive and negative classes\n",
    "        #     # 'scale_pos_weight': (y_train == 0).sum() / (y_train == 1).sum()\n",
    "        # }\n",
    "        # bst = lgb.train(params, train_data, valid_sets=[train_data, test_data])\n",
    "        # y_probs = bst.predict(x_test, num_iteration=bst.best_iteration)\n",
    "\n",
    "\n",
    "        # ----- Transformer\n",
    "        model = TabPFNClassifier(\n",
    "            device='cuda',\n",
    "            random_state=seed,\n",
    "            n_estimators = 4,\n",
    "            ignore_pretraining_limits=True)\n",
    "        model.fit(x_train, y_train)\n",
    "        y_probs = model.predict_proba(x_test)\n",
    "\n",
    "\n",
    "        Y_TRUES = np.append(Y_TRUES, y_test)\n",
    "        Y_PROBS.append(y_probs)\n",
    "        Y_PREDS = np.append(Y_PREDS, np.argmax(y_probs, axis=1))\n",
    "\n",
    "    Y_PROBS = np.concatenate(Y_PROBS, axis=0)\n",
    "\n",
    "\n",
    "    indx = Y_TRUES.argsort()\n",
    "    Y_TRUES = Y_TRUES[indx]\n",
    "    Y_PROBS = Y_PROBS[indx]\n",
    "    Y_PREDS = Y_PREDS[indx]\n",
    "\n",
    "\n",
    "# Evaluation metrics (multiclass)\n",
    "auc_roc_macro = roc_auc_score(Y_TRUES, Y_PROBS, multi_class='ovr', average='macro')\n",
    "auc_roc_weighted = roc_auc_score(Y_TRUES, Y_PROBS, multi_class='ovr', average='weighted')\n",
    "auc_pr_macro = average_precision_score(Y_TRUES, Y_PROBS, average='macro')\n",
    "auc_pr_weighted = average_precision_score(Y_TRUES, Y_PROBS, average='weighted')\n",
    "acc = accuracy_score(Y_TRUES, Y_PREDS)\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(Y_TRUES, Y_PREDS)\n",
    "print(cm)\n",
    "cm_str = '\\n'.join(['[' + ' '.join(map(str, row)) + ']' for row in cm])\n",
    "\n",
    "cm_flat = cm.flatten()\n",
    "\n",
    "results = {\n",
    "    'auc-roc-macro': f\"{auc_roc_macro:.4f}\",\n",
    "    'auc-roc-weighted': f\"{auc_roc_weighted:.4f}\",\n",
    "    'auc-pr-macro': f\"{auc_pr_macro:.4f}\",\n",
    "    'auc-pr-weighted': f\"{auc_pr_weighted:.4f}\",\n",
    "    'accuracy': f\"{acc:.4f}\",\n",
    "    'confusion_matrix': cm_str\n",
    "}\n",
    "\n",
    "for i, val in enumerate(cm_flat):\n",
    "    results[f'cm-{i}'] = str(int(val))\n",
    "\n",
    "results = pd.DataFrame([results])\n",
    "results.to_csv(os.path.join(results_dir, name + '.csv'), index=False)\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10779 11\n",
      "1 10617 173\n",
      "2 10488 302\n",
      "3 10599 191\n",
      "4 10604 186\n",
      "5 10767 23\n",
      "6 10437 353\n",
      "7 10541 249\n",
      "8 10500 290\n",
      "9 10611 179\n",
      "10 10771 19\n",
      "11 10560 230\n",
      "12 10532 258\n",
      "13 10716 74\n",
      "14 10524 266\n",
      "15 10548 242\n",
      "16 10665 125\n",
      "17 10508 282\n",
      "18 10668 122\n",
      "19 10570 220\n",
      "20 10588 202\n",
      "21 10521 269\n",
      "22 10627 163\n",
      "23 10600 190\n",
      "24 10771 19\n",
      "25 10520 270\n",
      "26 10495 295\n",
      "27 10520 270\n",
      "28 10605 185\n",
      "29 10616 174\n",
      "30 10576 214\n",
      "31 10556 234\n",
      "32 10687 103\n",
      "33 10560 230\n",
      "34 10625 165\n",
      "35 10751 39\n",
      "36 10525 265\n",
      "37 10436 354\n",
      "38 10621 169\n",
      "39 10463 327\n",
      "40 10524 266\n",
      "41 10584 206\n",
      "42 10632 158\n",
      "43 10520 270\n",
      "44 10728 62\n",
      "45 10699 91\n",
      "46 10763 27\n",
      "47 10580 210\n",
      "48 10672 118\n",
      "49 10528 262\n",
      "50 10627 163\n",
      "51 10541 249\n",
      "52 10502 288\n",
      "53 10575 215\n",
      "54 10532 258\n",
      "55 10775 15\n",
      "[[8793  607 1023  143]\n",
      " [  24   66   13   14]\n",
      " [  31   20   29    9]\n",
      " [   3    8    3    4]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auc-roc-macro</th>\n",
       "      <th>auc-roc-weighted</th>\n",
       "      <th>auc-pr-macro</th>\n",
       "      <th>auc-pr-weighted</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>cm-0</th>\n",
       "      <th>cm-1</th>\n",
       "      <th>cm-2</th>\n",
       "      <th>cm-3</th>\n",
       "      <th>...</th>\n",
       "      <th>cm-6</th>\n",
       "      <th>cm-7</th>\n",
       "      <th>cm-8</th>\n",
       "      <th>cm-9</th>\n",
       "      <th>cm-10</th>\n",
       "      <th>cm-11</th>\n",
       "      <th>cm-12</th>\n",
       "      <th>cm-13</th>\n",
       "      <th>cm-14</th>\n",
       "      <th>cm-15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.7645</td>\n",
       "      <td>0.8288</td>\n",
       "      <td>0.2809</td>\n",
       "      <td>0.9735</td>\n",
       "      <td>0.8241</td>\n",
       "      <td>[8793 607 1023 143]\\n[24 66 13 14]\\n[31 20 29 ...</td>\n",
       "      <td>8793</td>\n",
       "      <td>607</td>\n",
       "      <td>1023</td>\n",
       "      <td>143</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>31</td>\n",
       "      <td>20</td>\n",
       "      <td>29</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  auc-roc-macro auc-roc-weighted auc-pr-macro auc-pr-weighted accuracy  \\\n",
       "0        0.7645           0.8288       0.2809          0.9735   0.8241   \n",
       "\n",
       "                                    confusion_matrix  cm-0 cm-1  cm-2 cm-3  \\\n",
       "0  [8793 607 1023 143]\\n[24 66 13 14]\\n[31 20 29 ...  8793  607  1023  143   \n",
       "\n",
       "   ... cm-6 cm-7 cm-8 cm-9 cm-10 cm-11 cm-12 cm-13 cm-14 cm-15  \n",
       "0  ...   13   14   31   20    29     9     3     8     3     4  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gradient Boosting Classifier\n",
    "\n",
    "name = 'Gradient-Boosting'.lower()\n",
    "\n",
    "Y_TRUES = np.empty([0])\n",
    "Y_PROBS = []\n",
    "Y_PREDS = np.empty([0])\n",
    "\n",
    "\n",
    "if kfold:\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    split_iterator = cv.split(x)\n",
    "else:\n",
    "    cv = LeavePGroupsOut(n_groups=1)\n",
    "    split_iterator = cv.split(x, y, groups=p)\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(split_iterator, start=1):\n",
    "    participant = i if kfold else np.unique(p[test_idx])[0]\n",
    "\n",
    "    x_train, x_test = x[train_idx], x[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    print(participant, x_train.shape[0], x_test.shape[0])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    normalizer = MinMaxScaler()\n",
    "    normalizer.fit(x_train)\n",
    "    x_train = normalizer.transform(x_train)\n",
    "    x_test = normalizer.transform(x_test)\n",
    "\n",
    "    # Oversample class 1\n",
    "\n",
    "    # ros = RandomOverSampler(sampling_strategy=.1, random_state=seed)\n",
    "    # x_train, y_train = ros.fit_resample(x_train, y_train)\n",
    "\n",
    "    # # 1. Check original class distribution\n",
    "    # print(\"Original class distribution:\", Counter(y_train))\n",
    "    # # 2. Define the minority class (adjust if needed)\n",
    "    # minority_class = 1  # change this if your minority class label is different\n",
    "    # current_minority_count = sum(y_train == minority_class)\n",
    "    # # 3. Define desired new total count for the minority class (10x)\n",
    "    # target_minority_count = current_minority_count * 10\n",
    "    # # 4. Setup SMOTE with custom sampling strategy\n",
    "    # smote = SMOTE(sampling_strategy={minority_class: target_minority_count}, random_state=seed)\n",
    "    # # 5. Fit and resample\n",
    "    # x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "    # # 6. Confirm new distribution\n",
    "    # print(\"Resampled class distribution:\", Counter(y_train))\n",
    "\n",
    "\n",
    "    # ----- LightGBM\n",
    "    # train_data = lgb.Dataset(x_train, label=y_train)\n",
    "    # test_data = lgb.Dataset(x_test, label=y_test, reference=train_data)\n",
    "    # params = {\n",
    "    #     'verbose': -1,  # 👈 turn off training output\n",
    "    #     'objective': 'binary',\n",
    "    #     'metric': 'binary_logloss',  # or 'auc' if you prefer\n",
    "    #     'num_leaves': 64,\n",
    "    #     'learning_rate': 0.01,\n",
    "    #     'n_estimators': 100,\n",
    "    #     # 'is_unbalance': True  # Automatically balances positive and negative classes\n",
    "    #     # 'scale_pos_weight': (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    # }\n",
    "    # bst = lgb.train(params, train_data, valid_sets=[train_data, test_data])\n",
    "    # y_probs = bst.predict(x_test, num_iteration=bst.best_iteration)\n",
    "\n",
    "\n",
    "    # ----- Transformer\n",
    "    # model = TabPFNClassifier(\n",
    "    #     device='cuda',\n",
    "    #     random_state=seed,\n",
    "    #     n_estimators = 4,\n",
    "    #     ignore_pretraining_limits=True)\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- Gradient Boosting Classifier\n",
    "    model = GradientBoostingClassifier(\n",
    "        n_estimators=100,  # match LightGBM\n",
    "        learning_rate=0.001,  # match LightGBM\n",
    "        max_depth=4,  # similar to LightGBM default tree depth\n",
    "        subsample=1.0,  # default\n",
    "        random_state=seed\n",
    "    )\n",
    "    if weighted:\n",
    "        model.fit(x_train, y_train, sample_weight=compute_sample_weight(class_weight='balanced', y=y_train))\n",
    "    else:\n",
    "        model.fit(x_train, y_train)\n",
    "    \n",
    "    y_probs = model.predict_proba(x_test)\n",
    "    y_preds = model.predict(x_test)\n",
    "    \n",
    "\n",
    "    Y_TRUES = np.append(Y_TRUES, y_test)\n",
    "    Y_PROBS.append(y_probs)\n",
    "    Y_PREDS = np.append(Y_PREDS, y_preds)\n",
    "\n",
    "Y_PROBS = np.concatenate(Y_PROBS, axis=0)\n",
    "\n",
    "\n",
    "indx = Y_TRUES.argsort()\n",
    "Y_TRUES = Y_TRUES[indx]\n",
    "Y_PROBS = Y_PROBS[indx]\n",
    "Y_PREDS = Y_PREDS[indx]\n",
    "\n",
    "\n",
    "# Evaluation metrics (multiclass)\n",
    "auc_roc_macro = roc_auc_score(Y_TRUES, Y_PROBS, multi_class='ovr', average='macro')\n",
    "auc_roc_weighted = roc_auc_score(Y_TRUES, Y_PROBS, multi_class='ovr', average='weighted')\n",
    "auc_pr_macro = average_precision_score(Y_TRUES, Y_PROBS, average='macro')\n",
    "auc_pr_weighted = average_precision_score(Y_TRUES, Y_PROBS, average='weighted')\n",
    "acc = accuracy_score(Y_TRUES, Y_PREDS)\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(Y_TRUES, Y_PREDS)\n",
    "print(cm)\n",
    "cm_str = '\\n'.join(['[' + ' '.join(map(str, row)) + ']' for row in cm])\n",
    "\n",
    "cm_flat = cm.flatten()\n",
    "\n",
    "results = {\n",
    "    'auc-roc-macro': f\"{auc_roc_macro:.4f}\",\n",
    "    'auc-roc-weighted': f\"{auc_roc_weighted:.4f}\",\n",
    "    'auc-pr-macro': f\"{auc_pr_macro:.4f}\",\n",
    "    'auc-pr-weighted': f\"{auc_pr_weighted:.4f}\",\n",
    "    'accuracy': f\"{acc:.4f}\",\n",
    "    'confusion_matrix': cm_str\n",
    "}\n",
    "\n",
    "for i, val in enumerate(cm_flat):\n",
    "    results[f'cm-{i}'] = str(int(val))\n",
    "\n",
    "results = pd.DataFrame([results])\n",
    "results.to_csv(os.path.join(results_dir, name + '.csv'), index=False)\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10779 11\n",
      "1 10617 173\n",
      "2 10488 302\n",
      "3 10599 191\n",
      "4 10604 186\n",
      "5 10767 23\n",
      "6 10437 353\n",
      "7 10541 249\n",
      "8 10500 290\n",
      "9 10611 179\n",
      "10 10771 19\n",
      "11 10560 230\n",
      "12 10532 258\n",
      "13 10716 74\n",
      "14 10524 266\n",
      "15 10548 242\n",
      "16 10665 125\n",
      "17 10508 282\n",
      "18 10668 122\n",
      "19 10570 220\n",
      "20 10588 202\n",
      "21 10521 269\n",
      "22 10627 163\n",
      "23 10600 190\n",
      "24 10771 19\n",
      "25 10520 270\n",
      "26 10495 295\n",
      "27 10520 270\n",
      "28 10605 185\n",
      "29 10616 174\n",
      "30 10576 214\n",
      "31 10556 234\n",
      "32 10687 103\n",
      "33 10560 230\n",
      "34 10625 165\n",
      "35 10751 39\n",
      "36 10525 265\n",
      "37 10436 354\n",
      "38 10621 169\n",
      "39 10463 327\n",
      "40 10524 266\n",
      "41 10584 206\n",
      "42 10632 158\n",
      "43 10520 270\n",
      "44 10728 62\n",
      "45 10699 91\n",
      "46 10763 27\n",
      "47 10580 210\n",
      "48 10672 118\n",
      "49 10528 262\n",
      "50 10627 163\n",
      "51 10541 249\n",
      "52 10502 288\n",
      "53 10575 215\n",
      "54 10532 258\n",
      "55 10775 15\n",
      "[[8156  743 1453  214]\n",
      " [  14   66   23   14]\n",
      " [  16   27   34   12]\n",
      " [   0   10    2    6]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auc-roc-macro</th>\n",
       "      <th>auc-roc-weighted</th>\n",
       "      <th>auc-pr-macro</th>\n",
       "      <th>auc-pr-weighted</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>cm-0</th>\n",
       "      <th>cm-1</th>\n",
       "      <th>cm-2</th>\n",
       "      <th>cm-3</th>\n",
       "      <th>...</th>\n",
       "      <th>cm-6</th>\n",
       "      <th>cm-7</th>\n",
       "      <th>cm-8</th>\n",
       "      <th>cm-9</th>\n",
       "      <th>cm-10</th>\n",
       "      <th>cm-11</th>\n",
       "      <th>cm-12</th>\n",
       "      <th>cm-13</th>\n",
       "      <th>cm-14</th>\n",
       "      <th>cm-15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8354</td>\n",
       "      <td>0.8858</td>\n",
       "      <td>0.2991</td>\n",
       "      <td>0.9777</td>\n",
       "      <td>0.7657</td>\n",
       "      <td>[8156 743 1453 214]\\n[14 66 23 14]\\n[16 27 34 ...</td>\n",
       "      <td>8156</td>\n",
       "      <td>743</td>\n",
       "      <td>1453</td>\n",
       "      <td>214</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>27</td>\n",
       "      <td>34</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  auc-roc-macro auc-roc-weighted auc-pr-macro auc-pr-weighted accuracy  \\\n",
       "0        0.8354           0.8858       0.2991          0.9777   0.7657   \n",
       "\n",
       "                                    confusion_matrix  cm-0 cm-1  cm-2 cm-3  \\\n",
       "0  [8156 743 1453 214]\\n[14 66 23 14]\\n[16 27 34 ...  8156  743  1453  214   \n",
       "\n",
       "   ... cm-6 cm-7 cm-8 cm-9 cm-10 cm-11 cm-12 cm-13 cm-14 cm-15  \n",
       "0  ...   23   14   16   27    34    12     0    10     2     6  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "\n",
    "name = 'Logistic-Regression'.lower()\n",
    "\n",
    "Y_TRUES = np.empty([0])\n",
    "Y_PROBS = []\n",
    "Y_PREDS = np.empty([0])\n",
    "\n",
    "\n",
    "if kfold:\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    split_iterator = cv.split(x)\n",
    "else:\n",
    "    cv = LeavePGroupsOut(n_groups=1)\n",
    "    split_iterator = cv.split(x, y, groups=p)\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(split_iterator, start=1):\n",
    "    participant = i if kfold else np.unique(p[test_idx])[0]\n",
    "\n",
    "    x_train, x_test = x[train_idx], x[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    print(participant, x_train.shape[0], x_test.shape[0])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    normalizer = MinMaxScaler()\n",
    "    normalizer.fit(x_train)\n",
    "    x_train = normalizer.transform(x_train)\n",
    "    x_test = normalizer.transform(x_test)\n",
    "\n",
    "    # Oversample class 1\n",
    "\n",
    "    # ros = RandomOverSampler(sampling_strategy=.1, random_state=seed)\n",
    "    # x_train, y_train = ros.fit_resample(x_train, y_train)\n",
    "\n",
    "    # 1. Check original class distribution\n",
    "    # print(\"Original class distribution:\", Counter(y_train))\n",
    "    # # 2. Define the minority class (adjust if needed)\n",
    "    # minority_class = 1  # change this if your minority class label is different\n",
    "    # current_minority_count = sum(y_train == minority_class)\n",
    "    # # 3. Define desired new total count for the minority class (10x)\n",
    "    # target_minority_count = current_minority_count * 10\n",
    "    # # 4. Setup SMOTE with custom sampling strategy\n",
    "    # smote = SMOTE(sampling_strategy={minority_class: target_minority_count}, random_state=seed)\n",
    "    # # 5. Fit and resample\n",
    "    # x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "    # # 6. Confirm new distribution\n",
    "    # print(\"Resampled class distribution:\", Counter(y_train))\n",
    "\n",
    "\n",
    "    # Undersample class 0\n",
    "    # reduction_percent = 99\n",
    "    # print(\"Original class distribution:\", Counter(y_train))    \n",
    "    # class_0_idx = np.where(y_train == 0)[0]\n",
    "    # class_1_idx = np.where(y_train == 1)[0]\n",
    "    # # How many class 0 samples to keep\n",
    "    # n_keep = int(len(class_0_idx) * (1 - reduction_percent / 100.0))\n",
    "    # sampled_0_idx = np.random.choice(class_0_idx, size=n_keep, replace=False)\n",
    "    # # Combine sampled class 0 with all class 1\n",
    "    # final_idx = np.concatenate([sampled_0_idx, class_1_idx])\n",
    "    # x_train = x_train[final_idx]\n",
    "    # y_train =  y_train[final_idx]\n",
    "    # print(\"Resampled class distribution:\", Counter(y_train))\n",
    "\n",
    "\n",
    "    # ----- LightGBM\n",
    "    # train_data = lgb.Dataset(x_train, label=y_train)\n",
    "    # test_data = lgb.Dataset(x_test, label=y_test, reference=train_data)\n",
    "    # params = {\n",
    "    #     'verbose': -1,  # 👈 turn off training output\n",
    "    #     'objective': 'binary',\n",
    "    #     'metric': 'binary_logloss',  # or 'auc' if you prefer\n",
    "    #     'num_leaves': 64,\n",
    "    #     'learning_rate': 0.01,\n",
    "    #     'n_estimators': 100,\n",
    "    #     # 'is_unbalance': True  # Automatically balances positive and negative classes\n",
    "    #     # 'scale_pos_weight': (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    # }\n",
    "    # bst = lgb.train(params, train_data, valid_sets=[train_data, test_data])\n",
    "    # y_probs = bst.predict(x_test, num_iteration=bst.best_iteration)\n",
    "\n",
    "\n",
    "    # ----- Transformer\n",
    "    # model = TabPFNClassifier(\n",
    "    #     device='cuda',\n",
    "    #     random_state=seed,\n",
    "    #     n_estimators = 4,\n",
    "    #     ignore_pretraining_limits=True)\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- Gradient Boosting Classifier\n",
    "    # model = GradientBoostingClassifier(\n",
    "    #     n_estimators=100,  # match LightGBM\n",
    "    #     learning_rate=0.001,  # match LightGBM\n",
    "    #     max_depth=4,  # similar to LightGBM default tree depth\n",
    "    #     subsample=1.0,  # default\n",
    "    #     random_state=seed\n",
    "    # )\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "    \n",
    "\n",
    "    # ----- XGBoost\n",
    "    # model = XGBClassifier()\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- Random Forest\n",
    "    # model = RandomForestClassifier(\n",
    "    #     n_estimators=250,  # number of trees\n",
    "    #     max_depth=None,  # let the trees grow fully\n",
    "    #     random_state=seed\n",
    "    # )\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- SVM\n",
    "    # model = SVC(kernel='rbf', probability=True, random_state=seed)\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- DT\n",
    "    # model = DecisionTreeClassifier(criterion=\"log_loss\", max_depth=512, random_state=seed)\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict(x_test)\n",
    "\n",
    "\n",
    "    # MLP\n",
    "    # model = MLPClassifier(\n",
    "    #     hidden_layer_sizes=(128, 64),    # 2 hidden layers: 64 and 32 neurons\n",
    "    #     activation='relu',              # good default: 'relu'\n",
    "    #     solver='adam',                  # optimizer\n",
    "    #     alpha=0.0001,                   # L2 regularization\n",
    "    #     learning_rate_init=0.001,\n",
    "    #     max_iter=500,\n",
    "    #     early_stopping=True,\n",
    "    #     random_state=seed,\n",
    "    #     verbose=True,\n",
    "    # )\n",
    "    # model.fit(x_train, y_train)    \n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "    # y_preds = model.predict(x_test)\n",
    "\n",
    "\n",
    "    # LR\n",
    "    model = LogisticRegression(\n",
    "    penalty='l2',               # regularization (default)\n",
    "    C=1.0,                      # inverse of regularization strength\n",
    "    solver='lbfgs',             # optimizer (good default for small/medium data)\n",
    "    max_iter=1000,\n",
    "\n",
    "    class_weight='balanced' if weighted else None,\n",
    "\n",
    "    random_state=seed)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_probs = model.predict_proba(x_test)\n",
    "    y_preds = model.predict(x_test)\n",
    "\n",
    "\n",
    "    Y_TRUES = np.append(Y_TRUES, y_test)\n",
    "    Y_PROBS.append(y_probs)\n",
    "    Y_PREDS = np.append(Y_PREDS, y_preds)\n",
    "\n",
    "Y_PROBS = np.concatenate(Y_PROBS, axis=0)\n",
    "\n",
    "\n",
    "indx = Y_TRUES.argsort()\n",
    "Y_TRUES = Y_TRUES[indx]\n",
    "Y_PROBS = Y_PROBS[indx]\n",
    "Y_PREDS = Y_PREDS[indx]\n",
    "\n",
    "\n",
    "# Evaluation metrics (multiclass)\n",
    "auc_roc_macro = roc_auc_score(Y_TRUES, Y_PROBS, multi_class='ovr', average='macro')\n",
    "auc_roc_weighted = roc_auc_score(Y_TRUES, Y_PROBS, multi_class='ovr', average='weighted')\n",
    "auc_pr_macro = average_precision_score(Y_TRUES, Y_PROBS, average='macro')\n",
    "auc_pr_weighted = average_precision_score(Y_TRUES, Y_PROBS, average='weighted')\n",
    "acc = accuracy_score(Y_TRUES, Y_PREDS)\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(Y_TRUES, Y_PREDS)\n",
    "print(cm)\n",
    "cm_str = '\\n'.join(['[' + ' '.join(map(str, row)) + ']' for row in cm])\n",
    "\n",
    "cm_flat = cm.flatten()\n",
    "\n",
    "results = {\n",
    "    'auc-roc-macro': f\"{auc_roc_macro:.4f}\",\n",
    "    'auc-roc-weighted': f\"{auc_roc_weighted:.4f}\",\n",
    "    'auc-pr-macro': f\"{auc_pr_macro:.4f}\",\n",
    "    'auc-pr-weighted': f\"{auc_pr_weighted:.4f}\",\n",
    "    'accuracy': f\"{acc:.4f}\",\n",
    "    'confusion_matrix': cm_str\n",
    "}\n",
    "\n",
    "for i, val in enumerate(cm_flat):\n",
    "    results[f'cm-{i}'] = str(int(val))\n",
    "\n",
    "results = pd.DataFrame([results])\n",
    "results.to_csv(os.path.join(results_dir, name + '.csv'), index=False)\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10779 11\n",
      "1 10617 173\n",
      "2 10488 302\n",
      "3 10599 191\n",
      "4 10604 186\n",
      "5 10767 23\n",
      "6 10437 353\n",
      "7 10541 249\n",
      "8 10500 290\n",
      "9 10611 179\n",
      "10 10771 19\n",
      "11 10560 230\n",
      "12 10532 258\n",
      "13 10716 74\n",
      "14 10524 266\n",
      "15 10548 242\n",
      "16 10665 125\n",
      "17 10508 282\n",
      "18 10668 122\n",
      "19 10570 220\n",
      "20 10588 202\n",
      "21 10521 269\n",
      "22 10627 163\n",
      "23 10600 190\n",
      "24 10771 19\n",
      "25 10520 270\n",
      "26 10495 295\n",
      "27 10520 270\n",
      "28 10605 185\n",
      "29 10616 174\n",
      "30 10576 214\n",
      "31 10556 234\n",
      "32 10687 103\n",
      "33 10560 230\n",
      "34 10625 165\n",
      "35 10751 39\n",
      "36 10525 265\n",
      "37 10436 354\n",
      "38 10621 169\n",
      "39 10463 327\n",
      "40 10524 266\n",
      "41 10584 206\n",
      "42 10632 158\n",
      "43 10520 270\n",
      "44 10728 62\n",
      "45 10699 91\n",
      "46 10763 27\n",
      "47 10580 210\n",
      "48 10672 118\n",
      "49 10528 262\n",
      "50 10627 163\n",
      "51 10541 249\n",
      "52 10502 288\n",
      "53 10575 215\n",
      "54 10532 258\n",
      "55 10775 15\n",
      "[[7178 1329 1805  254]\n",
      " [  13   69   15   20]\n",
      " [  12   23   37   17]\n",
      " [   0    9    3    6]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auc-roc-macro</th>\n",
       "      <th>auc-roc-weighted</th>\n",
       "      <th>auc-pr-macro</th>\n",
       "      <th>auc-pr-weighted</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>cm-0</th>\n",
       "      <th>cm-1</th>\n",
       "      <th>cm-2</th>\n",
       "      <th>cm-3</th>\n",
       "      <th>...</th>\n",
       "      <th>cm-6</th>\n",
       "      <th>cm-7</th>\n",
       "      <th>cm-8</th>\n",
       "      <th>cm-9</th>\n",
       "      <th>cm-10</th>\n",
       "      <th>cm-11</th>\n",
       "      <th>cm-12</th>\n",
       "      <th>cm-13</th>\n",
       "      <th>cm-14</th>\n",
       "      <th>cm-15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8158</td>\n",
       "      <td>0.8632</td>\n",
       "      <td>0.2720</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>0.6756</td>\n",
       "      <td>[7178 1329 1805 254]\\n[13 69 15 20]\\n[12 23 37...</td>\n",
       "      <td>7178</td>\n",
       "      <td>1329</td>\n",
       "      <td>1805</td>\n",
       "      <td>254</td>\n",
       "      <td>...</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>37</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  auc-roc-macro auc-roc-weighted auc-pr-macro auc-pr-weighted accuracy  \\\n",
       "0        0.8158           0.8632       0.2720          0.9761   0.6756   \n",
       "\n",
       "                                    confusion_matrix  cm-0  cm-1  cm-2 cm-3  \\\n",
       "0  [7178 1329 1805 254]\\n[13 69 15 20]\\n[12 23 37...  7178  1329  1805  254   \n",
       "\n",
       "   ... cm-6 cm-7 cm-8 cm-9 cm-10 cm-11 cm-12 cm-13 cm-14 cm-15  \n",
       "0  ...   15   20   12   23    37    17     0     9     3     6  \n",
       "\n",
       "[1 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "\n",
    "name = 'Naive-Bayes'.lower()\n",
    "\n",
    "Y_TRUES = np.empty([0])\n",
    "Y_PROBS = []\n",
    "Y_PREDS = np.empty([0])\n",
    "\n",
    "\n",
    "if kfold:\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    split_iterator = cv.split(x)\n",
    "else:\n",
    "    cv = LeavePGroupsOut(n_groups=1)\n",
    "    split_iterator = cv.split(x, y, groups=p)\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(split_iterator, start=1):\n",
    "    participant = i if kfold else np.unique(p[test_idx])[0]\n",
    "\n",
    "    x_train, x_test = x[train_idx], x[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    print(participant, x_train.shape[0], x_test.shape[0])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    normalizer = MinMaxScaler()\n",
    "    normalizer.fit(x_train)\n",
    "    x_train = normalizer.transform(x_train)\n",
    "    x_test = normalizer.transform(x_test)\n",
    "\n",
    "    # Oversample class 1\n",
    "\n",
    "    # ros = RandomOverSampler(sampling_strategy=.1, random_state=seed)\n",
    "    # x_train, y_train = ros.fit_resample(x_train, y_train)\n",
    "\n",
    "    # # 1. Check original class distribution\n",
    "    # print(\"Original class distribution:\", Counter(y_train))\n",
    "    # # 2. Define the minority class (adjust if needed)\n",
    "    # minority_class = 1  # change this if your minority class label is different\n",
    "    # current_minority_count = sum(y_train == minority_class)\n",
    "    # # 3. Define desired new total count for the minority class (10x)\n",
    "    # target_minority_count = current_minority_count * 10\n",
    "    # # 4. Setup SMOTE with custom sampling strategy\n",
    "    # smote = SMOTE(sampling_strategy={minority_class: target_minority_count}, random_state=seed)\n",
    "    # # 5. Fit and resample\n",
    "    # x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "    # # 6. Confirm new distribution\n",
    "    # print(\"Resampled class distribution:\", Counter(y_train))\n",
    "\n",
    "\n",
    "    # Undersample class 0\n",
    "    # data_percent = 1\n",
    "    # reduction_percent = 100 - data_percent\n",
    "    # print(\"Original class distribution:\", Counter(y_train))    \n",
    "    # class_0_idx = np.where(y_train == 0)[0]\n",
    "    # class_1_idx = np.where(y_train == 1)[0]\n",
    "    # # How many class 0 samples to keep\n",
    "    # n_keep = int(len(class_0_idx) * (1 - reduction_percent / 100.0))\n",
    "    # sampled_0_idx = np.random.choice(class_0_idx, size=n_keep, replace=False)\n",
    "    # # Combine sampled class 0 with all class 1\n",
    "    # final_idx = np.concatenate([sampled_0_idx, class_1_idx])\n",
    "    # x_train = x_train[final_idx]\n",
    "    # y_train =  y_train[final_idx]\n",
    "    # print(\"Resampled class distribution:\", Counter(y_train))\n",
    "\n",
    "\n",
    "    # ----- LightGBM\n",
    "    # train_data = lgb.Dataset(x_train, label=y_train)\n",
    "    # test_data = lgb.Dataset(x_test, label=y_test, reference=train_data)\n",
    "    # params = {\n",
    "    #     'verbose': -1,  # 👈 turn off training output\n",
    "    #     'objective': 'binary',\n",
    "    #     'metric': 'binary_logloss',  # or 'auc' if you prefer\n",
    "    #     'num_leaves': 64,\n",
    "    #     'learning_rate': 0.01,\n",
    "    #     'n_estimators': 100,\n",
    "    #     # 'is_unbalance': True  # Automatically balances positive and negative classes\n",
    "    #     # 'scale_pos_weight': (y_train == 0).sum() / (y_train == 1).sum()\n",
    "    # }\n",
    "    # bst = lgb.train(params, train_data, valid_sets=[train_data, test_data])\n",
    "    # y_probs = bst.predict(x_test, num_iteration=bst.best_iteration)\n",
    "\n",
    "\n",
    "    # ----- Transformer\n",
    "    # model = TabPFNClassifier(\n",
    "    #     device='cuda',\n",
    "    #     random_state=seed,\n",
    "    #     n_estimators = 4,\n",
    "    #     ignore_pretraining_limits=True)\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- Gradient Boosting Classifier\n",
    "    # model = GradientBoostingClassifier(\n",
    "    #     n_estimators=100,  # match LightGBM\n",
    "    #     learning_rate=0.001,  # match LightGBM\n",
    "    #     max_depth=4,  # similar to LightGBM default tree depth\n",
    "    #     subsample=1.0,  # default\n",
    "    #     random_state=seed\n",
    "    # )\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "    \n",
    "\n",
    "    # ----- XGBoost\n",
    "    # model = XGBClassifier()\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- Random Forest\n",
    "    # model = RandomForestClassifier(\n",
    "    #     n_estimators=250,  # number of trees\n",
    "    #     max_depth=None,  # let the trees grow fully\n",
    "    #     random_state=seed\n",
    "    # )\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- SVM\n",
    "    # model = SVC(kernel='rbf', probability=True, random_state=seed)\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "\n",
    "\n",
    "    # ----- DT\n",
    "    # model = DecisionTreeClassifier(criterion=\"log_loss\", max_depth=512, random_state=seed)\n",
    "    # model.fit(x_train, y_train)\n",
    "    # y_probs = model.predict(x_test)\n",
    "\n",
    "\n",
    "    # MLP\n",
    "    # model = MLPClassifier(\n",
    "    #     hidden_layer_sizes=(128, 64),    # 2 hidden layers: 64 and 32 neurons\n",
    "    #     activation='relu',              # good default: 'relu'\n",
    "    #     solver='adam',                  # optimizer\n",
    "    #     alpha=0.0001,                   # L2 regularization\n",
    "    #     learning_rate_init=0.001,\n",
    "    #     max_iter=500,\n",
    "    #     early_stopping=True,\n",
    "    #     random_state=seed,\n",
    "    #     verbose=True,\n",
    "    # )\n",
    "    # model.fit(x_train, y_train)    \n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "    # y_preds = model.predict(x_test)\n",
    "\n",
    "\n",
    "    # LR\n",
    "    # model = LogisticRegression(\n",
    "    # penalty='l2',               # regularization (default)\n",
    "    # C=1.0,                      # inverse of regularization strength\n",
    "    # solver='lbfgs',             # optimizer (good default for small/medium data)\n",
    "    # max_iter=1000,\n",
    "    # class_weight='balanced',\n",
    "    # random_state=seed)\n",
    "    # model.fit(x_train, y_train)    \n",
    "    # y_probs = model.predict_proba(x_test)[:, 1]\n",
    "    # y_preds = model.predict(x_test)\n",
    "\n",
    "\n",
    "    # NB\n",
    "    model = GaussianNB()\n",
    "    model.fit(x_train,\n",
    "              y_train,\n",
    "\n",
    "              sample_weight=compute_sample_weight(class_weight='balanced', y=y_train) if weighted else None,\n",
    "\n",
    "              )\n",
    "    y_probs = model.predict_proba(x_test)\n",
    "    y_preds = model.predict(x_test)\n",
    "\n",
    "\n",
    "    Y_TRUES = np.append(Y_TRUES, y_test)\n",
    "    Y_PROBS.append(y_probs)\n",
    "    Y_PREDS = np.append(Y_PREDS, y_preds)\n",
    "\n",
    "Y_PROBS = np.concatenate(Y_PROBS, axis=0)\n",
    "\n",
    "\n",
    "indx = Y_TRUES.argsort()\n",
    "Y_TRUES = Y_TRUES[indx]\n",
    "Y_PROBS = Y_PROBS[indx]\n",
    "Y_PREDS = Y_PREDS[indx]\n",
    "\n",
    "\n",
    "# Evaluation metrics (multiclass)\n",
    "auc_roc_macro = roc_auc_score(Y_TRUES, Y_PROBS, multi_class='ovr', average='macro')\n",
    "auc_roc_weighted = roc_auc_score(Y_TRUES, Y_PROBS, multi_class='ovr', average='weighted')\n",
    "auc_pr_macro = average_precision_score(Y_TRUES, Y_PROBS, average='macro')\n",
    "auc_pr_weighted = average_precision_score(Y_TRUES, Y_PROBS, average='weighted')\n",
    "acc = accuracy_score(Y_TRUES, Y_PREDS)\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(Y_TRUES, Y_PREDS)\n",
    "print(cm)\n",
    "cm_str = '\\n'.join(['[' + ' '.join(map(str, row)) + ']' for row in cm])\n",
    "\n",
    "cm_flat = cm.flatten()\n",
    "\n",
    "results = {\n",
    "    'auc-roc-macro': f\"{auc_roc_macro:.4f}\",\n",
    "    'auc-roc-weighted': f\"{auc_roc_weighted:.4f}\",\n",
    "    'auc-pr-macro': f\"{auc_pr_macro:.4f}\",\n",
    "    'auc-pr-weighted': f\"{auc_pr_weighted:.4f}\",\n",
    "    'accuracy': f\"{acc:.4f}\",\n",
    "    'confusion_matrix': cm_str\n",
    "}\n",
    "\n",
    "for i, val in enumerate(cm_flat):\n",
    "    results[f'cm-{i}'] = str(int(val))\n",
    "\n",
    "results = pd.DataFrame([results])\n",
    "results.to_csv(os.path.join(results_dir, name + '.csv'), index=False)\n",
    "display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine\n",
    "import utils_data\n",
    "importlib.reload(utils_data)\n",
    "from utils_data import combine_single_row_csvs\n",
    "\n",
    "\n",
    "combine_single_row_csvs(results_dir, os.path.join(results_dir,os.path.basename(results_dir)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ali-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
