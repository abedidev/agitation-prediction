{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3080 Ti Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "import importlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, confusion_matrix, f1_score, accuracy_score, \\\n",
    "    precision_score, recall_score\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import KFold, LeavePGroupsOut\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tabpfn import TabPFNClassifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "import shap\n",
    "from sktime.transformations.panel.rocket import Rocket\n",
    "\n",
    "# from catboost import CatBoostClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "# matplotlib.use('TkAgg')\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler  # install via: pip install imbalanced-learn\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "from scipy.stats import skew\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "seed = 69\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "timescale = '06h'\n",
    "\n",
    "segment_length = 10\n",
    "\n",
    "weighted = True                # weighted      unweighted\n",
    "kfold = True                   # kfold         lopo\n",
    "\n",
    "results_dir = '/home/ali/PycharmProjects/tihm/results/06h-10-binary-sequential-kfold-weighted-ctx-sta-phy'\n",
    "\n",
    "os.makedirs(results_dir, exist_ok=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10790, 69)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "root = '/home/ali/PycharmProjects/tihm/dataset'\n",
    "\n",
    "if timescale == '06h':\n",
    "    dataset = pd.read_csv(os.path.join(root, 'data-06h.csv'))\n",
    "elif timescale == '12h':\n",
    "    dataset = pd.read_csv(os.path.join(root, 'data-12h.csv'))\n",
    "elif timescale == '24h':\n",
    "    dataset = pd.read_csv(os.path.join(root, 'data-24h.csv'))\n",
    "\n",
    "display(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "135.0\n",
      "135.0\n",
      "(array([  117,   358,   528,   532,   536,   537,   552,   564,   568,\n",
      "         572,   597,   661,   669,   923,   935,   940,   944,   963,\n",
      "         967,   983,  1039,  1044,  1055,  1074,  1662,  2660,  2986,\n",
      "        3010,  3163,  4157,  4261,  4439,  4442,  4454,  4460,  4464,\n",
      "        4467,  4468,  4470,  4471,  4472,  4474,  4476,  4480,  4482,\n",
      "        4483,  4484,  4487,  4488,  4491,  4492,  4495,  4504,  4506,\n",
      "        4507,  4515,  4519,  4520,  4530,  4532,  4555,  4571,  4579,\n",
      "        4593,  4692,  4740,  4741,  4748,  4764,  4772,  4792,  4891,\n",
      "        5066,  5067,  5195,  5386,  5503,  5504,  5523,  5543,  5564,\n",
      "        5567,  5587,  5591,  5600,  5622,  5626,  5649,  5827,  5828,\n",
      "        6034,  6120,  6152,  6156,  6164,  6185,  6204,  7192,  7197,\n",
      "        7241,  7384,  7493,  7501,  7502,  7509,  7517,  7518,  7525,\n",
      "        7533,  7534,  7537,  7541,  7568,  8661,  8976,  8977,  9070,\n",
      "        9074,  9087,  9094,  9103,  9186,  9426,  9470,  9514,  9526,\n",
      "        9660,  9891,  9915,  9935, 10128, 10148, 10257, 10262, 10368]),)\n",
      "(array([  116,   357,   527,   531,   535,   536,   551,   563,   567,\n",
      "         571,   596,   660,   668,   922,   934,   939,   943,   962,\n",
      "         966,   982,  1038,  1043,  1054,  1073,  1661,  2659,  2985,\n",
      "        3009,  3162,  4156,  4260,  4438,  4441,  4453,  4459,  4463,\n",
      "        4466,  4467,  4469,  4470,  4471,  4473,  4475,  4479,  4481,\n",
      "        4482,  4483,  4486,  4487,  4490,  4491,  4494,  4503,  4505,\n",
      "        4506,  4514,  4518,  4519,  4529,  4531,  4554,  4570,  4578,\n",
      "        4592,  4691,  4739,  4740,  4747,  4763,  4771,  4791,  4890,\n",
      "        5065,  5066,  5194,  5385,  5502,  5503,  5522,  5542,  5563,\n",
      "        5566,  5586,  5590,  5599,  5621,  5625,  5648,  5826,  5827,\n",
      "        6033,  6119,  6151,  6155,  6163,  6184,  6203,  7191,  7196,\n",
      "        7240,  7383,  7492,  7500,  7501,  7508,  7516,  7517,  7524,\n",
      "        7532,  7533,  7536,  7540,  7567,  8660,  8975,  8976,  9069,\n",
      "        9073,  9086,  9093,  9102,  9185,  9425,  9469,  9513,  9525,\n",
      "        9659,  9890,  9914,  9934, 10127, 10147, 10256, 10261, 10367]),)\n",
      "1755 135\n",
      "1752 135\n",
      "1400 117 89 18\n",
      "['id' 'date' '6h' 'back-door' 'bathroom' 'bedroom' 'fridge-door'\n",
      " 'front-door' 'hallway' 'kitchen' 'lounge' 'total-events'\n",
      " 'unique-locations' 'active-location-ratio' 'private-to-public-ratio'\n",
      " 'location-entropy' 'location-dominance-ratio' 'back-and-forth-count'\n",
      " 'num-transitions' 'back-door-count-max' 'back-door-count-mean'\n",
      " 'back-door-count-std' 'back-door-count-sum' 'bathroom-count-max'\n",
      " 'bathroom-count-mean' 'bathroom-count-std' 'bathroom-count-sum'\n",
      " 'bedroom-count-max' 'bedroom-count-mean' 'bedroom-count-std'\n",
      " 'bedroom-count-sum' 'fridge-door-count-max' 'fridge-door-count-mean'\n",
      " 'fridge-door-count-std' 'fridge-door-count-sum' 'front-door-count-max'\n",
      " 'front-door-count-mean' 'front-door-count-std' 'front-door-count-sum'\n",
      " 'hallway-count-max' 'hallway-count-mean' 'hallway-count-std'\n",
      " 'hallway-count-sum' 'kitchen-count-max' 'kitchen-count-mean'\n",
      " 'kitchen-count-std' 'kitchen-count-sum' 'lounge-count-max'\n",
      " 'lounge-count-mean' 'lounge-count-std' 'lounge-count-sum'\n",
      " 'body-temperature' 'body-weight' 'diastolic-blood-pressure' 'heart-rate'\n",
      " 'muscle-mass' 'systolic-blood-pressure' 'total-body-water'\n",
      " 'skin-temperature' 'blood-pressure' 'agitation' 'body-water' 'pulse'\n",
      " 'weight' 'body-temperature-label' 'age' 'sex' 'agitation-next'\n",
      " 'agitation-four']\n"
     ]
    }
   ],
   "source": [
    "# display(dataset)\n",
    "\n",
    "print(dataset['agitation'].equals(dataset['agitation-next']))\n",
    "\n",
    "print(np.sum(dataset['agitation']))\n",
    "print(np.sum(dataset['agitation-next']))\n",
    "\n",
    "print(np.where(dataset['agitation'] == 1))\n",
    "print(np.where(dataset['agitation-next'] == 1))\n",
    "\n",
    "print(np.sum(dataset['agitation'] == 0),\n",
    "      np.sum(dataset['agitation'] == 1))\n",
    "\n",
    "print(np.sum(dataset['agitation-next'] == 0),\n",
    "      np.sum(dataset['agitation-next'] == 1))\n",
    "\n",
    "print(np.sum(dataset['agitation-four'] == 0),\n",
    "      np.sum(dataset['agitation-four'] == 1),\n",
    "      np.sum(dataset['agitation-four'] == 2),\n",
    "      np.sum(dataset['agitation-four'] == 3))\n",
    "\n",
    "\n",
    "# display(dataset)\n",
    "print(dataset.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of participants: 56\n",
      "Total number of participants with at least one agitation episode: 27\n",
      "Total number of participants with no agitation episodes: 29\n",
      "Total number of agitation episodes: 135\n"
     ]
    }
   ],
   "source": [
    "# Agitation Statistics\n",
    "\n",
    "temp = dataset.copy()\n",
    "\n",
    "temp['agitation'].fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "print(f\"Total number of participants: {temp['id'].nunique()}\")\n",
    "print(f\"Total number of participants with at least one agitation episode: {temp[temp['agitation'] == 1]['id'].nunique()}\")\n",
    "print(f\"Total number of participants with no agitation episodes: {temp['id'].nunique() - temp[temp['agitation'] == 1]['id'].nunique()}\")\n",
    "print(f\"Total number of agitation episodes: {(temp['agitation'] == 1).sum()}\")\n",
    "\n",
    "\n",
    "ids, total_counts = np.unique(temp['id'], return_counts=True)\n",
    "agitated_ids, agitation_counts = np.unique(temp[temp['agitation'] == 1]['id'], return_counts=True)\n",
    "agitation_map = dict(zip(agitated_ids, agitation_counts))\n",
    "unique_dates = temp.groupby('id')['date'].nunique().reindex(ids, fill_value=0).values\n",
    "participant_summary_df = pd.DataFrame({\n",
    "    'participant_id': ids,\n",
    "    'six_hour_sample_count': total_counts,\n",
    "    'unique_dates': unique_dates,\n",
    "    'agitation_episodes': [agitation_map.get(pid, 0) for pid in ids]\n",
    "})\n",
    "participant_summary_df = participant_summary_df.sort_values(by='agitation_episodes', ascending=False).reset_index(drop=True)\n",
    "# display(participant_summary_df)\n",
    "\n",
    "\n",
    "# values, counts = np.unique(temp.loc[temp['agitation'] == 1, '6h'], return_counts=True)\n",
    "# values, counts = np.unique(temp.loc[temp['agitation'] == 1, '12h'], return_counts=True)\n",
    "# values, counts = np.unique(temp.loc[temp['agitation'] == 1], return_counts=True)\n",
    "\n",
    "# agitation_temporal_df = pd.DataFrame({\n",
    "#     '6h_time_block': values,\n",
    "#     'agitation_episode_count': counts\n",
    "# })\n",
    "# display(agitation_temporal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 32 8 8 2 6\n"
     ]
    }
   ],
   "source": [
    "# 03\n",
    "\n",
    "if timescale == '06h':\n",
    "    columns_indices = ['id', 'date', '6h']\n",
    "elif timescale == '12h':\n",
    "    columns_indices = ['id', 'date', '12h']\n",
    "elif timescale == '24h':\n",
    "    columns_indices = ['id', 'date']\n",
    "\n",
    "# 08\n",
    "columns_count = ['back-door', 'bathroom', 'bedroom', 'fridge-door',\n",
    "                 'front-door', 'hallway', 'kitchen', 'lounge']\n",
    "\n",
    "# 08\n",
    "columns_contextual = ['total-events', 'unique-locations', 'active-location-ratio', 'private-to-public-ratio',\n",
    "                      'location-entropy', 'location-dominance-ratio', 'back-and-forth-count', 'num-transitions']\n",
    "\n",
    "# 32\n",
    "columns_statistical = ['back-door-count-max', 'back-door-count-mean', 'back-door-count-std', 'back-door-count-sum',\n",
    "                       'bathroom-count-max', 'bathroom-count-mean', 'bathroom-count-std', 'bathroom-count-sum',\n",
    "                       'bedroom-count-max', 'bedroom-count-mean', 'bedroom-count-std', 'bedroom-count-sum',\n",
    "                       'fridge-door-count-max', 'fridge-door-count-mean', 'fridge-door-count-std', 'fridge-door-count-sum',\n",
    "                       'front-door-count-max', 'front-door-count-mean', 'front-door-count-std', 'front-door-count-sum',\n",
    "                       'hallway-count-max', 'hallway-count-mean', 'hallway-count-std', 'hallway-count-sum',\n",
    "                       'kitchen-count-max', 'kitchen-count-mean', 'kitchen-count-std','kitchen-count-sum',\n",
    "                       'lounge-count-max', 'lounge-count-mean', 'lounge-count-std','lounge-count-sum']\n",
    "\n",
    "# 08\n",
    "columns_physiology = ['body-temperature', 'body-weight', 'diastolic-blood-pressure', 'heart-rate',\n",
    "                      'muscle-mass', 'systolic-blood-pressure', 'total-body-water', 'skin-temperature']\n",
    "\n",
    "# 06\n",
    "columns_labels = ['blood-pressure', 'agitation', 'body-water', 'pulse', 'weight', 'body-temperature-label']\n",
    "\n",
    "# 02\n",
    "columns_demographics = ['age', 'sex']\n",
    "\n",
    "# 02\n",
    "columns_prediction = ['agitation-next', 'agitation-four']\n",
    "\n",
    "print(len(columns_count),\n",
    "      len(columns_statistical),\n",
    "      len(columns_contextual),\n",
    "      len(columns_physiology),\n",
    "      len(columns_demographics),\n",
    "      len(columns_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'date', '6h', 'back-door', 'bathroom', 'bedroom', 'fridge-door', 'front-door', 'hallway', 'kitchen', 'lounge', 'total-events', 'unique-locations', 'active-location-ratio', 'private-to-public-ratio', 'location-entropy', 'location-dominance-ratio', 'back-and-forth-count', 'num-transitions', 'back-door-count-max', 'back-door-count-mean', 'back-door-count-std', 'back-door-count-sum', 'bathroom-count-max', 'bathroom-count-mean', 'bathroom-count-std', 'bathroom-count-sum', 'bedroom-count-max', 'bedroom-count-mean', 'bedroom-count-std', 'bedroom-count-sum', 'fridge-door-count-max', 'fridge-door-count-mean', 'fridge-door-count-std', 'fridge-door-count-sum', 'front-door-count-max', 'front-door-count-mean', 'front-door-count-std', 'front-door-count-sum', 'hallway-count-max', 'hallway-count-mean', 'hallway-count-std', 'hallway-count-sum', 'kitchen-count-max', 'kitchen-count-mean', 'kitchen-count-std', 'kitchen-count-sum', 'lounge-count-max', 'lounge-count-mean', 'lounge-count-std', 'lounge-count-sum', 'body-temperature', 'body-weight', 'diastolic-blood-pressure', 'heart-rate', 'muscle-mass', 'systolic-blood-pressure', 'total-body-water', 'skin-temperature', 'blood-pressure', 'agitation', 'body-water', 'pulse', 'weight', 'body-temperature-label', 'age', 'sex', 'agitation-next', 'agitation-four']\n",
      "['id', 'date', '6h', 'back-door', 'bathroom', 'bedroom', 'fridge-door', 'front-door', 'hallway', 'kitchen', 'lounge', 'total-events', 'unique-locations', 'active-location-ratio', 'private-to-public-ratio', 'location-entropy', 'location-dominance-ratio', 'back-and-forth-count', 'num-transitions', 'back-door-count-max', 'back-door-count-mean', 'back-door-count-std', 'back-door-count-sum', 'bathroom-count-max', 'bathroom-count-mean', 'bathroom-count-std', 'bathroom-count-sum', 'bedroom-count-max', 'bedroom-count-mean', 'bedroom-count-std', 'bedroom-count-sum', 'fridge-door-count-max', 'fridge-door-count-mean', 'fridge-door-count-std', 'fridge-door-count-sum', 'front-door-count-max', 'front-door-count-mean', 'front-door-count-std', 'front-door-count-sum', 'hallway-count-max', 'hallway-count-mean', 'hallway-count-std', 'hallway-count-sum', 'kitchen-count-max', 'kitchen-count-mean', 'kitchen-count-std', 'kitchen-count-sum', 'lounge-count-max', 'lounge-count-mean', 'lounge-count-std', 'lounge-count-sum', 'body-temperature', 'body-weight', 'diastolic-blood-pressure', 'heart-rate', 'muscle-mass', 'systolic-blood-pressure', 'total-body-water', 'skin-temperature', 'blood-pressure', 'agitation', 'body-water', 'pulse', 'weight', 'body-temperature-label', 'age', 'sex', 'agitation-next', 'agitation-four']\n",
      "True\n",
      "['id', 'date', '6h', 'total-events', 'unique-locations', 'active-location-ratio', 'private-to-public-ratio', 'location-entropy', 'location-dominance-ratio', 'back-and-forth-count', 'num-transitions', 'back-door-count-max', 'back-door-count-mean', 'back-door-count-std', 'back-door-count-sum', 'bathroom-count-max', 'bathroom-count-mean', 'bathroom-count-std', 'bathroom-count-sum', 'bedroom-count-max', 'bedroom-count-mean', 'bedroom-count-std', 'bedroom-count-sum', 'fridge-door-count-max', 'fridge-door-count-mean', 'fridge-door-count-std', 'fridge-door-count-sum', 'front-door-count-max', 'front-door-count-mean', 'front-door-count-std', 'front-door-count-sum', 'hallway-count-max', 'hallway-count-mean', 'hallway-count-std', 'hallway-count-sum', 'kitchen-count-max', 'kitchen-count-mean', 'kitchen-count-std', 'kitchen-count-sum', 'lounge-count-max', 'lounge-count-mean', 'lounge-count-std', 'lounge-count-sum', 'body-temperature', 'body-weight', 'diastolic-blood-pressure', 'heart-rate', 'muscle-mass', 'systolic-blood-pressure', 'total-body-water', 'skin-temperature', 'agitation']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'dataset: '"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>6h</th>\n",
       "      <th>total-events</th>\n",
       "      <th>unique-locations</th>\n",
       "      <th>active-location-ratio</th>\n",
       "      <th>private-to-public-ratio</th>\n",
       "      <th>location-entropy</th>\n",
       "      <th>location-dominance-ratio</th>\n",
       "      <th>back-and-forth-count</th>\n",
       "      <th>...</th>\n",
       "      <th>lounge-count-sum</th>\n",
       "      <th>body-temperature</th>\n",
       "      <th>body-weight</th>\n",
       "      <th>diastolic-blood-pressure</th>\n",
       "      <th>heart-rate</th>\n",
       "      <th>muscle-mass</th>\n",
       "      <th>systolic-blood-pressure</th>\n",
       "      <th>total-body-water</th>\n",
       "      <th>skin-temperature</th>\n",
       "      <th>agitation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0697d</td>\n",
       "      <td>2019-06-28</td>\n",
       "      <td>00-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0697d</td>\n",
       "      <td>2019-06-28</td>\n",
       "      <td>06-12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0697d</td>\n",
       "      <td>2019-06-28</td>\n",
       "      <td>12-18</td>\n",
       "      <td>221.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.1691</td>\n",
       "      <td>2.7340</td>\n",
       "      <td>0.2579</td>\n",
       "      <td>35.0</td>\n",
       "      <td>...</td>\n",
       "      <td>49.0</td>\n",
       "      <td>36.1864</td>\n",
       "      <td>86.3</td>\n",
       "      <td>82.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>64.5</td>\n",
       "      <td>165.0</td>\n",
       "      <td>50.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0697d</td>\n",
       "      <td>2019-06-28</td>\n",
       "      <td>18-24</td>\n",
       "      <td>101.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.0889</td>\n",
       "      <td>1.8916</td>\n",
       "      <td>0.4851</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0697d</td>\n",
       "      <td>2019-06-29</td>\n",
       "      <td>00-06</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10785</th>\n",
       "      <td>fd100</td>\n",
       "      <td>2019-06-29</td>\n",
       "      <td>12-18</td>\n",
       "      <td>182.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.2283</td>\n",
       "      <td>2.5224</td>\n",
       "      <td>0.3077</td>\n",
       "      <td>31.0</td>\n",
       "      <td>...</td>\n",
       "      <td>46.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10786</th>\n",
       "      <td>fd100</td>\n",
       "      <td>2019-06-29</td>\n",
       "      <td>18-24</td>\n",
       "      <td>114.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.3671</td>\n",
       "      <td>2.4755</td>\n",
       "      <td>0.2982</td>\n",
       "      <td>19.0</td>\n",
       "      <td>...</td>\n",
       "      <td>34.0</td>\n",
       "      <td>36.2400</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10787</th>\n",
       "      <td>fd100</td>\n",
       "      <td>2019-06-30</td>\n",
       "      <td>00-06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10788</th>\n",
       "      <td>fd100</td>\n",
       "      <td>2019-06-30</td>\n",
       "      <td>06-12</td>\n",
       "      <td>146.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.5465</td>\n",
       "      <td>2.5379</td>\n",
       "      <td>0.2603</td>\n",
       "      <td>10.0</td>\n",
       "      <td>...</td>\n",
       "      <td>31.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10789</th>\n",
       "      <td>fd100</td>\n",
       "      <td>2019-06-30</td>\n",
       "      <td>12-18</td>\n",
       "      <td>127.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.3636</td>\n",
       "      <td>2.1376</td>\n",
       "      <td>0.3937</td>\n",
       "      <td>8.0</td>\n",
       "      <td>...</td>\n",
       "      <td>32.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10790 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id        date     6h  total-events  unique-locations  \\\n",
       "0      0697d  2019-06-28  00-06           0.0               0.0   \n",
       "1      0697d  2019-06-28  06-12           0.0               0.0   \n",
       "2      0697d  2019-06-28  12-18         221.0               8.0   \n",
       "3      0697d  2019-06-28  18-24         101.0               7.0   \n",
       "4      0697d  2019-06-29  00-06           3.0               1.0   \n",
       "...      ...         ...    ...           ...               ...   \n",
       "10785  fd100  2019-06-29  12-18         182.0               7.0   \n",
       "10786  fd100  2019-06-29  18-24         114.0               7.0   \n",
       "10787  fd100  2019-06-30  00-06           0.0               0.0   \n",
       "10788  fd100  2019-06-30  06-12         146.0               7.0   \n",
       "10789  fd100  2019-06-30  12-18         127.0               7.0   \n",
       "\n",
       "       active-location-ratio  private-to-public-ratio  location-entropy  \\\n",
       "0                      0.000                   1.0000            0.0000   \n",
       "1                      0.000                   1.0000            0.0000   \n",
       "2                      1.000                   0.1691            2.7340   \n",
       "3                      0.875                   0.0889            1.8916   \n",
       "4                      0.125                   1.0000           -0.0000   \n",
       "...                      ...                      ...               ...   \n",
       "10785                  0.875                   0.2283            2.5224   \n",
       "10786                  0.875                   0.3671            2.4755   \n",
       "10787                  0.000                   1.0000            0.0000   \n",
       "10788                  0.875                   0.5465            2.5379   \n",
       "10789                  0.875                   0.3636            2.1376   \n",
       "\n",
       "       location-dominance-ratio  back-and-forth-count  ...  lounge-count-sum  \\\n",
       "0                        0.0000                   0.0  ...               0.0   \n",
       "1                        0.0000                   0.0  ...               0.0   \n",
       "2                        0.2579                  35.0  ...              49.0   \n",
       "3                        0.4851                  22.0  ...              31.0   \n",
       "4                        1.0000                   0.0  ...               0.0   \n",
       "...                         ...                   ...  ...               ...   \n",
       "10785                    0.3077                  31.0  ...              46.0   \n",
       "10786                    0.2982                  19.0  ...              34.0   \n",
       "10787                    0.0000                   0.0  ...               0.0   \n",
       "10788                    0.2603                  10.0  ...              31.0   \n",
       "10789                    0.3937                   8.0  ...              32.0   \n",
       "\n",
       "       body-temperature  body-weight  diastolic-blood-pressure  heart-rate  \\\n",
       "0                   NaN          NaN                       NaN         NaN   \n",
       "1                   NaN          NaN                       NaN         NaN   \n",
       "2               36.1864         86.3                      82.0        42.0   \n",
       "3                   NaN          NaN                       NaN         NaN   \n",
       "4                   NaN          NaN                       NaN         NaN   \n",
       "...                 ...          ...                       ...         ...   \n",
       "10785               NaN          NaN                       NaN         NaN   \n",
       "10786           36.2400          NaN                       NaN         NaN   \n",
       "10787               NaN          NaN                       NaN         NaN   \n",
       "10788               NaN          NaN                       NaN         NaN   \n",
       "10789               NaN          NaN                       NaN         NaN   \n",
       "\n",
       "       muscle-mass  systolic-blood-pressure  total-body-water  \\\n",
       "0              NaN                      NaN               NaN   \n",
       "1              NaN                      NaN               NaN   \n",
       "2             64.5                    165.0              50.6   \n",
       "3              NaN                      NaN               NaN   \n",
       "4              NaN                      NaN               NaN   \n",
       "...            ...                      ...               ...   \n",
       "10785          NaN                      NaN               NaN   \n",
       "10786          NaN                      NaN               NaN   \n",
       "10787          NaN                      NaN               NaN   \n",
       "10788          NaN                      NaN               NaN   \n",
       "10789          NaN                      NaN               NaN   \n",
       "\n",
       "       skin-temperature  agitation  \n",
       "0                   NaN        0.0  \n",
       "1                   NaN        0.0  \n",
       "2                   NaN        0.0  \n",
       "3                   NaN        0.0  \n",
       "4                   NaN        0.0  \n",
       "...                 ...        ...  \n",
       "10785               NaN        NaN  \n",
       "10786               NaN        NaN  \n",
       "10787               NaN        NaN  \n",
       "10788               NaN        NaN  \n",
       "10789               NaN        NaN  \n",
       "\n",
       "[10790 rows x 52 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "columns = columns_indices + columns_count + columns_contextual + columns_statistical + columns_physiology + columns_labels + columns_demographics + columns_prediction\n",
    "print(dataset.columns.to_list())\n",
    "print(columns)\n",
    "\n",
    "print(columns == dataset.columns.to_list())\n",
    "\n",
    "\n",
    "# y = np.array(dataset[[label]]).squeeze()\n",
    "\n",
    "# if label == 'agitation' or label == 'agitation-next':\n",
    "#     y[y == -1] = 0\n",
    "#     y[y >= 1] = 1\n",
    "# elif label == 'agitation-four':\n",
    "#     y[y == -1] = 0\n",
    "#     y[y == -10] = 0\n",
    "\n",
    "# ids = np.array(dataset['id']).squeeze()\n",
    "# p = np.unique(ids, return_inverse=True)[1]\n",
    "\n",
    "# print(np.isnan(y).sum())\n",
    "# print(np.isnan(p).sum())\n",
    "\n",
    "# y = np.nan_to_num(y, nan=0)\n",
    "\n",
    "# print(np.isnan(y).sum())\n",
    "# print(np.isnan(p).sum())\n",
    "\n",
    "# print(np.unique(y))\n",
    "# print(y.sum())\n",
    "\n",
    "\n",
    "# To Drop\n",
    "dataset.drop(\n",
    "    \n",
    "    columns_count +\n",
    "    \n",
    "\n",
    "    # columns_contextual +\n",
    "\n",
    "    # columns_statistical +\n",
    "    \n",
    "    # columns_physiology +\n",
    "\n",
    "\n",
    "    list(set(columns_labels) - set(['agitation'])) +\n",
    "\n",
    "    columns_demographics +\n",
    "    columns_prediction\n",
    "\n",
    "    , axis=1, inplace=True)\n",
    "\n",
    "print(dataset.columns.to_list())\n",
    "\n",
    "# display('y: ', y)\n",
    "# display('p: ', p)\n",
    "display('dataset: ', dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98769\n",
      "8900\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Imputation\n",
    "\n",
    "import utils_data\n",
    "importlib.reload(utils_data)\n",
    "from utils_data import hierarchical_imputation, hierarchical_imputation_columns_to_exclude\n",
    "\n",
    "\n",
    "print(dataset.isna().sum().sum())\n",
    "\n",
    "dataset = hierarchical_imputation_columns_to_exclude(dataset, columns_to_exclude=columns_indices + ['agitation'])\n",
    "\n",
    "print(dataset.isna().sum().sum())\n",
    "\n",
    "temp = dataset.drop(columns_indices + ['agitation'], axis=1, inplace=False)\n",
    "                    \n",
    "print(temp.isna().sum().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n",
      "135\n",
      "(10230, 10, 48)\n",
      "(10230,)\n",
      "(10230,)\n"
     ]
    }
   ],
   "source": [
    "# Dataset - Sequential\n",
    "\n",
    "import utils_data\n",
    "importlib.reload(utils_data)\n",
    "from utils_data import segment_dataframe\n",
    "\n",
    "dataset.fillna(0, inplace=True)\n",
    "\n",
    "sements, labels, participants = segment_dataframe(dataset, segment_length, columns_indices)\n",
    "\n",
    "x = np.array(sements)\n",
    "y = np.array(labels)\n",
    "p = np.unique(participants, return_inverse=True)[1]\n",
    "\n",
    "np.save(os.path.join('/home/ali/PycharmProjects/tihm/xyp', 'x.npy'), x)\n",
    "np.save(os.path.join('/home/ali/PycharmProjects/tihm/xyp', 'y.npy'), y)\n",
    "np.save(os.path.join('/home/ali/PycharmProjects/tihm/xyp', 'p.npy'), p)\n",
    "\n",
    "print(np.unique(y))\n",
    "print(np.sum(y))\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 8184 2046\n",
      "Original class distribution: Counter({np.int64(0): 8067, np.int64(1): 117})\n",
      "Resampled class distribution: Counter({np.int64(0): 2016, np.int64(1): 117})\n",
      "2 8184 2046\n",
      "Original class distribution: Counter({np.int64(0): 8084, np.int64(1): 100})\n",
      "Resampled class distribution: Counter({np.int64(0): 2021, np.int64(1): 100})\n",
      "3 8184 2046\n",
      "Original class distribution: Counter({np.int64(0): 8069, np.int64(1): 115})\n",
      "Resampled class distribution: Counter({np.int64(0): 2017, np.int64(1): 115})\n",
      "4 8184 2046\n",
      "Original class distribution: Counter({np.int64(0): 8082, np.int64(1): 102})\n",
      "Resampled class distribution: Counter({np.int64(0): 2020, np.int64(1): 102})\n",
      "5 8184 2046\n",
      "Original class distribution: Counter({np.int64(0): 8078, np.int64(1): 106})\n",
      "Resampled class distribution: Counter({np.int64(0): 2019, np.int64(1): 106})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auc-roc</th>\n",
       "      <th>auc-pr</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>specificity</th>\n",
       "      <th>tn</th>\n",
       "      <th>fp</th>\n",
       "      <th>fn</th>\n",
       "      <th>tp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8540</td>\n",
       "      <td>0.0795</td>\n",
       "      <td>0.8776</td>\n",
       "      <td>0.0720</td>\n",
       "      <td>0.6963</td>\n",
       "      <td>0.1306</td>\n",
       "      <td>0.6963</td>\n",
       "      <td>0.8800</td>\n",
       "      <td>8884</td>\n",
       "      <td>1211</td>\n",
       "      <td>41</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  auc-roc  auc-pr accuracy precision  recall f1-score sensitivity specificity  \\\n",
       "0  0.8540  0.0795   0.8776    0.0720  0.6963   0.1306      0.6963      0.8800   \n",
       "\n",
       "     tn    fp  fn  tp  \n",
       "0  8884  1211  41  94  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ROCKET\n",
    "\n",
    "name = 'ROCKET'.lower()\n",
    "\n",
    "Y_TRUES = np.empty([0])\n",
    "Y_PROBS = np.empty([0])\n",
    "Y_PREDS = np.empty([0])\n",
    "\n",
    "\n",
    "if kfold:\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    split_iterator = cv.split(x)\n",
    "else:\n",
    "    cv = LeavePGroupsOut(n_groups=1)\n",
    "    split_iterator = cv.split(x, y, groups=p)\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(split_iterator, start=1):\n",
    "    participant = i if kfold else np.unique(p[test_idx])[0]\n",
    "\n",
    "    x_train, x_test = x[train_idx], x[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    print(participant, x_train.shape[0], x_test.shape[0])\n",
    "\n",
    "\n",
    "    num_samples, time_steps, num_features = x_train.shape\n",
    "    x_train_flat = x_train.reshape(-1, num_features)\n",
    "    x_test_flat = x_test.reshape(-1, num_features)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train_flat)\n",
    "    x_train_scaled = scaler.transform(x_train_flat)\n",
    "    x_test_scaled = scaler.transform(x_test_flat)\n",
    "\n",
    "    normalizer = MinMaxScaler()\n",
    "    normalizer.fit(x_train_scaled)\n",
    "    x_train_scaled = normalizer.transform(x_train_scaled)\n",
    "    x_test_scaled = normalizer.transform(x_test_scaled)\n",
    "\n",
    "    x_train = x_train_scaled.reshape(num_samples, time_steps, num_features)\n",
    "    x_test = x_test_scaled.reshape(x_test.shape[0], time_steps, num_features)\n",
    "\n",
    "    # Oversample class 1\n",
    "\n",
    "    # ros = RandomOverSampler(sampling_strategy=.1, random_state=seed)\n",
    "    # x_train, y_train = ros.fit_resample(x_train, y_train)\n",
    "\n",
    "    # # 1. Check original class distribution\n",
    "    # print(\"Original class distribution:\", Counter(y_train))\n",
    "    # # 2. Define the minority class (adjust if needed)\n",
    "    # minority_class = 1  # change this if your minority class label is different\n",
    "    # current_minority_count = sum(y_train == minority_class)\n",
    "    # # 3. Define desired new total count for the minority class (10x)\n",
    "    # target_minority_count = current_minority_count * 10\n",
    "    # # 4. Setup SMOTE with custom sampling strategy\n",
    "    # smote = SMOTE(sampling_strategy={minority_class: target_minority_count}, random_state=seed)\n",
    "    # # 5. Fit and resample\n",
    "    # x_train, y_train = smote.fit_resample(x_train, y_train)\n",
    "    # # 6. Confirm new distribution\n",
    "    # print(\"Resampled class distribution:\", Counter(y_train))\n",
    "\n",
    "\n",
    "    # Undersample class 0\n",
    "    reduction_percent = 75\n",
    "    print(\"Original class distribution:\", Counter(y_train))    \n",
    "    class_0_idx = np.where(y_train == 0)[0]\n",
    "    class_1_idx = np.where(y_train == 1)[0]\n",
    "    # How many class 0 samples to keep\n",
    "    n_keep = int(len(class_0_idx) * (1 - reduction_percent / 100.0))\n",
    "    sampled_0_idx = np.random.choice(class_0_idx, size=n_keep, replace=False)\n",
    "    # Combine sampled class 0 with all class 1\n",
    "    final_idx = np.concatenate([sampled_0_idx, class_1_idx])\n",
    "    x_train = x_train[final_idx]\n",
    "    y_train =  y_train[final_idx]\n",
    "    print(\"Resampled class distribution:\", Counter(y_train))\n",
    "\n",
    "\n",
    "    rocket = Rocket(num_kernels=1_000)\n",
    "    rocket.fit(x_train)\n",
    "    x_train_transformed = rocket.transform(x_train)\n",
    "    x_test_transformed = rocket.transform(x_test)\n",
    "\n",
    "\n",
    "    # ----- Gradient Boosting Classifier\n",
    "    model = GradientBoostingClassifier(\n",
    "        n_estimators=100,  # match LightGBM\n",
    "        learning_rate=0.001,  # match LightGBM\n",
    "        max_depth=4,  # similar to LightGBM default tree depth\n",
    "        subsample=1.0,  # default\n",
    "        random_state=seed\n",
    "    )\n",
    "    if weighted:\n",
    "        model.fit(x_train_transformed, y_train, sample_weight=compute_sample_weight(class_weight='balanced', y=y_train))\n",
    "    else:\n",
    "        model.fit(x_train_transformed, y_train)\n",
    "\n",
    "    y_probs = model.predict_proba(x_test_transformed)[:, 1]\n",
    "    y_preds = model.predict(x_test_transformed)\n",
    "    \n",
    "\n",
    "    Y_TRUES = np.append(Y_TRUES, y_test)\n",
    "    Y_PROBS = np.append(Y_PROBS, y_probs)\n",
    "    Y_PREDS = np.append(Y_PREDS, y_preds)\n",
    "\n",
    "\n",
    "indx = Y_TRUES.argsort()\n",
    "Y_TRUES = Y_TRUES[indx]\n",
    "Y_PROBS = Y_PROBS[indx]\n",
    "Y_PREDS = Y_PREDS[indx]\n",
    "\n",
    "\n",
    "# Evaluation metrics\n",
    "auc_roc = roc_auc_score(Y_TRUES, Y_PROBS)\n",
    "precision, recall, _ = precision_recall_curve(Y_TRUES, Y_PROBS)\n",
    "auc_pr = auc(recall, precision)\n",
    "acc = accuracy_score(Y_TRUES, Y_PREDS)\n",
    "pre = precision_score(Y_TRUES, Y_PREDS)\n",
    "rec = recall_score(Y_TRUES, Y_PREDS)\n",
    "f1 = f1_score(Y_TRUES, Y_PREDS)\n",
    "\n",
    "# Confusion matrix: [ [TN, FP], [FN, TP] ]\n",
    "tn, fp, fn, tp = confusion_matrix(Y_TRUES, Y_PREDS).ravel()\n",
    "\n",
    "# Sensitivity = Recall = TP / (TP + FN)\n",
    "sensitivity = tp / (tp + fn)\n",
    "\n",
    "# Specificity = TN / (TN + FP)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "results = {\n",
    "    'auc-roc': f\"{auc_roc:.4f}\",\n",
    "    'auc-pr': f\"{auc_pr:.4f}\",\n",
    "    'accuracy': f\"{acc:.4f}\",\n",
    "    'precision': f\"{pre:.4f}\",\n",
    "    'recall': f\"{rec:.4f}\",\n",
    "    'f1-score': f\"{f1:.4f}\",\n",
    "    'sensitivity': f\"{sensitivity:.4f}\",\n",
    "    'specificity': f\"{specificity:.4f}\",\n",
    "    'tn': str(int(tn)),\n",
    "    'fp': str(int(fp)),\n",
    "    'fn': str(int(fn)),\n",
    "    'tp': str(int(tp))\n",
    "}\n",
    "\n",
    "\n",
    "results = pd.DataFrame([results])\n",
    "results.to_csv(os.path.join(results_dir, name + '.csv'), index=False)\n",
    "display(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ali-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
